{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUT4eN+ILYuaAflcWEjHz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akinolanath5519/Spam-or-Ham-Email-Classifier/blob/main/Email_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1WP9e5Pg7wqy"
      },
      "outputs": [],
      "source": [
        "#importing the libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "msg=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lxhfGSOp8KmD",
        "outputId": "192f4dcc-959e-4ad5-db60-0fc7c85f72a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-658c1ad5-9861-4dcc-a437-6df0aa27e69b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-658c1ad5-9861-4dcc-a437-6df0aa27e69b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving spam.csv to spam.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N87TeL8C93ik",
        "outputId": "1e8675de-4dbd-4fa5-98f2-af819ffb2024"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV with proper encoding\n",
        "df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
        "\n",
        "# Keep only relevant columns\n",
        "df = df[['v1', 'v2']]\n",
        "\n",
        "# Rename columns for clarity\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "# Check the first few rows\n",
        "df.head(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FLRW0JPC8YMf",
        "outputId": "20ee3b55-8a62-4058-d62c-f0bc0aabb7b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30d2163a-895e-4b0c-af08-b221b785c50a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30d2163a-895e-4b0c-af08-b221b785c50a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-30d2163a-895e-4b0c-af08-b221b785c50a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-30d2163a-895e-4b0c-af08-b221b785c50a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-96f594b1-9dcc-462f-bdb8-1889686dd61d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96f594b1-9dcc-462f-bdb8-1889686dd61d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-96f594b1-9dcc-462f-bdb8-1889686dd61d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"Did u download the fring app?\",\n          \"Pass dis to all ur contacts n see wat u get! Red;i'm in luv wid u. Blue;u put a smile on my face. Purple;u r realy hot. Pink;u r so swt. Orange;i thnk i lyk u. Green;i realy wana go out wid u. Yelow;i wnt u bck. Black;i'm jealous of u. Brown;i miss you Nw plz giv me one color\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the distribution of the label to know if the we have imbalance class\n",
        "df['label'].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "xUUGD4pqEk47",
        "outputId": "03df9da2-df5f-4894-ba59-751d9cbbdf12"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "ham     4825\n",
              "spam     747\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>747</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "KiCya0XV8h72"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Text Preprocessing 1\n"
      ],
      "metadata": {
        "id": "eBwVAzSiAtB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_tokens(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "    print(\"After lowercasing:\", text)\n",
        "\n",
        "    # 2. Remove special characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    print(\"After removing special chars:\", text)\n",
        "\n",
        "    # 3. Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    print(\"After tokenization:\", tokens)\n",
        "\n",
        "    # 4. Stop word removal\n",
        "    tokens_no_stop = [word for word in tokens if word not in stop_words]\n",
        "    print(\"After stop word removal:\", tokens_no_stop)\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens_no_stop]\n",
        "    print(\"After lemmatization:\", tokens_lemmatized)\n",
        "\n",
        "    return tokens_lemmatized\n",
        "\n",
        "# Example email\n",
        "example_email = df['message'][4]  # or df['text'][0] depending on your column\n",
        "tokens = preprocess_text_tokens(example_email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoIFEAbY-2pb",
        "outputId": "c4b95b63-16ff-4d5b-e5bf-cc1d0366f783"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lowercasing: nah i don't think he goes to usf, he lives around here though\n",
            "After removing special chars: nah i dont think he goes to usf he lives around here though\n",
            "After tokenization: ['nah', 'i', 'dont', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']\n",
            "After stop word removal: ['nah', 'dont', 'think', 'goes', 'usf', 'lives', 'around', 'though']\n",
            "After lemmatization: ['nah', 'dont', 'think', 'go', 'usf', 'life', 'around', 'though']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting text into vectors using techniques bag of words,TF-IDF"
      ],
      "metadata": {
        "id": "wwHqS0kLA6Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert tokens back to string\n",
        "def tokens_to_string(tokens):\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing and convert tokens to string\n",
        "df['clean_text'] = df['message'].apply(lambda x: tokens_to_string(preprocess_text_tokens(x)))\n",
        "\n",
        "# # Check\n",
        "# print(df[['message', 'clean_text']].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEENnsfG-3FU",
        "outputId": "c17d8d50-9540-4946-ea5a-3e4743b19a98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "After lowercasing: \\urgent! this is the 2nd attempt to contact u!u have won å£1000call 09071512432 b4 300603t&csbcm4235wc1n3xx.callcost150ppmmobilesvary. maxå£7. 50\\\"\"\n",
            "After removing special chars: urgent this is the nd attempt to contact uu have won call  b tcsbcmwcnxxcallcostppmmobilesvary max \n",
            "After tokenization: ['urgent', 'this', 'is', 'the', 'nd', 'attempt', 'to', 'contact', 'uu', 'have', 'won', 'call', 'b', 'tcsbcmwcnxxcallcostppmmobilesvary', 'max']\n",
            "After stop word removal: ['urgent', 'nd', 'attempt', 'contact', 'uu', 'call', 'b', 'tcsbcmwcnxxcallcostppmmobilesvary', 'max']\n",
            "After lemmatization: ['urgent', 'nd', 'attempt', 'contact', 'uu', 'call', 'b', 'tcsbcmwcnxxcallcostppmmobilesvary', 'max']\n",
            "After lowercasing: :( but your not here....\n",
            "After removing special chars:  but your not here\n",
            "After tokenization: ['but', 'your', 'not', 'here']\n",
            "After stop word removal: []\n",
            "After lemmatization: []\n",
            "After lowercasing: not directly behind... abt 4 rows behind ì_...\n",
            "After removing special chars: not directly behind abt  rows behind \n",
            "After tokenization: ['not', 'directly', 'behind', 'abt', 'rows', 'behind']\n",
            "After stop word removal: ['directly', 'behind', 'abt', 'rows', 'behind']\n",
            "After lemmatization: ['directly', 'behind', 'abt', 'row', 'behind']\n",
            "After lowercasing: congratulations ur awarded 500 of cd vouchers or 125gift guaranteed & free entry 2 100 wkly draw txt music to 87066\n",
            "After removing special chars: congratulations ur awarded  of cd vouchers or gift guaranteed  free entry   wkly draw txt music to \n",
            "After tokenization: ['congratulations', 'ur', 'awarded', 'of', 'cd', 'vouchers', 'or', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music', 'to']\n",
            "After stop word removal: ['congratulations', 'ur', 'awarded', 'cd', 'vouchers', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music']\n",
            "After lemmatization: ['congratulation', 'ur', 'awarded', 'cd', 'voucher', 'gift', 'guaranteed', 'free', 'entry', 'wkly', 'draw', 'txt', 'music']\n",
            "After lowercasing: had your contract mobile 11 mnths? latest motorola, nokia etc. all free! double mins & text on orange tariffs. text yes for callback, no to remove from records\n",
            "After removing special chars: had your contract mobile  mnths latest motorola nokia etc all free double mins  text on orange tariffs text yes for callback no to remove from records\n",
            "After tokenization: ['had', 'your', 'contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'all', 'free', 'double', 'mins', 'text', 'on', 'orange', 'tariffs', 'text', 'yes', 'for', 'callback', 'no', 'to', 'remove', 'from', 'records']\n",
            "After stop word removal: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'mins', 'text', 'orange', 'tariffs', 'text', 'yes', 'callback', 'remove', 'records']\n",
            "After lemmatization: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'min', 'text', 'orange', 'tariff', 'text', 'yes', 'callback', 'remove', 'record']\n",
            "After lowercasing: urgent! call 09066350750 from your landline. your complimentary 4* ibiza holiday or 10,000 cash await collection sae t&cs po box 434 sk3 8wp 150 ppm 18+\n",
            "After removing special chars: urgent call  from your landline your complimentary  ibiza holiday or  cash await collection sae tcs po box  sk wp  ppm \n",
            "After tokenization: ['urgent', 'call', 'from', 'your', 'landline', 'your', 'complimentary', 'ibiza', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 'tcs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After stop word removal: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'tcs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After lemmatization: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'tc', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After lowercasing: no plans yet. what are you doing ?\n",
            "After removing special chars: no plans yet what are you doing \n",
            "After tokenization: ['no', 'plans', 'yet', 'what', 'are', 'you', 'doing']\n",
            "After stop word removal: ['plans', 'yet']\n",
            "After lemmatization: ['plan', 'yet']\n",
            "After lowercasing: hi ....my engagement has been fixd on  &lt;#&gt; th of next month. i know its really shocking bt....hmm njan vilikkam....t ws al of a sudn;-(.\n",
            "After removing special chars: hi my engagement has been fixd on  ltgt th of next month i know its really shocking bthmm njan vilikkamt ws al of a sudn\n",
            "After tokenization: ['hi', 'my', 'engagement', 'has', 'been', 'fixd', 'on', 'ltgt', 'th', 'of', 'next', 'month', 'i', 'know', 'its', 'really', 'shocking', 'bthmm', 'njan', 'vilikkamt', 'ws', 'al', 'of', 'a', 'sudn']\n",
            "After stop word removal: ['hi', 'engagement', 'fixd', 'ltgt', 'th', 'next', 'month', 'know', 'really', 'shocking', 'bthmm', 'njan', 'vilikkamt', 'ws', 'al', 'sudn']\n",
            "After lemmatization: ['hi', 'engagement', 'fixd', 'ltgt', 'th', 'next', 'month', 'know', 'really', 'shocking', 'bthmm', 'njan', 'vilikkamt', 'w', 'al', 'sudn']\n",
            "After lowercasing: not course. only maths one day one chapter with in one month we can finish.\n",
            "After removing special chars: not course only maths one day one chapter with in one month we can finish\n",
            "After tokenization: ['not', 'course', 'only', 'maths', 'one', 'day', 'one', 'chapter', 'with', 'in', 'one', 'month', 'we', 'can', 'finish']\n",
            "After stop word removal: ['course', 'maths', 'one', 'day', 'one', 'chapter', 'one', 'month', 'finish']\n",
            "After lemmatization: ['course', 'math', 'one', 'day', 'one', 'chapter', 'one', 'month', 'finish']\n",
            "After lowercasing: wow didn't think it was that common. i take it all back ur not a freak! unless u chop it off:-)\n",
            "After removing special chars: wow didnt think it was that common i take it all back ur not a freak unless u chop it off\n",
            "After tokenization: ['wow', 'didnt', 'think', 'it', 'was', 'that', 'common', 'i', 'take', 'it', 'all', 'back', 'ur', 'not', 'a', 'freak', 'unless', 'u', 'chop', 'it', 'off']\n",
            "After stop word removal: ['wow', 'didnt', 'think', 'common', 'take', 'back', 'ur', 'freak', 'unless', 'u', 'chop']\n",
            "After lemmatization: ['wow', 'didnt', 'think', 'common', 'take', 'back', 'ur', 'freak', 'unless', 'u', 'chop']\n",
            "After lowercasing: for ur chance to win a å£250 wkly shopping spree txt: shop to 80878. t's&c's www.txt-2-shop.com custcare 08715705022, 1x150p/wk\n",
            "After removing special chars: for ur chance to win a  wkly shopping spree txt shop to  tscs wwwtxtshopcom custcare  xpwk\n",
            "After tokenization: ['for', 'ur', 'chance', 'to', 'win', 'a', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'to', 'tscs', 'wwwtxtshopcom', 'custcare', 'xpwk']\n",
            "After stop word removal: ['ur', 'chance', 'win', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'tscs', 'wwwtxtshopcom', 'custcare', 'xpwk']\n",
            "After lemmatization: ['ur', 'chance', 'win', 'wkly', 'shopping', 'spree', 'txt', 'shop', 'tscs', 'wwwtxtshopcom', 'custcare', 'xpwk']\n",
            "After lowercasing: noooooooo please. last thing i need is stress. for once in your life be fair.\n",
            "After removing special chars: noooooooo please last thing i need is stress for once in your life be fair\n",
            "After tokenization: ['noooooooo', 'please', 'last', 'thing', 'i', 'need', 'is', 'stress', 'for', 'once', 'in', 'your', 'life', 'be', 'fair']\n",
            "After stop word removal: ['noooooooo', 'please', 'last', 'thing', 'need', 'stress', 'life', 'fair']\n",
            "After lemmatization: ['noooooooo', 'please', 'last', 'thing', 'need', 'stress', 'life', 'fair']\n",
            "After lowercasing: u have a secret admirer who is looking 2 make contact with u-find out who they r*reveal who thinks ur so special-call on 09065171142-stopsms-08718727870150ppm\n",
            "After removing special chars: u have a secret admirer who is looking  make contact with ufind out who they rreveal who thinks ur so specialcall on stopsmsppm\n",
            "After tokenization: ['u', 'have', 'a', 'secret', 'admirer', 'who', 'is', 'looking', 'make', 'contact', 'with', 'ufind', 'out', 'who', 'they', 'rreveal', 'who', 'thinks', 'ur', 'so', 'specialcall', 'on', 'stopsmsppm']\n",
            "After stop word removal: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'ufind', 'rreveal', 'thinks', 'ur', 'specialcall', 'stopsmsppm']\n",
            "After lemmatization: ['u', 'secret', 'admirer', 'looking', 'make', 'contact', 'ufind', 'rreveal', 'think', 'ur', 'specialcall', 'stopsmsppm']\n",
            "After lowercasing: mila, age23, blonde, new in uk. i look sex with uk guys. if u like fun with me. text mtalk to 69866.18 . 30pp/txt 1st 5free. å£1.50 increments. help08718728876\n",
            "After removing special chars: mila age blonde new in uk i look sex with uk guys if u like fun with me text mtalk to   pptxt st free  increments help\n",
            "After tokenization: ['mila', 'age', 'blonde', 'new', 'in', 'uk', 'i', 'look', 'sex', 'with', 'uk', 'guys', 'if', 'u', 'like', 'fun', 'with', 'me', 'text', 'mtalk', 'to', 'pptxt', 'st', 'free', 'increments', 'help']\n",
            "After stop word removal: ['mila', 'age', 'blonde', 'new', 'uk', 'look', 'sex', 'uk', 'guys', 'u', 'like', 'fun', 'text', 'mtalk', 'pptxt', 'st', 'free', 'increments', 'help']\n",
            "After lemmatization: ['mila', 'age', 'blonde', 'new', 'uk', 'look', 'sex', 'uk', 'guy', 'u', 'like', 'fun', 'text', 'mtalk', 'pptxt', 'st', 'free', 'increment', 'help']\n",
            "After lowercasing: i'll see if i can swing by in a bit, got some things to take care of here firsg\n",
            "After removing special chars: ill see if i can swing by in a bit got some things to take care of here firsg\n",
            "After tokenization: ['ill', 'see', 'if', 'i', 'can', 'swing', 'by', 'in', 'a', 'bit', 'got', 'some', 'things', 'to', 'take', 'care', 'of', 'here', 'firsg']\n",
            "After stop word removal: ['ill', 'see', 'swing', 'bit', 'got', 'things', 'take', 'care', 'firsg']\n",
            "After lemmatization: ['ill', 'see', 'swing', 'bit', 'got', 'thing', 'take', 'care', 'firsg']\n",
            "After lowercasing: i wanted to wish you a happy new year and i wanted to talk to you about some legal advice to do with when gary and i split but in person. i'll make a trip to ptbo for that. i hope everything is good with you babe and i love ya :)\n",
            "After removing special chars: i wanted to wish you a happy new year and i wanted to talk to you about some legal advice to do with when gary and i split but in person ill make a trip to ptbo for that i hope everything is good with you babe and i love ya \n",
            "After tokenization: ['i', 'wanted', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'and', 'i', 'wanted', 'to', 'talk', 'to', 'you', 'about', 'some', 'legal', 'advice', 'to', 'do', 'with', 'when', 'gary', 'and', 'i', 'split', 'but', 'in', 'person', 'ill', 'make', 'a', 'trip', 'to', 'ptbo', 'for', 'that', 'i', 'hope', 'everything', 'is', 'good', 'with', 'you', 'babe', 'and', 'i', 'love', 'ya']\n",
            "After stop word removal: ['wanted', 'wish', 'happy', 'new', 'year', 'wanted', 'talk', 'legal', 'advice', 'gary', 'split', 'person', 'ill', 'make', 'trip', 'ptbo', 'hope', 'everything', 'good', 'babe', 'love', 'ya']\n",
            "After lemmatization: ['wanted', 'wish', 'happy', 'new', 'year', 'wanted', 'talk', 'legal', 'advice', 'gary', 'split', 'person', 'ill', 'make', 'trip', 'ptbo', 'hope', 'everything', 'good', 'babe', 'love', 'ya']\n",
            "After lowercasing: have you not finished work yet or something?\n",
            "After removing special chars: have you not finished work yet or something\n",
            "After tokenization: ['have', 'you', 'not', 'finished', 'work', 'yet', 'or', 'something']\n",
            "After stop word removal: ['finished', 'work', 'yet', 'something']\n",
            "After lemmatization: ['finished', 'work', 'yet', 'something']\n",
            "After lowercasing: tomorrow i am not going to theatre. . . so i can come wherever u call me. . . tell me where and when to come tomorrow\n",
            "After removing special chars: tomorrow i am not going to theatre   so i can come wherever u call me   tell me where and when to come tomorrow\n",
            "After tokenization: ['tomorrow', 'i', 'am', 'not', 'going', 'to', 'theatre', 'so', 'i', 'can', 'come', 'wherever', 'u', 'call', 'me', 'tell', 'me', 'where', 'and', 'when', 'to', 'come', 'tomorrow']\n",
            "After stop word removal: ['tomorrow', 'going', 'theatre', 'come', 'wherever', 'u', 'call', 'tell', 'come', 'tomorrow']\n",
            "After lemmatization: ['tomorrow', 'going', 'theatre', 'come', 'wherever', 'u', 'call', 'tell', 'come', 'tomorrow']\n",
            "After lowercasing: well done england! get the official poly ringtone or colour flag on yer mobile! text tone or flag to 84199 now! opt-out txt eng stop. box39822 w111wx å£1.50\n",
            "After removing special chars: well done england get the official poly ringtone or colour flag on yer mobile text tone or flag to  now optout txt eng stop box wwx \n",
            "After tokenization: ['well', 'done', 'england', 'get', 'the', 'official', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'text', 'tone', 'or', 'flag', 'to', 'now', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After stop word removal: ['well', 'done', 'england', 'get', 'official', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After lemmatization: ['well', 'done', 'england', 'get', 'official', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After lowercasing: right it wasnt you who phoned it was someone with a number like yours!\n",
            "After removing special chars: right it wasnt you who phoned it was someone with a number like yours\n",
            "After tokenization: ['right', 'it', 'wasnt', 'you', 'who', 'phoned', 'it', 'was', 'someone', 'with', 'a', 'number', 'like', 'yours']\n",
            "After stop word removal: ['right', 'wasnt', 'phoned', 'someone', 'number', 'like']\n",
            "After lemmatization: ['right', 'wasnt', 'phoned', 'someone', 'number', 'like']\n",
            "After lowercasing: it's ok i wun b angry. msg u aft i come home tonight.\n",
            "After removing special chars: its ok i wun b angry msg u aft i come home tonight\n",
            "After tokenization: ['its', 'ok', 'i', 'wun', 'b', 'angry', 'msg', 'u', 'aft', 'i', 'come', 'home', 'tonight']\n",
            "After stop word removal: ['ok', 'wun', 'b', 'angry', 'msg', 'u', 'aft', 'come', 'home', 'tonight']\n",
            "After lemmatization: ['ok', 'wun', 'b', 'angry', 'msg', 'u', 'aft', 'come', 'home', 'tonight']\n",
            "After lowercasing: i had a good time too. its nice to do something a bit different with my weekends for a change. see ya soon\n",
            "After removing special chars: i had a good time too its nice to do something a bit different with my weekends for a change see ya soon\n",
            "After tokenization: ['i', 'had', 'a', 'good', 'time', 'too', 'its', 'nice', 'to', 'do', 'something', 'a', 'bit', 'different', 'with', 'my', 'weekends', 'for', 'a', 'change', 'see', 'ya', 'soon']\n",
            "After stop word removal: ['good', 'time', 'nice', 'something', 'bit', 'different', 'weekends', 'change', 'see', 'ya', 'soon']\n",
            "After lemmatization: ['good', 'time', 'nice', 'something', 'bit', 'different', 'weekend', 'change', 'see', 'ya', 'soon']\n",
            "After lowercasing: yo sorry was in the shower sup\n",
            "After removing special chars: yo sorry was in the shower sup\n",
            "After tokenization: ['yo', 'sorry', 'was', 'in', 'the', 'shower', 'sup']\n",
            "After stop word removal: ['yo', 'sorry', 'shower', 'sup']\n",
            "After lemmatization: ['yo', 'sorry', 'shower', 'sup']\n",
            "After lowercasing: carlos is down but i have to pick it up from him, so i'll swing by usf in a little bit\n",
            "After removing special chars: carlos is down but i have to pick it up from him so ill swing by usf in a little bit\n",
            "After tokenization: ['carlos', 'is', 'down', 'but', 'i', 'have', 'to', 'pick', 'it', 'up', 'from', 'him', 'so', 'ill', 'swing', 'by', 'usf', 'in', 'a', 'little', 'bit']\n",
            "After stop word removal: ['carlos', 'pick', 'ill', 'swing', 'usf', 'little', 'bit']\n",
            "After lemmatization: ['carlos', 'pick', 'ill', 'swing', 'usf', 'little', 'bit']\n",
            "After lowercasing: full heat pa:-) i have applyed oil pa.\n",
            "After removing special chars: full heat pa i have applyed oil pa\n",
            "After tokenization: ['full', 'heat', 'pa', 'i', 'have', 'applyed', 'oil', 'pa']\n",
            "After stop word removal: ['full', 'heat', 'pa', 'applyed', 'oil', 'pa']\n",
            "After lemmatization: ['full', 'heat', 'pa', 'applyed', 'oil', 'pa']\n",
            "After lowercasing: i'm stuck in da middle of da row on da right hand side of da lt... \n",
            "After removing special chars: im stuck in da middle of da row on da right hand side of da lt \n",
            "After tokenization: ['im', 'stuck', 'in', 'da', 'middle', 'of', 'da', 'row', 'on', 'da', 'right', 'hand', 'side', 'of', 'da', 'lt']\n",
            "After stop word removal: ['im', 'stuck', 'da', 'middle', 'da', 'row', 'da', 'right', 'hand', 'side', 'da', 'lt']\n",
            "After lemmatization: ['im', 'stuck', 'da', 'middle', 'da', 'row', 'da', 'right', 'hand', 'side', 'da', 'lt']\n",
            "After lowercasing: have you laid your airtel line to rest?\n",
            "After removing special chars: have you laid your airtel line to rest\n",
            "After tokenization: ['have', 'you', 'laid', 'your', 'airtel', 'line', 'to', 'rest']\n",
            "After stop word removal: ['laid', 'airtel', 'line', 'rest']\n",
            "After lemmatization: ['laid', 'airtel', 'line', 'rest']\n",
            "After lowercasing: hi did u decide wot 2 get 4 his bday if not ill prob jus get him a voucher frm virgin or sumfing \n",
            "After removing special chars: hi did u decide wot  get  his bday if not ill prob jus get him a voucher frm virgin or sumfing \n",
            "After tokenization: ['hi', 'did', 'u', 'decide', 'wot', 'get', 'his', 'bday', 'if', 'not', 'ill', 'prob', 'jus', 'get', 'him', 'a', 'voucher', 'frm', 'virgin', 'or', 'sumfing']\n",
            "After stop word removal: ['hi', 'u', 'decide', 'wot', 'get', 'bday', 'ill', 'prob', 'jus', 'get', 'voucher', 'frm', 'virgin', 'sumfing']\n",
            "After lemmatization: ['hi', 'u', 'decide', 'wot', 'get', 'bday', 'ill', 'prob', 'jus', 'get', 'voucher', 'frm', 'virgin', 'sumfing']\n",
            "After lowercasing: freemsg: txt: call to no: 86888 & claim your reward of 3 hours talk time to use from your phone now! subscribe6gbp/mnth inc 3hrs 16 stop?txtstop\n",
            "After removing special chars: freemsg txt call to no   claim your reward of  hours talk time to use from your phone now subscribegbpmnth inc hrs  stoptxtstop\n",
            "After tokenization: ['freemsg', 'txt', 'call', 'to', 'no', 'claim', 'your', 'reward', 'of', 'hours', 'talk', 'time', 'to', 'use', 'from', 'your', 'phone', 'now', 'subscribegbpmnth', 'inc', 'hrs', 'stoptxtstop']\n",
            "After stop word removal: ['freemsg', 'txt', 'call', 'claim', 'reward', 'hours', 'talk', 'time', 'use', 'phone', 'subscribegbpmnth', 'inc', 'hrs', 'stoptxtstop']\n",
            "After lemmatization: ['freemsg', 'txt', 'call', 'claim', 'reward', 'hour', 'talk', 'time', 'use', 'phone', 'subscribegbpmnth', 'inc', 'hr', 'stoptxtstop']\n",
            "After lowercasing: \\hey j! r u feeling any better\n",
            "After removing special chars: hey j r u feeling any better\n",
            "After tokenization: ['hey', 'j', 'r', 'u', 'feeling', 'any', 'better']\n",
            "After stop word removal: ['hey', 'j', 'r', 'u', 'feeling', 'better']\n",
            "After lemmatization: ['hey', 'j', 'r', 'u', 'feeling', 'better']\n",
            "After lowercasing: and i don't plan on staying the night but i prolly won't be back til late\n",
            "After removing special chars: and i dont plan on staying the night but i prolly wont be back til late\n",
            "After tokenization: ['and', 'i', 'dont', 'plan', 'on', 'staying', 'the', 'night', 'but', 'i', 'prolly', 'wont', 'be', 'back', 'til', 'late']\n",
            "After stop word removal: ['dont', 'plan', 'staying', 'night', 'prolly', 'wont', 'back', 'til', 'late']\n",
            "After lemmatization: ['dont', 'plan', 'staying', 'night', 'prolly', 'wont', 'back', 'til', 'late']\n",
            "After lowercasing: thanx 4 puttin da fone down on me!!\n",
            "After removing special chars: thanx  puttin da fone down on me\n",
            "After tokenization: ['thanx', 'puttin', 'da', 'fone', 'down', 'on', 'me']\n",
            "After stop word removal: ['thanx', 'puttin', 'da', 'fone']\n",
            "After lemmatization: ['thanx', 'puttin', 'da', 'fone']\n",
            "After lowercasing: i need an 8th but i'm off campus atm, could i pick up in an hour or two?\n",
            "After removing special chars: i need an th but im off campus atm could i pick up in an hour or two\n",
            "After tokenization: ['i', 'need', 'an', 'th', 'but', 'im', 'off', 'campus', 'atm', 'could', 'i', 'pick', 'up', 'in', 'an', 'hour', 'or', 'two']\n",
            "After stop word removal: ['need', 'th', 'im', 'campus', 'atm', 'could', 'pick', 'hour', 'two']\n",
            "After lemmatization: ['need', 'th', 'im', 'campus', 'atm', 'could', 'pick', 'hour', 'two']\n",
            "After lowercasing: oh... haha... den we shld had went today too... gee, nvm la... kaiez, i dun mind goin jazz oso... scared hiphop open cant catch up... \n",
            "After removing special chars: oh haha den we shld had went today too gee nvm la kaiez i dun mind goin jazz oso scared hiphop open cant catch up \n",
            "After tokenization: ['oh', 'haha', 'den', 'we', 'shld', 'had', 'went', 'today', 'too', 'gee', 'nvm', 'la', 'kaiez', 'i', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scared', 'hiphop', 'open', 'cant', 'catch', 'up']\n",
            "After stop word removal: ['oh', 'haha', 'den', 'shld', 'went', 'today', 'gee', 'nvm', 'la', 'kaiez', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scared', 'hiphop', 'open', 'cant', 'catch']\n",
            "After lemmatization: ['oh', 'haha', 'den', 'shld', 'went', 'today', 'gee', 'nvm', 'la', 'kaiez', 'dun', 'mind', 'goin', 'jazz', 'oso', 'scared', 'hiphop', 'open', 'cant', 'catch']\n",
            "After lowercasing: been running but only managed 5 minutes and then needed oxygen! might have to resort to the roller option!\n",
            "After removing special chars: been running but only managed  minutes and then needed oxygen might have to resort to the roller option\n",
            "After tokenization: ['been', 'running', 'but', 'only', 'managed', 'minutes', 'and', 'then', 'needed', 'oxygen', 'might', 'have', 'to', 'resort', 'to', 'the', 'roller', 'option']\n",
            "After stop word removal: ['running', 'managed', 'minutes', 'needed', 'oxygen', 'might', 'resort', 'roller', 'option']\n",
            "After lemmatization: ['running', 'managed', 'minute', 'needed', 'oxygen', 'might', 'resort', 'roller', 'option']\n",
            "After lowercasing: we live in the next  &lt;#&gt; mins\n",
            "After removing special chars: we live in the next  ltgt mins\n",
            "After tokenization: ['we', 'live', 'in', 'the', 'next', 'ltgt', 'mins']\n",
            "After stop word removal: ['live', 'next', 'ltgt', 'mins']\n",
            "After lemmatization: ['live', 'next', 'ltgt', 'min']\n",
            "After lowercasing: y de asking like this.\n",
            "After removing special chars: y de asking like this\n",
            "After tokenization: ['y', 'de', 'asking', 'like', 'this']\n",
            "After stop word removal: ['de', 'asking', 'like']\n",
            "After lemmatization: ['de', 'asking', 'like']\n",
            "After lowercasing: just glad to be talking to you.\n",
            "After removing special chars: just glad to be talking to you\n",
            "After tokenization: ['just', 'glad', 'to', 'be', 'talking', 'to', 'you']\n",
            "After stop word removal: ['glad', 'talking']\n",
            "After lemmatization: ['glad', 'talking']\n",
            "After lowercasing: wat time ì_ finish?\n",
            "After removing special chars: wat time  finish\n",
            "After tokenization: ['wat', 'time', 'finish']\n",
            "After stop word removal: ['wat', 'time', 'finish']\n",
            "After lemmatization: ['wat', 'time', 'finish']\n",
            "After lowercasing: sorry da. i gone mad so many pending works what to do.\n",
            "After removing special chars: sorry da i gone mad so many pending works what to do\n",
            "After tokenization: ['sorry', 'da', 'i', 'gone', 'mad', 'so', 'many', 'pending', 'works', 'what', 'to', 'do']\n",
            "After stop word removal: ['sorry', 'da', 'gone', 'mad', 'many', 'pending', 'works']\n",
            "After lemmatization: ['sorry', 'da', 'gone', 'mad', 'many', 'pending', 'work']\n",
            "After lowercasing: how much you got for cleaning\n",
            "After removing special chars: how much you got for cleaning\n",
            "After tokenization: ['how', 'much', 'you', 'got', 'for', 'cleaning']\n",
            "After stop word removal: ['much', 'got', 'cleaning']\n",
            "After lemmatization: ['much', 'got', 'cleaning']\n",
            "After lowercasing: hows my favourite person today? r u workin hard? couldn't sleep again last nite nearly rang u at 4.30\n",
            "After removing special chars: hows my favourite person today r u workin hard couldnt sleep again last nite nearly rang u at \n",
            "After tokenization: ['hows', 'my', 'favourite', 'person', 'today', 'r', 'u', 'workin', 'hard', 'couldnt', 'sleep', 'again', 'last', 'nite', 'nearly', 'rang', 'u', 'at']\n",
            "After stop word removal: ['hows', 'favourite', 'person', 'today', 'r', 'u', 'workin', 'hard', 'couldnt', 'sleep', 'last', 'nite', 'nearly', 'rang', 'u']\n",
            "After lemmatization: ['hows', 'favourite', 'person', 'today', 'r', 'u', 'workin', 'hard', 'couldnt', 'sleep', 'last', 'nite', 'nearly', 'rang', 'u']\n",
            "After lowercasing: sunshine quiz! win a super sony dvd recorder if you canname the capital of australia? text mquiz to 82277. b\n",
            "After removing special chars: sunshine quiz win a super sony dvd recorder if you canname the capital of australia text mquiz to  b\n",
            "After tokenization: ['sunshine', 'quiz', 'win', 'a', 'super', 'sony', 'dvd', 'recorder', 'if', 'you', 'canname', 'the', 'capital', 'of', 'australia', 'text', 'mquiz', 'to', 'b']\n",
            "After stop word removal: ['sunshine', 'quiz', 'win', 'super', 'sony', 'dvd', 'recorder', 'canname', 'capital', 'australia', 'text', 'mquiz', 'b']\n",
            "After lemmatization: ['sunshine', 'quiz', 'win', 'super', 'sony', 'dvd', 'recorder', 'canname', 'capital', 'australia', 'text', 'mquiz', 'b']\n",
            "After lowercasing: ìï called dad oredi...\n",
            "After removing special chars:  called dad oredi\n",
            "After tokenization: ['called', 'dad', 'oredi']\n",
            "After stop word removal: ['called', 'dad', 'oredi']\n",
            "After lemmatization: ['called', 'dad', 'oredi']\n",
            "After lowercasing: good. do you think you could send me some pix? i would love to see your top and bottom...\n",
            "After removing special chars: good do you think you could send me some pix i would love to see your top and bottom\n",
            "After tokenization: ['good', 'do', 'you', 'think', 'you', 'could', 'send', 'me', 'some', 'pix', 'i', 'would', 'love', 'to', 'see', 'your', 'top', 'and', 'bottom']\n",
            "After stop word removal: ['good', 'think', 'could', 'send', 'pix', 'would', 'love', 'see', 'top', 'bottom']\n",
            "After lemmatization: ['good', 'think', 'could', 'send', 'pix', 'would', 'love', 'see', 'top', 'bottom']\n",
            "After lowercasing: nvm... i'm going to wear my sport shoes anyway... i'm going to be late leh.\n",
            "After removing special chars: nvm im going to wear my sport shoes anyway im going to be late leh\n",
            "After tokenization: ['nvm', 'im', 'going', 'to', 'wear', 'my', 'sport', 'shoes', 'anyway', 'im', 'going', 'to', 'be', 'late', 'leh']\n",
            "After stop word removal: ['nvm', 'im', 'going', 'wear', 'sport', 'shoes', 'anyway', 'im', 'going', 'late', 'leh']\n",
            "After lemmatization: ['nvm', 'im', 'going', 'wear', 'sport', 'shoe', 'anyway', 'im', 'going', 'late', 'leh']\n",
            "After lowercasing: sorry, i'll call later in meeting.\n",
            "After removing special chars: sorry ill call later in meeting\n",
            "After tokenization: ['sorry', 'ill', 'call', 'later', 'in', 'meeting']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later', 'meeting']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later', 'meeting']\n",
            "After lowercasing: this is a long fuckin showr\n",
            "After removing special chars: this is a long fuckin showr\n",
            "After tokenization: ['this', 'is', 'a', 'long', 'fuckin', 'showr']\n",
            "After stop word removal: ['long', 'fuckin', 'showr']\n",
            "After lemmatization: ['long', 'fuckin', 'showr']\n",
            "After lowercasing: received, understood n acted upon!\n",
            "After removing special chars: received understood n acted upon\n",
            "After tokenization: ['received', 'understood', 'n', 'acted', 'upon']\n",
            "After stop word removal: ['received', 'understood', 'n', 'acted', 'upon']\n",
            "After lemmatization: ['received', 'understood', 'n', 'acted', 'upon']\n",
            "After lowercasing: they finally came to fix the ceiling.\n",
            "After removing special chars: they finally came to fix the ceiling\n",
            "After tokenization: ['they', 'finally', 'came', 'to', 'fix', 'the', 'ceiling']\n",
            "After stop word removal: ['finally', 'came', 'fix', 'ceiling']\n",
            "After lemmatization: ['finally', 'came', 'fix', 'ceiling']\n",
            "After lowercasing: u need my presnts always bcz u cant mis love. \\jeevithathile irulinae neekunna prakasamanu sneham\\\" prakasam ennal prabha 'that mns prabha is'love' got it. dont mis me....\"\n",
            "After removing special chars: u need my presnts always bcz u cant mis love jeevithathile irulinae neekunna prakasamanu sneham prakasam ennal prabha that mns prabha islove got it dont mis me\n",
            "After tokenization: ['u', 'need', 'my', 'presnts', 'always', 'bcz', 'u', 'cant', 'mis', 'love', 'jeevithathile', 'irulinae', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'that', 'mns', 'prabha', 'islove', 'got', 'it', 'dont', 'mis', 'me']\n",
            "After stop word removal: ['u', 'need', 'presnts', 'always', 'bcz', 'u', 'cant', 'mis', 'love', 'jeevithathile', 'irulinae', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'mns', 'prabha', 'islove', 'got', 'dont', 'mis']\n",
            "After lemmatization: ['u', 'need', 'presnts', 'always', 'bcz', 'u', 'cant', 'mi', 'love', 'jeevithathile', 'irulinae', 'neekunna', 'prakasamanu', 'sneham', 'prakasam', 'ennal', 'prabha', 'mn', 'prabha', 'islove', 'got', 'dont', 'mi']\n",
            "After lowercasing: jus finish blowing my hair. u finish dinner already?\n",
            "After removing special chars: jus finish blowing my hair u finish dinner already\n",
            "After tokenization: ['jus', 'finish', 'blowing', 'my', 'hair', 'u', 'finish', 'dinner', 'already']\n",
            "After stop word removal: ['jus', 'finish', 'blowing', 'hair', 'u', 'finish', 'dinner', 'already']\n",
            "After lemmatization: ['jus', 'finish', 'blowing', 'hair', 'u', 'finish', 'dinner', 'already']\n",
            "After lowercasing: i'm on the bus. love you\n",
            "After removing special chars: im on the bus love you\n",
            "After tokenization: ['im', 'on', 'the', 'bus', 'love', 'you']\n",
            "After stop word removal: ['im', 'bus', 'love']\n",
            "After lemmatization: ['im', 'bus', 'love']\n",
            "After lowercasing: lol ... i knew that .... i saw him in the dollar store\n",
            "After removing special chars: lol  i knew that  i saw him in the dollar store\n",
            "After tokenization: ['lol', 'i', 'knew', 'that', 'i', 'saw', 'him', 'in', 'the', 'dollar', 'store']\n",
            "After stop word removal: ['lol', 'knew', 'saw', 'dollar', 'store']\n",
            "After lemmatization: ['lol', 'knew', 'saw', 'dollar', 'store']\n",
            "After lowercasing: please call our customer service representative on 0800 169 6031 between 10am-9pm as you have won a guaranteed å£1000 cash or å£5000 prize!\n",
            "After removing special chars: please call our customer service representative on    between ampm as you have won a guaranteed  cash or  prize\n",
            "After tokenization: ['please', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'ampm', 'as', 'you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'prize']\n",
            "After stop word removal: ['please', 'call', 'customer', 'service', 'representative', 'ampm', 'guaranteed', 'cash', 'prize']\n",
            "After lemmatization: ['please', 'call', 'customer', 'service', 'representative', 'ampm', 'guaranteed', 'cash', 'prize']\n",
            "After lowercasing: todays voda numbers ending with 7634 are selected to receive a å£350 reward. if you have a match please call 08712300220 quoting claim code 7684 standard rates apply.\n",
            "After removing special chars: todays voda numbers ending with  are selected to receive a  reward if you have a match please call  quoting claim code  standard rates apply\n",
            "After tokenization: ['todays', 'voda', 'numbers', 'ending', 'with', 'are', 'selected', 'to', 'receive', 'a', 'reward', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'apply']\n",
            "After stop word removal: ['todays', 'voda', 'numbers', 'ending', 'selected', 'receive', 'reward', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'apply']\n",
            "After lemmatization: ['today', 'voda', 'number', 'ending', 'selected', 'receive', 'reward', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rate', 'apply']\n",
            "After lowercasing: only saturday and sunday holiday so its very difficult:)\n",
            "After removing special chars: only saturday and sunday holiday so its very difficult\n",
            "After tokenization: ['only', 'saturday', 'and', 'sunday', 'holiday', 'so', 'its', 'very', 'difficult']\n",
            "After stop word removal: ['saturday', 'sunday', 'holiday', 'difficult']\n",
            "After lemmatization: ['saturday', 'sunday', 'holiday', 'difficult']\n",
            "After lowercasing: everybody had fun this evening. miss you.\n",
            "After removing special chars: everybody had fun this evening miss you\n",
            "After tokenization: ['everybody', 'had', 'fun', 'this', 'evening', 'miss', 'you']\n",
            "After stop word removal: ['everybody', 'fun', 'evening', 'miss']\n",
            "After lemmatization: ['everybody', 'fun', 'evening', 'miss']\n",
            "After lowercasing: got hella gas money, want to go on a grand nature adventure with galileo in a little bit?\n",
            "After removing special chars: got hella gas money want to go on a grand nature adventure with galileo in a little bit\n",
            "After tokenization: ['got', 'hella', 'gas', 'money', 'want', 'to', 'go', 'on', 'a', 'grand', 'nature', 'adventure', 'with', 'galileo', 'in', 'a', 'little', 'bit']\n",
            "After stop word removal: ['got', 'hella', 'gas', 'money', 'want', 'go', 'grand', 'nature', 'adventure', 'galileo', 'little', 'bit']\n",
            "After lemmatization: ['got', 'hella', 'gas', 'money', 'want', 'go', 'grand', 'nature', 'adventure', 'galileo', 'little', 'bit']\n",
            "After lowercasing: i'm in a meeting, call me later at\n",
            "After removing special chars: im in a meeting call me later at\n",
            "After tokenization: ['im', 'in', 'a', 'meeting', 'call', 'me', 'later', 'at']\n",
            "After stop word removal: ['im', 'meeting', 'call', 'later']\n",
            "After lemmatization: ['im', 'meeting', 'call', 'later']\n",
            "After lowercasing: oh wow thats gay. will firmware update help\n",
            "After removing special chars: oh wow thats gay will firmware update help\n",
            "After tokenization: ['oh', 'wow', 'thats', 'gay', 'will', 'firmware', 'update', 'help']\n",
            "After stop word removal: ['oh', 'wow', 'thats', 'gay', 'firmware', 'update', 'help']\n",
            "After lemmatization: ['oh', 'wow', 'thats', 'gay', 'firmware', 'update', 'help']\n",
            "After lowercasing: these won't do. have to move on to morphine\n",
            "After removing special chars: these wont do have to move on to morphine\n",
            "After tokenization: ['these', 'wont', 'do', 'have', 'to', 'move', 'on', 'to', 'morphine']\n",
            "After stop word removal: ['wont', 'move', 'morphine']\n",
            "After lemmatization: ['wont', 'move', 'morphine']\n",
            "After lowercasing: how come i din c ì_... yup i cut my hair...\n",
            "After removing special chars: how come i din c  yup i cut my hair\n",
            "After tokenization: ['how', 'come', 'i', 'din', 'c', 'yup', 'i', 'cut', 'my', 'hair']\n",
            "After stop word removal: ['come', 'din', 'c', 'yup', 'cut', 'hair']\n",
            "After lemmatization: ['come', 'din', 'c', 'yup', 'cut', 'hair']\n",
            "After lowercasing: k k pa had your lunch aha.\n",
            "After removing special chars: k k pa had your lunch aha\n",
            "After tokenization: ['k', 'k', 'pa', 'had', 'your', 'lunch', 'aha']\n",
            "After stop word removal: ['k', 'k', 'pa', 'lunch', 'aha']\n",
            "After lemmatization: ['k', 'k', 'pa', 'lunch', 'aha']\n",
            "After lowercasing: oh ho. is this the first time u use these type of words\n",
            "After removing special chars: oh ho is this the first time u use these type of words\n",
            "After tokenization: ['oh', 'ho', 'is', 'this', 'the', 'first', 'time', 'u', 'use', 'these', 'type', 'of', 'words']\n",
            "After stop word removal: ['oh', 'ho', 'first', 'time', 'u', 'use', 'type', 'words']\n",
            "After lemmatization: ['oh', 'ho', 'first', 'time', 'u', 'use', 'type', 'word']\n",
            "After lowercasing: captain vijaykanth is doing comedy in captain tv..he is drunken :)\n",
            "After removing special chars: captain vijaykanth is doing comedy in captain tvhe is drunken \n",
            "After tokenization: ['captain', 'vijaykanth', 'is', 'doing', 'comedy', 'in', 'captain', 'tvhe', 'is', 'drunken']\n",
            "After stop word removal: ['captain', 'vijaykanth', 'comedy', 'captain', 'tvhe', 'drunken']\n",
            "After lemmatization: ['captain', 'vijaykanth', 'comedy', 'captain', 'tvhe', 'drunken']\n",
            "After lowercasing: of course. i guess god's just got me on hold right now.\n",
            "After removing special chars: of course i guess gods just got me on hold right now\n",
            "After tokenization: ['of', 'course', 'i', 'guess', 'gods', 'just', 'got', 'me', 'on', 'hold', 'right', 'now']\n",
            "After stop word removal: ['course', 'guess', 'gods', 'got', 'hold', 'right']\n",
            "After lemmatization: ['course', 'guess', 'god', 'got', 'hold', 'right']\n",
            "After lowercasing: do you hide anythiing or keeping distance from me\n",
            "After removing special chars: do you hide anythiing or keeping distance from me\n",
            "After tokenization: ['do', 'you', 'hide', 'anythiing', 'or', 'keeping', 'distance', 'from', 'me']\n",
            "After stop word removal: ['hide', 'anythiing', 'keeping', 'distance']\n",
            "After lemmatization: ['hide', 'anythiing', 'keeping', 'distance']\n",
            "After lowercasing: havent.\n",
            "After removing special chars: havent\n",
            "After tokenization: ['havent']\n",
            "After stop word removal: ['havent']\n",
            "After lemmatization: ['havent']\n",
            "After lowercasing: you are being ripped off! get your mobile content from www.clubmoby.com call 08717509990 poly/true/pix/ringtones/games six downloads for only 3\n",
            "After removing special chars: you are being ripped off get your mobile content from wwwclubmobycom call  polytruepixringtonesgames six downloads for only \n",
            "After tokenization: ['you', 'are', 'being', 'ripped', 'off', 'get', 'your', 'mobile', 'content', 'from', 'wwwclubmobycom', 'call', 'polytruepixringtonesgames', 'six', 'downloads', 'for', 'only']\n",
            "After stop word removal: ['ripped', 'get', 'mobile', 'content', 'wwwclubmobycom', 'call', 'polytruepixringtonesgames', 'six', 'downloads']\n",
            "After lemmatization: ['ripped', 'get', 'mobile', 'content', 'wwwclubmobycom', 'call', 'polytruepixringtonesgames', 'six', 'downloads']\n",
            "After lowercasing: sorry i din lock my keypad.\n",
            "After removing special chars: sorry i din lock my keypad\n",
            "After tokenization: ['sorry', 'i', 'din', 'lock', 'my', 'keypad']\n",
            "After stop word removal: ['sorry', 'din', 'lock', 'keypad']\n",
            "After lemmatization: ['sorry', 'din', 'lock', 'keypad']\n",
            "After lowercasing: did u got that persons story\n",
            "After removing special chars: did u got that persons story\n",
            "After tokenization: ['did', 'u', 'got', 'that', 'persons', 'story']\n",
            "After stop word removal: ['u', 'got', 'persons', 'story']\n",
            "After lemmatization: ['u', 'got', 'person', 'story']\n",
            "After lowercasing: are you planning to come chennai?\n",
            "After removing special chars: are you planning to come chennai\n",
            "After tokenization: ['are', 'you', 'planning', 'to', 'come', 'chennai']\n",
            "After stop word removal: ['planning', 'come', 'chennai']\n",
            "After lemmatization: ['planning', 'come', 'chennai']\n",
            "After lowercasing: we tried to contact you re your reply to our offer of a video phone 750 anytime any network mins half price line rental camcorder reply or call 08000930705\n",
            "After removing special chars: we tried to contact you re your reply to our offer of a video phone  anytime any network mins half price line rental camcorder reply or call \n",
            "After tokenization: ['we', 'tried', 'to', 'contact', 'you', 're', 'your', 'reply', 'to', 'our', 'offer', 'of', 'a', 'video', 'phone', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'or', 'call']\n",
            "After stop word removal: ['tried', 'contact', 'reply', 'offer', 'video', 'phone', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'call']\n",
            "After lemmatization: ['tried', 'contact', 'reply', 'offer', 'video', 'phone', 'anytime', 'network', 'min', 'half', 'price', 'line', 'rental', 'camcorder', 'reply', 'call']\n",
            "After lowercasing: god created gap btwn ur fingers so dat sum1 vry special will fill those gaps by holding ur hands.. now plz dont ask y he created so much gap between legs !!!\n",
            "After removing special chars: god created gap btwn ur fingers so dat sum vry special will fill those gaps by holding ur hands now plz dont ask y he created so much gap between legs \n",
            "After tokenization: ['god', 'created', 'gap', 'btwn', 'ur', 'fingers', 'so', 'dat', 'sum', 'vry', 'special', 'will', 'fill', 'those', 'gaps', 'by', 'holding', 'ur', 'hands', 'now', 'plz', 'dont', 'ask', 'y', 'he', 'created', 'so', 'much', 'gap', 'between', 'legs']\n",
            "After stop word removal: ['god', 'created', 'gap', 'btwn', 'ur', 'fingers', 'dat', 'sum', 'vry', 'special', 'fill', 'gaps', 'holding', 'ur', 'hands', 'plz', 'dont', 'ask', 'created', 'much', 'gap', 'legs']\n",
            "After lemmatization: ['god', 'created', 'gap', 'btwn', 'ur', 'finger', 'dat', 'sum', 'vry', 'special', 'fill', 'gap', 'holding', 'ur', 'hand', 'plz', 'dont', 'ask', 'created', 'much', 'gap', 'leg']\n",
            "After lowercasing: we are okay. going to sleep now. later\n",
            "After removing special chars: we are okay going to sleep now later\n",
            "After tokenization: ['we', 'are', 'okay', 'going', 'to', 'sleep', 'now', 'later']\n",
            "After stop word removal: ['okay', 'going', 'sleep', 'later']\n",
            "After lemmatization: ['okay', 'going', 'sleep', 'later']\n",
            "After lowercasing: please protect yourself from e-threats. sib never asks for sensitive information like passwords,atm/sms pin thru email. never share your password with anybody.\n",
            "After removing special chars: please protect yourself from ethreats sib never asks for sensitive information like passwordsatmsms pin thru email never share your password with anybody\n",
            "After tokenization: ['please', 'protect', 'yourself', 'from', 'ethreats', 'sib', 'never', 'asks', 'for', 'sensitive', 'information', 'like', 'passwordsatmsms', 'pin', 'thru', 'email', 'never', 'share', 'your', 'password', 'with', 'anybody']\n",
            "After stop word removal: ['please', 'protect', 'ethreats', 'sib', 'never', 'asks', 'sensitive', 'information', 'like', 'passwordsatmsms', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybody']\n",
            "After lemmatization: ['please', 'protect', 'ethreats', 'sib', 'never', 'asks', 'sensitive', 'information', 'like', 'passwordsatmsms', 'pin', 'thru', 'email', 'never', 'share', 'password', 'anybody']\n",
            "After lowercasing: finally it has happened..! aftr decades..! beer is now cheaper than petrol! the goverment expects us to \\drink\\\". . . but don't \\\"drive \\\"\"\n",
            "After removing special chars: finally it has happened aftr decades beer is now cheaper than petrol the goverment expects us to drink   but dont drive \n",
            "After tokenization: ['finally', 'it', 'has', 'happened', 'aftr', 'decades', 'beer', 'is', 'now', 'cheaper', 'than', 'petrol', 'the', 'goverment', 'expects', 'us', 'to', 'drink', 'but', 'dont', 'drive']\n",
            "After stop word removal: ['finally', 'happened', 'aftr', 'decades', 'beer', 'cheaper', 'petrol', 'goverment', 'expects', 'us', 'drink', 'dont', 'drive']\n",
            "After lemmatization: ['finally', 'happened', 'aftr', 'decade', 'beer', 'cheaper', 'petrol', 'goverment', 'expects', 'u', 'drink', 'dont', 'drive']\n",
            "After lowercasing: a å£400 xmas reward is waiting for you! our computer has randomly picked you from our loyal mobile customers to receive a å£400 reward. just call 09066380611 \n",
            "After removing special chars: a  xmas reward is waiting for you our computer has randomly picked you from our loyal mobile customers to receive a  reward just call  \n",
            "After tokenization: ['a', 'xmas', 'reward', 'is', 'waiting', 'for', 'you', 'our', 'computer', 'has', 'randomly', 'picked', 'you', 'from', 'our', 'loyal', 'mobile', 'customers', 'to', 'receive', 'a', 'reward', 'just', 'call']\n",
            "After stop word removal: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customers', 'receive', 'reward', 'call']\n",
            "After lemmatization: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customer', 'receive', 'reward', 'call']\n",
            "After lowercasing: where r e meeting tmr?\n",
            "After removing special chars: where r e meeting tmr\n",
            "After tokenization: ['where', 'r', 'e', 'meeting', 'tmr']\n",
            "After stop word removal: ['r', 'e', 'meeting', 'tmr']\n",
            "After lemmatization: ['r', 'e', 'meeting', 'tmr']\n",
            "After lowercasing: lol yes. but it will add some spice to your day.\n",
            "After removing special chars: lol yes but it will add some spice to your day\n",
            "After tokenization: ['lol', 'yes', 'but', 'it', 'will', 'add', 'some', 'spice', 'to', 'your', 'day']\n",
            "After stop word removal: ['lol', 'yes', 'add', 'spice', 'day']\n",
            "After lemmatization: ['lol', 'yes', 'add', 'spice', 'day']\n",
            "After lowercasing: hope you are having a great day.\n",
            "After removing special chars: hope you are having a great day\n",
            "After tokenization: ['hope', 'you', 'are', 'having', 'a', 'great', 'day']\n",
            "After stop word removal: ['hope', 'great', 'day']\n",
            "After lemmatization: ['hope', 'great', 'day']\n",
            "After lowercasing: our prasanth ettans mother passed away last night. just pray for her and family.\n",
            "After removing special chars: our prasanth ettans mother passed away last night just pray for her and family\n",
            "After tokenization: ['our', 'prasanth', 'ettans', 'mother', 'passed', 'away', 'last', 'night', 'just', 'pray', 'for', 'her', 'and', 'family']\n",
            "After stop word removal: ['prasanth', 'ettans', 'mother', 'passed', 'away', 'last', 'night', 'pray', 'family']\n",
            "After lemmatization: ['prasanth', 'ettans', 'mother', 'passed', 'away', 'last', 'night', 'pray', 'family']\n",
            "After lowercasing: k, i'll work something out\n",
            "After removing special chars: k ill work something out\n",
            "After tokenization: ['k', 'ill', 'work', 'something', 'out']\n",
            "After stop word removal: ['k', 'ill', 'work', 'something']\n",
            "After lemmatization: ['k', 'ill', 'work', 'something']\n",
            "After lowercasing: private! your 2003 account statement for shows 800 un-redeemed s. i. m. points. call 08718738002 identifier code: 48922 expires 21/11/04\n",
            "After removing special chars: private your  account statement for shows  unredeemed s i m points call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'point', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: this message is from a great doctor in india:-): 1) do not drink appy fizz. it contains cancer causing age\n",
            "After removing special chars: this message is from a great doctor in india  do not drink appy fizz it contains cancer causing age\n",
            "After tokenization: ['this', 'message', 'is', 'from', 'a', 'great', 'doctor', 'in', 'india', 'do', 'not', 'drink', 'appy', 'fizz', 'it', 'contains', 'cancer', 'causing', 'age']\n",
            "After stop word removal: ['message', 'great', 'doctor', 'india', 'drink', 'appy', 'fizz', 'contains', 'cancer', 'causing', 'age']\n",
            "After lemmatization: ['message', 'great', 'doctor', 'india', 'drink', 'appy', 'fizz', 'contains', 'cancer', 'causing', 'age']\n",
            "After lowercasing: i cant pick the phone right now. pls send a message\n",
            "After removing special chars: i cant pick the phone right now pls send a message\n",
            "After tokenization: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
            "After stop word removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lemmatization: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lowercasing: you call him and tell now infront of them. call him now.\n",
            "After removing special chars: you call him and tell now infront of them call him now\n",
            "After tokenization: ['you', 'call', 'him', 'and', 'tell', 'now', 'infront', 'of', 'them', 'call', 'him', 'now']\n",
            "After stop word removal: ['call', 'tell', 'infront', 'call']\n",
            "After lemmatization: ['call', 'tell', 'infront', 'call']\n",
            "After lowercasing: ok no prob...\n",
            "After removing special chars: ok no prob\n",
            "After tokenization: ['ok', 'no', 'prob']\n",
            "After stop word removal: ['ok', 'prob']\n",
            "After lemmatization: ['ok', 'prob']\n",
            "After lowercasing: ladies first and genus second k .\n",
            "After removing special chars: ladies first and genus second k \n",
            "After tokenization: ['ladies', 'first', 'and', 'genus', 'second', 'k']\n",
            "After stop word removal: ['ladies', 'first', 'genus', 'second', 'k']\n",
            "After lemmatization: ['lady', 'first', 'genus', 'second', 'k']\n",
            "After lowercasing: no. yes please. been swimming?\n",
            "After removing special chars: no yes please been swimming\n",
            "After tokenization: ['no', 'yes', 'please', 'been', 'swimming']\n",
            "After stop word removal: ['yes', 'please', 'swimming']\n",
            "After lemmatization: ['yes', 'please', 'swimming']\n",
            "After lowercasing: mum not going robinson already.\n",
            "After removing special chars: mum not going robinson already\n",
            "After tokenization: ['mum', 'not', 'going', 'robinson', 'already']\n",
            "After stop word removal: ['mum', 'going', 'robinson', 'already']\n",
            "After lemmatization: ['mum', 'going', 'robinson', 'already']\n",
            "After lowercasing: ok set let u noe e details later...\n",
            "After removing special chars: ok set let u noe e details later\n",
            "After tokenization: ['ok', 'set', 'let', 'u', 'noe', 'e', 'details', 'later']\n",
            "After stop word removal: ['ok', 'set', 'let', 'u', 'noe', 'e', 'details', 'later']\n",
            "After lemmatization: ['ok', 'set', 'let', 'u', 'noe', 'e', 'detail', 'later']\n",
            "After lowercasing: not..tel software name..\n",
            "After removing special chars: nottel software name\n",
            "After tokenization: ['nottel', 'software', 'name']\n",
            "After stop word removal: ['nottel', 'software', 'name']\n",
            "After lemmatization: ['nottel', 'software', 'name']\n",
            "After lowercasing: i send the print  outs da.\n",
            "After removing special chars: i send the print  outs da\n",
            "After tokenization: ['i', 'send', 'the', 'print', 'outs', 'da']\n",
            "After stop word removal: ['send', 'print', 'outs', 'da']\n",
            "After lemmatization: ['send', 'print', 'out', 'da']\n",
            "After lowercasing: im realy soz imat my mums 2nite what about 2moro \n",
            "After removing special chars: im realy soz imat my mums nite what about moro \n",
            "After tokenization: ['im', 'realy', 'soz', 'imat', 'my', 'mums', 'nite', 'what', 'about', 'moro']\n",
            "After stop word removal: ['im', 'realy', 'soz', 'imat', 'mums', 'nite', 'moro']\n",
            "After lemmatization: ['im', 'realy', 'soz', 'imat', 'mum', 'nite', 'moro']\n",
            "After lowercasing: when i was born, god said, \\oh no! another idiot\\\". when you were born\n",
            "After removing special chars: when i was born god said oh no another idiot when you were born\n",
            "After tokenization: ['when', 'i', 'was', 'born', 'god', 'said', 'oh', 'no', 'another', 'idiot', 'when', 'you', 'were', 'born']\n",
            "After stop word removal: ['born', 'god', 'said', 'oh', 'another', 'idiot', 'born']\n",
            "After lemmatization: ['born', 'god', 'said', 'oh', 'another', 'idiot', 'born']\n",
            "After lowercasing: i didnt get ur full msg..sometext is missing, send it again\n",
            "After removing special chars: i didnt get ur full msgsometext is missing send it again\n",
            "After tokenization: ['i', 'didnt', 'get', 'ur', 'full', 'msgsometext', 'is', 'missing', 'send', 'it', 'again']\n",
            "After stop word removal: ['didnt', 'get', 'ur', 'full', 'msgsometext', 'missing', 'send']\n",
            "After lemmatization: ['didnt', 'get', 'ur', 'full', 'msgsometext', 'missing', 'send']\n",
            "After lowercasing: probably not, i'm almost out of gas and i get some cash tomorrow\n",
            "After removing special chars: probably not im almost out of gas and i get some cash tomorrow\n",
            "After tokenization: ['probably', 'not', 'im', 'almost', 'out', 'of', 'gas', 'and', 'i', 'get', 'some', 'cash', 'tomorrow']\n",
            "After stop word removal: ['probably', 'im', 'almost', 'gas', 'get', 'cash', 'tomorrow']\n",
            "After lemmatization: ['probably', 'im', 'almost', 'gas', 'get', 'cash', 'tomorrow']\n",
            "After lowercasing: customer service announcement. we recently tried to make a delivery to you but were unable to do so, please call 07099833605 to re-schedule. ref:9280114\n",
            "After removing special chars: customer service announcement we recently tried to make a delivery to you but were unable to do so please call  to reschedule ref\n",
            "After tokenization: ['customer', 'service', 'announcement', 'we', 'recently', 'tried', 'to', 'make', 'a', 'delivery', 'to', 'you', 'but', 'were', 'unable', 'to', 'do', 'so', 'please', 'call', 'to', 'reschedule', 'ref']\n",
            "After stop word removal: ['customer', 'service', 'announcement', 'recently', 'tried', 'make', 'delivery', 'unable', 'please', 'call', 'reschedule', 'ref']\n",
            "After lemmatization: ['customer', 'service', 'announcement', 'recently', 'tried', 'make', 'delivery', 'unable', 'please', 'call', 'reschedule', 'ref']\n",
            "After lowercasing: i forgot 2 ask ì_ all smth.. there's a card on da present lei... how? ìï all want 2 write smth or sign on it?\n",
            "After removing special chars: i forgot  ask  all smth theres a card on da present lei how  all want  write smth or sign on it\n",
            "After tokenization: ['i', 'forgot', 'ask', 'all', 'smth', 'theres', 'a', 'card', 'on', 'da', 'present', 'lei', 'how', 'all', 'want', 'write', 'smth', 'or', 'sign', 'on', 'it']\n",
            "After stop word removal: ['forgot', 'ask', 'smth', 'theres', 'card', 'da', 'present', 'lei', 'want', 'write', 'smth', 'sign']\n",
            "After lemmatization: ['forgot', 'ask', 'smth', 'there', 'card', 'da', 'present', 'lei', 'want', 'write', 'smth', 'sign']\n",
            "After lowercasing: i'm leaving my house now.\n",
            "After removing special chars: im leaving my house now\n",
            "After tokenization: ['im', 'leaving', 'my', 'house', 'now']\n",
            "After stop word removal: ['im', 'leaving', 'house']\n",
            "After lemmatization: ['im', 'leaving', 'house']\n",
            "After lowercasing: hi babe its chloe, how r u? i was smashed on saturday night, it was great! how was your weekend? u been missing me? sp visionsms.com text stop to stop 150p/text\n",
            "After removing special chars: hi babe its chloe how r u i was smashed on saturday night it was great how was your weekend u been missing me sp visionsmscom text stop to stop ptext\n",
            "After tokenization: ['hi', 'babe', 'its', 'chloe', 'how', 'r', 'u', 'i', 'was', 'smashed', 'on', 'saturday', 'night', 'it', 'was', 'great', 'how', 'was', 'your', 'weekend', 'u', 'been', 'missing', 'me', 'sp', 'visionsmscom', 'text', 'stop', 'to', 'stop', 'ptext']\n",
            "After stop word removal: ['hi', 'babe', 'chloe', 'r', 'u', 'smashed', 'saturday', 'night', 'great', 'weekend', 'u', 'missing', 'sp', 'visionsmscom', 'text', 'stop', 'stop', 'ptext']\n",
            "After lemmatization: ['hi', 'babe', 'chloe', 'r', 'u', 'smashed', 'saturday', 'night', 'great', 'weekend', 'u', 'missing', 'sp', 'visionsmscom', 'text', 'stop', 'stop', 'ptext']\n",
            "After lowercasing: ìï ready then call me...\n",
            "After removing special chars:  ready then call me\n",
            "After tokenization: ['ready', 'then', 'call', 'me']\n",
            "After stop word removal: ['ready', 'call']\n",
            "After lemmatization: ['ready', 'call']\n",
            "After lowercasing: wewa is 130. iriver 255. all 128 mb.\n",
            "After removing special chars: wewa is  iriver  all  mb\n",
            "After tokenization: ['wewa', 'is', 'iriver', 'all', 'mb']\n",
            "After stop word removal: ['wewa', 'iriver', 'mb']\n",
            "After lemmatization: ['wewa', 'iriver', 'mb']\n",
            "After lowercasing: it is a good thing i'm now getting the connection to bw\n",
            "After removing special chars: it is a good thing im now getting the connection to bw\n",
            "After tokenization: ['it', 'is', 'a', 'good', 'thing', 'im', 'now', 'getting', 'the', 'connection', 'to', 'bw']\n",
            "After stop word removal: ['good', 'thing', 'im', 'getting', 'connection', 'bw']\n",
            "After lemmatization: ['good', 'thing', 'im', 'getting', 'connection', 'bw']\n",
            "After lowercasing: sry da..jst nw only i came to home..\n",
            "After removing special chars: sry dajst nw only i came to home\n",
            "After tokenization: ['sry', 'dajst', 'nw', 'only', 'i', 'came', 'to', 'home']\n",
            "After stop word removal: ['sry', 'dajst', 'nw', 'came', 'home']\n",
            "After lemmatization: ['sry', 'dajst', 'nw', 'came', 'home']\n",
            "After lowercasing: that's cool he'll be here all night, lemme know when you're around\n",
            "After removing special chars: thats cool hell be here all night lemme know when youre around\n",
            "After tokenization: ['thats', 'cool', 'hell', 'be', 'here', 'all', 'night', 'lem', 'me', 'know', 'when', 'youre', 'around']\n",
            "After stop word removal: ['thats', 'cool', 'hell', 'night', 'lem', 'know', 'youre', 'around']\n",
            "After lemmatization: ['thats', 'cool', 'hell', 'night', 'lem', 'know', 'youre', 'around']\n",
            "After lowercasing: are you staying in town ?\n",
            "After removing special chars: are you staying in town \n",
            "After tokenization: ['are', 'you', 'staying', 'in', 'town']\n",
            "After stop word removal: ['staying', 'town']\n",
            "After lemmatization: ['staying', 'town']\n",
            "After lowercasing: haha yeah, 2 oz is kind of a shitload\n",
            "After removing special chars: haha yeah  oz is kind of a shitload\n",
            "After tokenization: ['haha', 'yeah', 'oz', 'is', 'kind', 'of', 'a', 'shitload']\n",
            "After stop word removal: ['haha', 'yeah', 'oz', 'kind', 'shitload']\n",
            "After lemmatization: ['haha', 'yeah', 'oz', 'kind', 'shitload']\n",
            "After lowercasing: ok u can take me shopping when u get paid =d\n",
            "After removing special chars: ok u can take me shopping when u get paid d\n",
            "After tokenization: ['ok', 'u', 'can', 'take', 'me', 'shopping', 'when', 'u', 'get', 'paid', 'd']\n",
            "After stop word removal: ['ok', 'u', 'take', 'shopping', 'u', 'get', 'paid']\n",
            "After lemmatization: ['ok', 'u', 'take', 'shopping', 'u', 'get', 'paid']\n",
            "After lowercasing: my life means a lot to me, not because i love my life, but because i love the people in my life, the world calls them friends, i call them my world:-).. ge:-)..\n",
            "After removing special chars: my life means a lot to me not because i love my life but because i love the people in my life the world calls them friends i call them my world ge\n",
            "After tokenization: ['my', 'life', 'means', 'a', 'lot', 'to', 'me', 'not', 'because', 'i', 'love', 'my', 'life', 'but', 'because', 'i', 'love', 'the', 'people', 'in', 'my', 'life', 'the', 'world', 'calls', 'them', 'friends', 'i', 'call', 'them', 'my', 'world', 'ge']\n",
            "After stop word removal: ['life', 'means', 'lot', 'love', 'life', 'love', 'people', 'life', 'world', 'calls', 'friends', 'call', 'world', 'ge']\n",
            "After lemmatization: ['life', 'mean', 'lot', 'love', 'life', 'love', 'people', 'life', 'world', 'call', 'friend', 'call', 'world', 'ge']\n",
            "After lowercasing: alright we'll bring it to you, see you in like  &lt;#&gt;  mins\n",
            "After removing special chars: alright well bring it to you see you in like  ltgt  mins\n",
            "After tokenization: ['alright', 'well', 'bring', 'it', 'to', 'you', 'see', 'you', 'in', 'like', 'ltgt', 'mins']\n",
            "After stop word removal: ['alright', 'well', 'bring', 'see', 'like', 'ltgt', 'mins']\n",
            "After lemmatization: ['alright', 'well', 'bring', 'see', 'like', 'ltgt', 'min']\n",
            "After lowercasing: but pls dont play in others life.\n",
            "After removing special chars: but pls dont play in others life\n",
            "After tokenization: ['but', 'pls', 'dont', 'play', 'in', 'others', 'life']\n",
            "After stop word removal: ['pls', 'dont', 'play', 'others', 'life']\n",
            "After lemmatization: ['pls', 'dont', 'play', 'others', 'life']\n",
            "After lowercasing: eatin my lunch...\n",
            "After removing special chars: eatin my lunch\n",
            "After tokenization: ['eatin', 'my', 'lunch']\n",
            "After stop word removal: ['eatin', 'lunch']\n",
            "After lemmatization: ['eatin', 'lunch']\n",
            "After lowercasing: hmmm.but you should give it on one day..\n",
            "After removing special chars: hmmmbut you should give it on one day\n",
            "After tokenization: ['hmmmbut', 'you', 'should', 'give', 'it', 'on', 'one', 'day']\n",
            "After stop word removal: ['hmmmbut', 'give', 'one', 'day']\n",
            "After lemmatization: ['hmmmbut', 'give', 'one', 'day']\n",
            "After lowercasing: didn't try, g and i decided not to head out\n",
            "After removing special chars: didnt try g and i decided not to head out\n",
            "After tokenization: ['didnt', 'try', 'g', 'and', 'i', 'decided', 'not', 'to', 'head', 'out']\n",
            "After stop word removal: ['didnt', 'try', 'g', 'decided', 'head']\n",
            "After lemmatization: ['didnt', 'try', 'g', 'decided', 'head']\n",
            "After lowercasing: ok no prob\n",
            "After removing special chars: ok no prob\n",
            "After tokenization: ['ok', 'no', 'prob']\n",
            "After stop word removal: ['ok', 'prob']\n",
            "After lemmatization: ['ok', 'prob']\n",
            "After lowercasing: surly ill give it to you:-) while coming to review.\n",
            "After removing special chars: surly ill give it to you while coming to review\n",
            "After tokenization: ['surly', 'ill', 'give', 'it', 'to', 'you', 'while', 'coming', 'to', 'review']\n",
            "After stop word removal: ['surly', 'ill', 'give', 'coming', 'review']\n",
            "After lemmatization: ['surly', 'ill', 'give', 'coming', 'review']\n",
            "After lowercasing: by march ending, i should be ready. but will call you for sure. the problem is that my capital never complete. how far with you. how's work and the ladies\n",
            "After removing special chars: by march ending i should be ready but will call you for sure the problem is that my capital never complete how far with you hows work and the ladies\n",
            "After tokenization: ['by', 'march', 'ending', 'i', 'should', 'be', 'ready', 'but', 'will', 'call', 'you', 'for', 'sure', 'the', 'problem', 'is', 'that', 'my', 'capital', 'never', 'complete', 'how', 'far', 'with', 'you', 'hows', 'work', 'and', 'the', 'ladies']\n",
            "After stop word removal: ['march', 'ending', 'ready', 'call', 'sure', 'problem', 'capital', 'never', 'complete', 'far', 'hows', 'work', 'ladies']\n",
            "After lemmatization: ['march', 'ending', 'ready', 'call', 'sure', 'problem', 'capital', 'never', 'complete', 'far', 'hows', 'work', 'lady']\n",
            "After lowercasing: tessy..pls do me a favor. pls convey my birthday wishes to nimya..pls dnt forget it. today is her birthday shijas\n",
            "After removing special chars: tessypls do me a favor pls convey my birthday wishes to nimyapls dnt forget it today is her birthday shijas\n",
            "After tokenization: ['tessypls', 'do', 'me', 'a', 'favor', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimyapls', 'dnt', 'forget', 'it', 'today', 'is', 'her', 'birthday', 'shijas']\n",
            "After stop word removal: ['tessypls', 'favor', 'pls', 'convey', 'birthday', 'wishes', 'nimyapls', 'dnt', 'forget', 'today', 'birthday', 'shijas']\n",
            "After lemmatization: ['tessypls', 'favor', 'pls', 'convey', 'birthday', 'wish', 'nimyapls', 'dnt', 'forget', 'today', 'birthday', 'shijas']\n",
            "After lowercasing: pls give her the food preferably pap very slowly with loads of sugar. you can take up to an hour to give it. and then some water. very very slowly.\n",
            "After removing special chars: pls give her the food preferably pap very slowly with loads of sugar you can take up to an hour to give it and then some water very very slowly\n",
            "After tokenization: ['pls', 'give', 'her', 'the', 'food', 'preferably', 'pap', 'very', 'slowly', 'with', 'loads', 'of', 'sugar', 'you', 'can', 'take', 'up', 'to', 'an', 'hour', 'to', 'give', 'it', 'and', 'then', 'some', 'water', 'very', 'very', 'slowly']\n",
            "After stop word removal: ['pls', 'give', 'food', 'preferably', 'pap', 'slowly', 'loads', 'sugar', 'take', 'hour', 'give', 'water', 'slowly']\n",
            "After lemmatization: ['pls', 'give', 'food', 'preferably', 'pap', 'slowly', 'load', 'sugar', 'take', 'hour', 'give', 'water', 'slowly']\n",
            "After lowercasing: urgent! your mobile no 07808726822 was awarded a å£2,000 bonus caller prize on 02/09/03! this is our 2nd attempt to contact you! call 0871-872-9758 box95qu\n",
            "After removing special chars: urgent your mobile no  was awarded a  bonus caller prize on  this is our nd attempt to contact you call  boxqu\n",
            "After tokenization: ['urgent', 'your', 'mobile', 'no', 'was', 'awarded', 'a', 'bonus', 'caller', 'prize', 'on', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'you', 'call', 'boxqu']\n",
            "After stop word removal: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'contact', 'call', 'boxqu']\n",
            "After lemmatization: ['urgent', 'mobile', 'awarded', 'bonus', 'caller', 'prize', 'nd', 'attempt', 'contact', 'call', 'boxqu']\n",
            "After lowercasing: a guy who gets used but is too dumb to realize it.\n",
            "After removing special chars: a guy who gets used but is too dumb to realize it\n",
            "After tokenization: ['a', 'guy', 'who', 'gets', 'used', 'but', 'is', 'too', 'dumb', 'to', 'realize', 'it']\n",
            "After stop word removal: ['guy', 'gets', 'used', 'dumb', 'realize']\n",
            "After lemmatization: ['guy', 'get', 'used', 'dumb', 'realize']\n",
            "After lowercasing: okey dokey, iû÷ll be over in a bit just sorting some stuff out.\n",
            "After removing special chars: okey dokey ill be over in a bit just sorting some stuff out\n",
            "After tokenization: ['okey', 'dokey', 'ill', 'be', 'over', 'in', 'a', 'bit', 'just', 'sorting', 'some', 'stuff', 'out']\n",
            "After stop word removal: ['okey', 'dokey', 'ill', 'bit', 'sorting', 'stuff']\n",
            "After lemmatization: ['okey', 'dokey', 'ill', 'bit', 'sorting', 'stuff']\n",
            "After lowercasing: don no da:)whats you plan?\n",
            "After removing special chars: don no dawhats you plan\n",
            "After tokenization: ['don', 'no', 'dawhats', 'you', 'plan']\n",
            "After stop word removal: ['dawhats', 'plan']\n",
            "After lemmatization: ['dawhats', 'plan']\n",
            "After lowercasing: yes fine \n",
            "After removing special chars: yes fine \n",
            "After tokenization: ['yes', 'fine']\n",
            "After stop word removal: ['yes', 'fine']\n",
            "After lemmatization: ['yes', 'fine']\n",
            "After lowercasing: win: we have a winner! mr. t. foley won an ipod! more exciting prizes soon, so keep an eye on ur mobile or visit www.win-82050.co.uk\n",
            "After removing special chars: win we have a winner mr t foley won an ipod more exciting prizes soon so keep an eye on ur mobile or visit wwwwincouk\n",
            "After tokenization: ['win', 'we', 'have', 'a', 'winner', 'mr', 't', 'foley', 'won', 'an', 'ipod', 'more', 'exciting', 'prizes', 'soon', 'so', 'keep', 'an', 'eye', 'on', 'ur', 'mobile', 'or', 'visit', 'wwwwincouk']\n",
            "After stop word removal: ['win', 'winner', 'mr', 'foley', 'ipod', 'exciting', 'prizes', 'soon', 'keep', 'eye', 'ur', 'mobile', 'visit', 'wwwwincouk']\n",
            "After lemmatization: ['win', 'winner', 'mr', 'foley', 'ipod', 'exciting', 'prize', 'soon', 'keep', 'eye', 'ur', 'mobile', 'visit', 'wwwwincouk']\n",
            "After lowercasing: i liked the new mobile\n",
            "After removing special chars: i liked the new mobile\n",
            "After tokenization: ['i', 'liked', 'the', 'new', 'mobile']\n",
            "After stop word removal: ['liked', 'new', 'mobile']\n",
            "After lemmatization: ['liked', 'new', 'mobile']\n",
            "After lowercasing: anytime...\n",
            "After removing special chars: anytime\n",
            "After tokenization: ['anytime']\n",
            "After stop word removal: ['anytime']\n",
            "After lemmatization: ['anytime']\n",
            "After lowercasing: mmmmmmm *snuggles into you* ...*deep contented sigh* ... *whispers* ... i fucking love you so much i can barely stand it ...\n",
            "After removing special chars: mmmmmmm snuggles into you deep contented sigh  whispers  i fucking love you so much i can barely stand it \n",
            "After tokenization: ['mmmmmmm', 'snuggles', 'into', 'you', 'deep', 'contented', 'sigh', 'whispers', 'i', 'fucking', 'love', 'you', 'so', 'much', 'i', 'can', 'barely', 'stand', 'it']\n",
            "After stop word removal: ['mmmmmmm', 'snuggles', 'deep', 'contented', 'sigh', 'whispers', 'fucking', 'love', 'much', 'barely', 'stand']\n",
            "After lemmatization: ['mmmmmmm', 'snuggle', 'deep', 'contented', 'sigh', 'whisper', 'fucking', 'love', 'much', 'barely', 'stand']\n",
            "After lowercasing: yar but they say got some error.\n",
            "After removing special chars: yar but they say got some error\n",
            "After tokenization: ['yar', 'but', 'they', 'say', 'got', 'some', 'error']\n",
            "After stop word removal: ['yar', 'say', 'got', 'error']\n",
            "After lemmatization: ['yar', 'say', 'got', 'error']\n",
            "After lowercasing: hey anyway i have to :-)\n",
            "After removing special chars: hey anyway i have to \n",
            "After tokenization: ['hey', 'anyway', 'i', 'have', 'to']\n",
            "After stop word removal: ['hey', 'anyway']\n",
            "After lemmatization: ['hey', 'anyway']\n",
            "After lowercasing: wow so healthy. old airport rd lor. cant thk of anything else. but i'll b bathing my dog later.\n",
            "After removing special chars: wow so healthy old airport rd lor cant thk of anything else but ill b bathing my dog later\n",
            "After tokenization: ['wow', 'so', 'healthy', 'old', 'airport', 'rd', 'lor', 'cant', 'thk', 'of', 'anything', 'else', 'but', 'ill', 'b', 'bathing', 'my', 'dog', 'later']\n",
            "After stop word removal: ['wow', 'healthy', 'old', 'airport', 'rd', 'lor', 'cant', 'thk', 'anything', 'else', 'ill', 'b', 'bathing', 'dog', 'later']\n",
            "After lemmatization: ['wow', 'healthy', 'old', 'airport', 'rd', 'lor', 'cant', 'thk', 'anything', 'else', 'ill', 'b', 'bathing', 'dog', 'later']\n",
            "After lowercasing: wif my family booking tour package.\n",
            "After removing special chars: wif my family booking tour package\n",
            "After tokenization: ['wif', 'my', 'family', 'booking', 'tour', 'package']\n",
            "After stop word removal: ['wif', 'family', 'booking', 'tour', 'package']\n",
            "After lemmatization: ['wif', 'family', 'booking', 'tour', 'package']\n",
            "After lowercasing: did you say bold, then torch later. or one torch and 2bold?\n",
            "After removing special chars: did you say bold then torch later or one torch and bold\n",
            "After tokenization: ['did', 'you', 'say', 'bold', 'then', 'torch', 'later', 'or', 'one', 'torch', 'and', 'bold']\n",
            "After stop word removal: ['say', 'bold', 'torch', 'later', 'one', 'torch', 'bold']\n",
            "After lemmatization: ['say', 'bold', 'torch', 'later', 'one', 'torch', 'bold']\n",
            "After lowercasing: haha awesome, i might need to take you up on that, what you doin tonight?\n",
            "After removing special chars: haha awesome i might need to take you up on that what you doin tonight\n",
            "After tokenization: ['haha', 'awesome', 'i', 'might', 'need', 'to', 'take', 'you', 'up', 'on', 'that', 'what', 'you', 'doin', 'tonight']\n",
            "After stop word removal: ['haha', 'awesome', 'might', 'need', 'take', 'doin', 'tonight']\n",
            "After lemmatization: ['haha', 'awesome', 'might', 'need', 'take', 'doin', 'tonight']\n",
            "After lowercasing: ya i knw u vl giv..its ok thanks kano..anyway enjoy wit ur family wit 1st salary..:-);-)\n",
            "After removing special chars: ya i knw u vl givits ok thanks kanoanyway enjoy wit ur family wit st salary\n",
            "After tokenization: ['ya', 'i', 'knw', 'u', 'vl', 'givits', 'ok', 'thanks', 'kanoanyway', 'enjoy', 'wit', 'ur', 'family', 'wit', 'st', 'salary']\n",
            "After stop word removal: ['ya', 'knw', 'u', 'vl', 'givits', 'ok', 'thanks', 'kanoanyway', 'enjoy', 'wit', 'ur', 'family', 'wit', 'st', 'salary']\n",
            "After lemmatization: ['ya', 'knw', 'u', 'vl', 'givits', 'ok', 'thanks', 'kanoanyway', 'enjoy', 'wit', 'ur', 'family', 'wit', 'st', 'salary']\n",
            "After lowercasing: huh so slow i tot u reach long ago liao... u 2 more days only i 4 more leh...\n",
            "After removing special chars: huh so slow i tot u reach long ago liao u  more days only i  more leh\n",
            "After tokenization: ['huh', 'so', 'slow', 'i', 'tot', 'u', 'reach', 'long', 'ago', 'liao', 'u', 'more', 'days', 'only', 'i', 'more', 'leh']\n",
            "After stop word removal: ['huh', 'slow', 'tot', 'u', 'reach', 'long', 'ago', 'liao', 'u', 'days', 'leh']\n",
            "After lemmatization: ['huh', 'slow', 'tot', 'u', 'reach', 'long', 'ago', 'liao', 'u', 'day', 'leh']\n",
            "After lowercasing: thats cool princess! i will cover your face in hot sticky cum :)\n",
            "After removing special chars: thats cool princess i will cover your face in hot sticky cum \n",
            "After tokenization: ['thats', 'cool', 'princess', 'i', 'will', 'cover', 'your', 'face', 'in', 'hot', 'sticky', 'cum']\n",
            "After stop word removal: ['thats', 'cool', 'princess', 'cover', 'face', 'hot', 'sticky', 'cum']\n",
            "After lemmatization: ['thats', 'cool', 'princess', 'cover', 'face', 'hot', 'sticky', 'cum']\n",
            "After lowercasing: big brotherû÷s really scraped the barrel with this shower of social misfits\n",
            "After removing special chars: big brothers really scraped the barrel with this shower of social misfits\n",
            "After tokenization: ['big', 'brothers', 'really', 'scraped', 'the', 'barrel', 'with', 'this', 'shower', 'of', 'social', 'misfits']\n",
            "After stop word removal: ['big', 'brothers', 'really', 'scraped', 'barrel', 'shower', 'social', 'misfits']\n",
            "After lemmatization: ['big', 'brother', 'really', 'scraped', 'barrel', 'shower', 'social', 'misfit']\n",
            "After lowercasing: oops i thk i dun haf enuff... i go check then tell ì_..\n",
            "After removing special chars: oops i thk i dun haf enuff i go check then tell \n",
            "After tokenization: ['oops', 'i', 'thk', 'i', 'dun', 'haf', 'enuff', 'i', 'go', 'check', 'then', 'tell']\n",
            "After stop word removal: ['oops', 'thk', 'dun', 'haf', 'enuff', 'go', 'check', 'tell']\n",
            "After lemmatization: ['oops', 'thk', 'dun', 'haf', 'enuff', 'go', 'check', 'tell']\n",
            "After lowercasing: s:)8 min to go for lunch:)\n",
            "After removing special chars: s min to go for lunch\n",
            "After tokenization: ['s', 'min', 'to', 'go', 'for', 'lunch']\n",
            "After stop word removal: ['min', 'go', 'lunch']\n",
            "After lemmatization: ['min', 'go', 'lunch']\n",
            "After lowercasing: hey. what happened? u switch off ur cell d whole day. this isnt good. now if u do care, give me a call tomorrow.\n",
            "After removing special chars: hey what happened u switch off ur cell d whole day this isnt good now if u do care give me a call tomorrow\n",
            "After tokenization: ['hey', 'what', 'happened', 'u', 'switch', 'off', 'ur', 'cell', 'd', 'whole', 'day', 'this', 'isnt', 'good', 'now', 'if', 'u', 'do', 'care', 'give', 'me', 'a', 'call', 'tomorrow']\n",
            "After stop word removal: ['hey', 'happened', 'u', 'switch', 'ur', 'cell', 'whole', 'day', 'isnt', 'good', 'u', 'care', 'give', 'call', 'tomorrow']\n",
            "After lemmatization: ['hey', 'happened', 'u', 'switch', 'ur', 'cell', 'whole', 'day', 'isnt', 'good', 'u', 'care', 'give', 'call', 'tomorrow']\n",
            "After lowercasing: k will do, addie &amp; i are doing some art so i'll be here when you get home\n",
            "After removing special chars: k will do addie amp i are doing some art so ill be here when you get home\n",
            "After tokenization: ['k', 'will', 'do', 'addie', 'amp', 'i', 'are', 'doing', 'some', 'art', 'so', 'ill', 'be', 'here', 'when', 'you', 'get', 'home']\n",
            "After stop word removal: ['k', 'addie', 'amp', 'art', 'ill', 'get', 'home']\n",
            "After lemmatization: ['k', 'addie', 'amp', 'art', 'ill', 'get', 'home']\n",
            "After lowercasing: my uncles in atlanta. wish you guys a great semester.\n",
            "After removing special chars: my uncles in atlanta wish you guys a great semester\n",
            "After tokenization: ['my', 'uncles', 'in', 'atlanta', 'wish', 'you', 'guys', 'a', 'great', 'semester']\n",
            "After stop word removal: ['uncles', 'atlanta', 'wish', 'guys', 'great', 'semester']\n",
            "After lemmatization: ['uncle', 'atlanta', 'wish', 'guy', 'great', 'semester']\n",
            "After lowercasing: aiyo... her lesson so early... i'm still sleepin, haha... okie, u go home liao den confirm w me lor...\n",
            "After removing special chars: aiyo her lesson so early im still sleepin haha okie u go home liao den confirm w me lor\n",
            "After tokenization: ['aiyo', 'her', 'lesson', 'so', 'early', 'im', 'still', 'sleepin', 'haha', 'okie', 'u', 'go', 'home', 'liao', 'den', 'confirm', 'w', 'me', 'lor']\n",
            "After stop word removal: ['aiyo', 'lesson', 'early', 'im', 'still', 'sleepin', 'haha', 'okie', 'u', 'go', 'home', 'liao', 'den', 'confirm', 'w', 'lor']\n",
            "After lemmatization: ['aiyo', 'lesson', 'early', 'im', 'still', 'sleepin', 'haha', 'okie', 'u', 'go', 'home', 'liao', 'den', 'confirm', 'w', 'lor']\n",
            "After lowercasing: forgot to tell ì_ smth.. can ì_ like number the sections so that it's clearer..\n",
            "After removing special chars: forgot to tell  smth can  like number the sections so that its clearer\n",
            "After tokenization: ['forgot', 'to', 'tell', 'smth', 'can', 'like', 'number', 'the', 'sections', 'so', 'that', 'its', 'clearer']\n",
            "After stop word removal: ['forgot', 'tell', 'smth', 'like', 'number', 'sections', 'clearer']\n",
            "After lemmatization: ['forgot', 'tell', 'smth', 'like', 'number', 'section', 'clearer']\n",
            "After lowercasing: yup. anything lor, if u dun wan it's ok...\n",
            "After removing special chars: yup anything lor if u dun wan its ok\n",
            "After tokenization: ['yup', 'anything', 'lor', 'if', 'u', 'dun', 'wan', 'its', 'ok']\n",
            "After stop word removal: ['yup', 'anything', 'lor', 'u', 'dun', 'wan', 'ok']\n",
            "After lemmatization: ['yup', 'anything', 'lor', 'u', 'dun', 'wan', 'ok']\n",
            "After lowercasing: i'm home, my love ... if your still awake ... *loving kiss*\n",
            "After removing special chars: im home my love  if your still awake  loving kiss\n",
            "After tokenization: ['im', 'home', 'my', 'love', 'if', 'your', 'still', 'awake', 'loving', 'kiss']\n",
            "After stop word removal: ['im', 'home', 'love', 'still', 'awake', 'loving', 'kiss']\n",
            "After lemmatization: ['im', 'home', 'love', 'still', 'awake', 'loving', 'kiss']\n",
            "After lowercasing: hello peach! my cake tasts lush!\n",
            "After removing special chars: hello peach my cake tasts lush\n",
            "After tokenization: ['hello', 'peach', 'my', 'cake', 'tasts', 'lush']\n",
            "After stop word removal: ['hello', 'peach', 'cake', 'tasts', 'lush']\n",
            "After lemmatization: ['hello', 'peach', 'cake', 'tasts', 'lush']\n",
            "After lowercasing: free game. get rayman golf 4 free from the o2 games arcade. 1st get ur games settings. reply post, then save & activ8. press 0 key for arcade. termsapply\n",
            "After removing special chars: free game get rayman golf  free from the o games arcade st get ur games settings reply post then save  activ press  key for arcade termsapply\n",
            "After tokenization: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'from', 'the', 'o', 'games', 'arcade', 'st', 'get', 'ur', 'games', 'settings', 'reply', 'post', 'then', 'save', 'activ', 'press', 'key', 'for', 'arcade', 'termsapply']\n",
            "After stop word removal: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'games', 'arcade', 'st', 'get', 'ur', 'games', 'settings', 'reply', 'post', 'save', 'activ', 'press', 'key', 'arcade', 'termsapply']\n",
            "After lemmatization: ['free', 'game', 'get', 'rayman', 'golf', 'free', 'game', 'arcade', 'st', 'get', 'ur', 'game', 'setting', 'reply', 'post', 'save', 'activ', 'press', 'key', 'arcade', 'termsapply']\n",
            "After lowercasing: there'll be a minor shindig at my place later tonight, you interested?\n",
            "After removing special chars: therell be a minor shindig at my place later tonight you interested\n",
            "After tokenization: ['therell', 'be', 'a', 'minor', 'shindig', 'at', 'my', 'place', 'later', 'tonight', 'you', 'interested']\n",
            "After stop word removal: ['therell', 'minor', 'shindig', 'place', 'later', 'tonight', 'interested']\n",
            "After lemmatization: ['therell', 'minor', 'shindig', 'place', 'later', 'tonight', 'interested']\n",
            "After lowercasing: jason says it's cool if we pick some up from his place in like an hour\n",
            "After removing special chars: jason says its cool if we pick some up from his place in like an hour\n",
            "After tokenization: ['jason', 'says', 'its', 'cool', 'if', 'we', 'pick', 'some', 'up', 'from', 'his', 'place', 'in', 'like', 'an', 'hour']\n",
            "After stop word removal: ['jason', 'says', 'cool', 'pick', 'place', 'like', 'hour']\n",
            "After lemmatization: ['jason', 'say', 'cool', 'pick', 'place', 'like', 'hour']\n",
            "After lowercasing: had your mobile 10 mths? update to the latest camera/video phones for free. keep ur same number, get extra free mins/texts. text yes for a call\n",
            "After removing special chars: had your mobile  mths update to the latest cameravideo phones for free keep ur same number get extra free minstexts text yes for a call\n",
            "After tokenization: ['had', 'your', 'mobile', 'mths', 'update', 'to', 'the', 'latest', 'cameravideo', 'phones', 'for', 'free', 'keep', 'ur', 'same', 'number', 'get', 'extra', 'free', 'minstexts', 'text', 'yes', 'for', 'a', 'call']\n",
            "After stop word removal: ['mobile', 'mths', 'update', 'latest', 'cameravideo', 'phones', 'free', 'keep', 'ur', 'number', 'get', 'extra', 'free', 'minstexts', 'text', 'yes', 'call']\n",
            "After lemmatization: ['mobile', 'mths', 'update', 'latest', 'cameravideo', 'phone', 'free', 'keep', 'ur', 'number', 'get', 'extra', 'free', 'minstexts', 'text', 'yes', 'call']\n",
            "After lowercasing: i (career tel) have added u as a contact on indyarocks.com to send free sms. to remove from phonebook - sms no to  &lt;#&gt;\n",
            "After removing special chars: i career tel have added u as a contact on indyarockscom to send free sms to remove from phonebook  sms no to  ltgt\n",
            "After tokenization: ['i', 'career', 'tel', 'have', 'added', 'u', 'as', 'a', 'contact', 'on', 'indyarockscom', 'to', 'send', 'free', 'sms', 'to', 'remove', 'from', 'phonebook', 'sms', 'no', 'to', 'ltgt']\n",
            "After stop word removal: ['career', 'tel', 'added', 'u', 'contact', 'indyarockscom', 'send', 'free', 'sms', 'remove', 'phonebook', 'sms', 'ltgt']\n",
            "After lemmatization: ['career', 'tel', 'added', 'u', 'contact', 'indyarockscom', 'send', 'free', 'sm', 'remove', 'phonebook', 'sm', 'ltgt']\n",
            "After lowercasing: i've reached already.\n",
            "After removing special chars: ive reached already\n",
            "After tokenization: ['ive', 'reached', 'already']\n",
            "After stop word removal: ['ive', 'reached', 'already']\n",
            "After lemmatization: ['ive', 'reached', 'already']\n",
            "After lowercasing: i dont know ask to my brother. nothing problem some thing that. just i told .\n",
            "After removing special chars: i dont know ask to my brother nothing problem some thing that just i told \n",
            "After tokenization: ['i', 'dont', 'know', 'ask', 'to', 'my', 'brother', 'nothing', 'problem', 'some', 'thing', 'that', 'just', 'i', 'told']\n",
            "After stop word removal: ['dont', 'know', 'ask', 'brother', 'nothing', 'problem', 'thing', 'told']\n",
            "After lemmatization: ['dont', 'know', 'ask', 'brother', 'nothing', 'problem', 'thing', 'told']\n",
            "After lowercasing: k:)eng rocking in ashes:)\n",
            "After removing special chars: keng rocking in ashes\n",
            "After tokenization: ['keng', 'rocking', 'in', 'ashes']\n",
            "After stop word removal: ['keng', 'rocking', 'ashes']\n",
            "After lemmatization: ['keng', 'rocking', 'ash']\n",
            "After lowercasing: wat time r ì_ going to xin's hostel?\n",
            "After removing special chars: wat time r  going to xins hostel\n",
            "After tokenization: ['wat', 'time', 'r', 'going', 'to', 'xins', 'hostel']\n",
            "After stop word removal: ['wat', 'time', 'r', 'going', 'xins', 'hostel']\n",
            "After lemmatization: ['wat', 'time', 'r', 'going', 'xins', 'hostel']\n",
            "After lowercasing: good morning my dear shijutta........... have a great &amp; successful day.\n",
            "After removing special chars: good morning my dear shijutta have a great amp successful day\n",
            "After tokenization: ['good', 'morning', 'my', 'dear', 'shijutta', 'have', 'a', 'great', 'amp', 'successful', 'day']\n",
            "After stop word removal: ['good', 'morning', 'dear', 'shijutta', 'great', 'amp', 'successful', 'day']\n",
            "After lemmatization: ['good', 'morning', 'dear', 'shijutta', 'great', 'amp', 'successful', 'day']\n",
            "After lowercasing: buy space invaders 4 a chance 2 win orig arcade game console. press 0 for games arcade (std wap charge) see o2.co.uk/games 4 terms + settings. no purchase\n",
            "After removing special chars: buy space invaders  a chance  win orig arcade game console press  for games arcade std wap charge see ocoukgames  terms  settings no purchase\n",
            "After tokenization: ['buy', 'space', 'invaders', 'a', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'for', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'ocoukgames', 'terms', 'settings', 'no', 'purchase']\n",
            "After stop word removal: ['buy', 'space', 'invaders', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'games', 'arcade', 'std', 'wap', 'charge', 'see', 'ocoukgames', 'terms', 'settings', 'purchase']\n",
            "After lemmatization: ['buy', 'space', 'invader', 'chance', 'win', 'orig', 'arcade', 'game', 'console', 'press', 'game', 'arcade', 'std', 'wap', 'charge', 'see', 'ocoukgames', 'term', 'setting', 'purchase']\n",
            "After lowercasing: oh k:)after that placement there ah?\n",
            "After removing special chars: oh kafter that placement there ah\n",
            "After tokenization: ['oh', 'kafter', 'that', 'placement', 'there', 'ah']\n",
            "After stop word removal: ['oh', 'kafter', 'placement', 'ah']\n",
            "After lemmatization: ['oh', 'kafter', 'placement', 'ah']\n",
            "After lowercasing: not for possession, especially not first offense\n",
            "After removing special chars: not for possession especially not first offense\n",
            "After tokenization: ['not', 'for', 'possession', 'especially', 'not', 'first', 'offense']\n",
            "After stop word removal: ['possession', 'especially', 'first', 'offense']\n",
            "After lemmatization: ['possession', 'especially', 'first', 'offense']\n",
            "After lowercasing: nt only for driving even for many reasons she is called bbd..thts it chikku, then hw abt dvg cold..heard tht vinobanagar violence hw is the condition..and hw ru ? any problem?\n",
            "After removing special chars: nt only for driving even for many reasons she is called bbdthts it chikku then hw abt dvg coldheard tht vinobanagar violence hw is the conditionand hw ru  any problem\n",
            "After tokenization: ['nt', 'only', 'for', 'driving', 'even', 'for', 'many', 'reasons', 'she', 'is', 'called', 'bbdthts', 'it', 'chikku', 'then', 'hw', 'abt', 'dvg', 'coldheard', 'tht', 'vinobanagar', 'violence', 'hw', 'is', 'the', 'conditionand', 'hw', 'ru', 'any', 'problem']\n",
            "After stop word removal: ['nt', 'driving', 'even', 'many', 'reasons', 'called', 'bbdthts', 'chikku', 'hw', 'abt', 'dvg', 'coldheard', 'tht', 'vinobanagar', 'violence', 'hw', 'conditionand', 'hw', 'ru', 'problem']\n",
            "After lemmatization: ['nt', 'driving', 'even', 'many', 'reason', 'called', 'bbdthts', 'chikku', 'hw', 'abt', 'dvg', 'coldheard', 'tht', 'vinobanagar', 'violence', 'hw', 'conditionand', 'hw', 'ru', 'problem']\n",
            "After lowercasing: i bought the test yesterday. its something that lets you know the exact day u ovulate.when will get 2u in about 2 to 3wks. but pls pls dont fret. i know u r worried. pls relax. also is there anything in ur past history u need to tell me?\n",
            "After removing special chars: i bought the test yesterday its something that lets you know the exact day u ovulatewhen will get u in about  to wks but pls pls dont fret i know u r worried pls relax also is there anything in ur past history u need to tell me\n",
            "After tokenization: ['i', 'bought', 'the', 'test', 'yesterday', 'its', 'something', 'that', 'lets', 'you', 'know', 'the', 'exact', 'day', 'u', 'ovulatewhen', 'will', 'get', 'u', 'in', 'about', 'to', 'wks', 'but', 'pls', 'pls', 'dont', 'fret', 'i', 'know', 'u', 'r', 'worried', 'pls', 'relax', 'also', 'is', 'there', 'anything', 'in', 'ur', 'past', 'history', 'u', 'need', 'to', 'tell', 'me']\n",
            "After stop word removal: ['bought', 'test', 'yesterday', 'something', 'lets', 'know', 'exact', 'day', 'u', 'ovulatewhen', 'get', 'u', 'wks', 'pls', 'pls', 'dont', 'fret', 'know', 'u', 'r', 'worried', 'pls', 'relax', 'also', 'anything', 'ur', 'past', 'history', 'u', 'need', 'tell']\n",
            "After lemmatization: ['bought', 'test', 'yesterday', 'something', 'let', 'know', 'exact', 'day', 'u', 'ovulatewhen', 'get', 'u', 'wks', 'pls', 'pls', 'dont', 'fret', 'know', 'u', 'r', 'worried', 'pls', 'relax', 'also', 'anything', 'ur', 'past', 'history', 'u', 'need', 'tell']\n",
            "After lowercasing: we have pizza if u want\n",
            "After removing special chars: we have pizza if u want\n",
            "After tokenization: ['we', 'have', 'pizza', 'if', 'u', 'want']\n",
            "After stop word removal: ['pizza', 'u', 'want']\n",
            "After lemmatization: ['pizza', 'u', 'want']\n",
            "After lowercasing: i keep seeing weird shit and bein all \\woah\\\" then realising it's actually reasonable and i'm all \\\"oh\\\"\"\n",
            "After removing special chars: i keep seeing weird shit and bein all woah then realising its actually reasonable and im all oh\n",
            "After tokenization: ['i', 'keep', 'seeing', 'weird', 'shit', 'and', 'bein', 'all', 'woah', 'then', 'realising', 'its', 'actually', 'reasonable', 'and', 'im', 'all', 'oh']\n",
            "After stop word removal: ['keep', 'seeing', 'weird', 'shit', 'bein', 'woah', 'realising', 'actually', 'reasonable', 'im', 'oh']\n",
            "After lemmatization: ['keep', 'seeing', 'weird', 'shit', 'bein', 'woah', 'realising', 'actually', 'reasonable', 'im', 'oh']\n",
            "After lowercasing: many more happy returns of the day. i wish you happy birthday.\n",
            "After removing special chars: many more happy returns of the day i wish you happy birthday\n",
            "After tokenization: ['many', 'more', 'happy', 'returns', 'of', 'the', 'day', 'i', 'wish', 'you', 'happy', 'birthday']\n",
            "After stop word removal: ['many', 'happy', 'returns', 'day', 'wish', 'happy', 'birthday']\n",
            "After lemmatization: ['many', 'happy', 'return', 'day', 'wish', 'happy', 'birthday']\n",
            "After lowercasing: ya very nice. . .be ready on thursday\n",
            "After removing special chars: ya very nice  be ready on thursday\n",
            "After tokenization: ['ya', 'very', 'nice', 'be', 'ready', 'on', 'thursday']\n",
            "After stop word removal: ['ya', 'nice', 'ready', 'thursday']\n",
            "After lemmatization: ['ya', 'nice', 'ready', 'thursday']\n",
            "After lowercasing: i am in hospital da. . i will return home in evening\n",
            "After removing special chars: i am in hospital da  i will return home in evening\n",
            "After tokenization: ['i', 'am', 'in', 'hospital', 'da', 'i', 'will', 'return', 'home', 'in', 'evening']\n",
            "After stop word removal: ['hospital', 'da', 'return', 'home', 'evening']\n",
            "After lemmatization: ['hospital', 'da', 'return', 'home', 'evening']\n",
            "After lowercasing: \\thinking of u ;) x\\\"\"\n",
            "After removing special chars: thinking of u  x\n",
            "After tokenization: ['thinking', 'of', 'u', 'x']\n",
            "After stop word removal: ['thinking', 'u', 'x']\n",
            "After lemmatization: ['thinking', 'u', 'x']\n",
            "After lowercasing: camera - you are awarded a sipix digital camera! call 09061221066 fromm landline. delivery within 28 days.\n",
            "After removing special chars: camera  you are awarded a sipix digital camera call  fromm landline delivery within  days\n",
            "After tokenization: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
            "After stop word removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
            "After lemmatization: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'day']\n",
            "After lowercasing: orh i tot u say she now still dun believe.\n",
            "After removing special chars: orh i tot u say she now still dun believe\n",
            "After tokenization: ['orh', 'i', 'tot', 'u', 'say', 'she', 'now', 'still', 'dun', 'believe']\n",
            "After stop word removal: ['orh', 'tot', 'u', 'say', 'still', 'dun', 'believe']\n",
            "After lemmatization: ['orh', 'tot', 'u', 'say', 'still', 'dun', 'believe']\n",
            "After lowercasing: when you just put in the + sign, choose my number and the pin will show. right?\n",
            "After removing special chars: when you just put in the  sign choose my number and the pin will show right\n",
            "After tokenization: ['when', 'you', 'just', 'put', 'in', 'the', 'sign', 'choose', 'my', 'number', 'and', 'the', 'pin', 'will', 'show', 'right']\n",
            "After stop word removal: ['put', 'sign', 'choose', 'number', 'pin', 'show', 'right']\n",
            "After lemmatization: ['put', 'sign', 'choose', 'number', 'pin', 'show', 'right']\n",
            "After lowercasing: the beauty of life is in next second.. which hides thousands of secrets. i wish every second will be wonderful in ur life...!! gud n8\n",
            "After removing special chars: the beauty of life is in next second which hides thousands of secrets i wish every second will be wonderful in ur life gud n\n",
            "After tokenization: ['the', 'beauty', 'of', 'life', 'is', 'in', 'next', 'second', 'which', 'hides', 'thousands', 'of', 'secrets', 'i', 'wish', 'every', 'second', 'will', 'be', 'wonderful', 'in', 'ur', 'life', 'gud', 'n']\n",
            "After stop word removal: ['beauty', 'life', 'next', 'second', 'hides', 'thousands', 'secrets', 'wish', 'every', 'second', 'wonderful', 'ur', 'life', 'gud', 'n']\n",
            "After lemmatization: ['beauty', 'life', 'next', 'second', 'hide', 'thousand', 'secret', 'wish', 'every', 'second', 'wonderful', 'ur', 'life', 'gud', 'n']\n",
            "After lowercasing: thanx u darlin!im cool thanx. a few bday drinks 2 nite. 2morrow off! take care c u soon.xxx\n",
            "After removing special chars: thanx u darlinim cool thanx a few bday drinks  nite morrow off take care c u soonxxx\n",
            "After tokenization: ['thanx', 'u', 'darlinim', 'cool', 'thanx', 'a', 'few', 'bday', 'drinks', 'nite', 'morrow', 'off', 'take', 'care', 'c', 'u', 'soonxxx']\n",
            "After stop word removal: ['thanx', 'u', 'darlinim', 'cool', 'thanx', 'bday', 'drinks', 'nite', 'morrow', 'take', 'care', 'c', 'u', 'soonxxx']\n",
            "After lemmatization: ['thanx', 'u', 'darlinim', 'cool', 'thanx', 'bday', 'drink', 'nite', 'morrow', 'take', 'care', 'c', 'u', 'soonxxx']\n",
            "After lowercasing: if you're still up, maybe leave the credit card so i can get gas when i get back like he told me to\n",
            "After removing special chars: if youre still up maybe leave the credit card so i can get gas when i get back like he told me to\n",
            "After tokenization: ['if', 'youre', 'still', 'up', 'maybe', 'leave', 'the', 'credit', 'card', 'so', 'i', 'can', 'get', 'gas', 'when', 'i', 'get', 'back', 'like', 'he', 'told', 'me', 'to']\n",
            "After stop word removal: ['youre', 'still', 'maybe', 'leave', 'credit', 'card', 'get', 'gas', 'get', 'back', 'like', 'told']\n",
            "After lemmatization: ['youre', 'still', 'maybe', 'leave', 'credit', 'card', 'get', 'gas', 'get', 'back', 'like', 'told']\n",
            "After lowercasing: your weekly cool-mob tones are ready to download !this weeks new tones include: 1) crazy frog-axel f>>> 2) akon-lonely>>> 3) black eyed-dont p >>>more info in n\n",
            "After removing special chars: your weekly coolmob tones are ready to download this weeks new tones include  crazy frogaxel f  akonlonely  black eyeddont p more info in n\n",
            "After tokenization: ['your', 'weekly', 'coolmob', 'tones', 'are', 'ready', 'to', 'download', 'this', 'weeks', 'new', 'tones', 'include', 'crazy', 'frogaxel', 'f', 'akonlonely', 'black', 'eyeddont', 'p', 'more', 'info', 'in', 'n']\n",
            "After stop word removal: ['weekly', 'coolmob', 'tones', 'ready', 'download', 'weeks', 'new', 'tones', 'include', 'crazy', 'frogaxel', 'f', 'akonlonely', 'black', 'eyeddont', 'p', 'info', 'n']\n",
            "After lemmatization: ['weekly', 'coolmob', 'tone', 'ready', 'download', 'week', 'new', 'tone', 'include', 'crazy', 'frogaxel', 'f', 'akonlonely', 'black', 'eyeddont', 'p', 'info', 'n']\n",
            "After lowercasing: well boy am i glad g wasted all night at applebees for nothing\n",
            "After removing special chars: well boy am i glad g wasted all night at applebees for nothing\n",
            "After tokenization: ['well', 'boy', 'am', 'i', 'glad', 'g', 'wasted', 'all', 'night', 'at', 'applebees', 'for', 'nothing']\n",
            "After stop word removal: ['well', 'boy', 'glad', 'g', 'wasted', 'night', 'applebees', 'nothing']\n",
            "After lemmatization: ['well', 'boy', 'glad', 'g', 'wasted', 'night', 'applebees', 'nothing']\n",
            "After lowercasing: cashbin.co.uk (get lots of cash this weekend!) www.cashbin.co.uk dear welcome to the weekend we have got our biggest and best ever cash give away!! these..\n",
            "After removing special chars: cashbincouk get lots of cash this weekend wwwcashbincouk dear welcome to the weekend we have got our biggest and best ever cash give away these\n",
            "After tokenization: ['cashbincouk', 'get', 'lots', 'of', 'cash', 'this', 'weekend', 'wwwcashbincouk', 'dear', 'welcome', 'to', 'the', 'weekend', 'we', 'have', 'got', 'our', 'biggest', 'and', 'best', 'ever', 'cash', 'give', 'away', 'these']\n",
            "After stop word removal: ['cashbincouk', 'get', 'lots', 'cash', 'weekend', 'wwwcashbincouk', 'dear', 'welcome', 'weekend', 'got', 'biggest', 'best', 'ever', 'cash', 'give', 'away']\n",
            "After lemmatization: ['cashbincouk', 'get', 'lot', 'cash', 'weekend', 'wwwcashbincouk', 'dear', 'welcome', 'weekend', 'got', 'biggest', 'best', 'ever', 'cash', 'give', 'away']\n",
            "After lowercasing: ok lor... or u wan me go look 4 u?\n",
            "After removing special chars: ok lor or u wan me go look  u\n",
            "After tokenization: ['ok', 'lor', 'or', 'u', 'wan', 'me', 'go', 'look', 'u']\n",
            "After stop word removal: ['ok', 'lor', 'u', 'wan', 'go', 'look', 'u']\n",
            "After lemmatization: ['ok', 'lor', 'u', 'wan', 'go', 'look', 'u']\n",
            "After lowercasing: u wan 2 haf lunch i'm in da canteen now.\n",
            "After removing special chars: u wan  haf lunch im in da canteen now\n",
            "After tokenization: ['u', 'wan', 'haf', 'lunch', 'im', 'in', 'da', 'canteen', 'now']\n",
            "After stop word removal: ['u', 'wan', 'haf', 'lunch', 'im', 'da', 'canteen']\n",
            "After lemmatization: ['u', 'wan', 'haf', 'lunch', 'im', 'da', 'canteen']\n",
            "After lowercasing: don't make life too stressfull.. always find time to laugh.. it may not add years to your life! but surely adds more life to ur years!! gud ni8..swt dreams..\n",
            "After removing special chars: dont make life too stressfull always find time to laugh it may not add years to your life but surely adds more life to ur years gud niswt dreams\n",
            "After tokenization: ['dont', 'make', 'life', 'too', 'stressfull', 'always', 'find', 'time', 'to', 'laugh', 'it', 'may', 'not', 'add', 'years', 'to', 'your', 'life', 'but', 'surely', 'adds', 'more', 'life', 'to', 'ur', 'years', 'gud', 'niswt', 'dreams']\n",
            "After stop word removal: ['dont', 'make', 'life', 'stressfull', 'always', 'find', 'time', 'laugh', 'may', 'add', 'years', 'life', 'surely', 'adds', 'life', 'ur', 'years', 'gud', 'niswt', 'dreams']\n",
            "After lemmatization: ['dont', 'make', 'life', 'stressfull', 'always', 'find', 'time', 'laugh', 'may', 'add', 'year', 'life', 'surely', 'add', 'life', 'ur', 'year', 'gud', 'niswt', 'dream']\n",
            "After lowercasing: hey, looks like i was wrong and one of the kappa guys numbers is still on my phone, if you want i can text him and see if he's around\n",
            "After removing special chars: hey looks like i was wrong and one of the kappa guys numbers is still on my phone if you want i can text him and see if hes around\n",
            "After tokenization: ['hey', 'looks', 'like', 'i', 'was', 'wrong', 'and', 'one', 'of', 'the', 'kappa', 'guys', 'numbers', 'is', 'still', 'on', 'my', 'phone', 'if', 'you', 'want', 'i', 'can', 'text', 'him', 'and', 'see', 'if', 'hes', 'around']\n",
            "After stop word removal: ['hey', 'looks', 'like', 'wrong', 'one', 'kappa', 'guys', 'numbers', 'still', 'phone', 'want', 'text', 'see', 'hes', 'around']\n",
            "After lemmatization: ['hey', 'look', 'like', 'wrong', 'one', 'kappa', 'guy', 'number', 'still', 'phone', 'want', 'text', 'see', 'he', 'around']\n",
            "After lowercasing: urgent! your mobile number has been awarded with a å£2000 prize guaranteed. call 09061790121 from land line. claim 3030. valid 12hrs only 150ppm\n",
            "After removing special chars: urgent your mobile number has been awarded with a  prize guaranteed call  from land line claim  valid hrs only ppm\n",
            "After tokenization: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
            "After stop word removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
            "After lemmatization: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
            "After lowercasing: thanks 4 your continued support your question this week will enter u in2 our draw 4 å£100 cash. name the new us president? txt ans to 80082\n",
            "After removing special chars: thanks  your continued support your question this week will enter u in our draw   cash name the new us president txt ans to \n",
            "After tokenization: ['thanks', 'your', 'continued', 'support', 'your', 'question', 'this', 'week', 'will', 'enter', 'u', 'in', 'our', 'draw', 'cash', 'name', 'the', 'new', 'us', 'president', 'txt', 'ans', 'to']\n",
            "After stop word removal: ['thanks', 'continued', 'support', 'question', 'week', 'enter', 'u', 'draw', 'cash', 'name', 'new', 'us', 'president', 'txt', 'ans']\n",
            "After lemmatization: ['thanks', 'continued', 'support', 'question', 'week', 'enter', 'u', 'draw', 'cash', 'name', 'new', 'u', 'president', 'txt', 'an']\n",
            "After lowercasing: i'm home. doc gave me pain meds says everything is fine.\n",
            "After removing special chars: im home doc gave me pain meds says everything is fine\n",
            "After tokenization: ['im', 'home', 'doc', 'gave', 'me', 'pain', 'meds', 'says', 'everything', 'is', 'fine']\n",
            "After stop word removal: ['im', 'home', 'doc', 'gave', 'pain', 'meds', 'says', 'everything', 'fine']\n",
            "After lemmatization: ['im', 'home', 'doc', 'gave', 'pain', 'med', 'say', 'everything', 'fine']\n",
            "After lowercasing: it's ì© only $140 ard...ìä rest all ard $180 at least...which is ì© price 4 ì© 2 bedrm ($900)\n",
            "After removing special chars: its  only  ard rest all ard  at leastwhich is  price    bedrm \n",
            "After tokenization: ['its', 'only', 'ard', 'rest', 'all', 'ard', 'at', 'leastwhich', 'is', 'price', 'bedrm']\n",
            "After stop word removal: ['ard', 'rest', 'ard', 'leastwhich', 'price', 'bedrm']\n",
            "After lemmatization: ['ard', 'rest', 'ard', 'leastwhich', 'price', 'bedrm']\n",
            "After lowercasing: me too! have a lovely night xxx\n",
            "After removing special chars: me too have a lovely night xxx\n",
            "After tokenization: ['me', 'too', 'have', 'a', 'lovely', 'night', 'xxx']\n",
            "After stop word removal: ['lovely', 'night', 'xxx']\n",
            "After lemmatization: ['lovely', 'night', 'xxx']\n",
            "After lowercasing: prepare to be pleasured :)\n",
            "After removing special chars: prepare to be pleasured \n",
            "After tokenization: ['prepare', 'to', 'be', 'pleasured']\n",
            "After stop word removal: ['prepare', 'pleasured']\n",
            "After lemmatization: ['prepare', 'pleasured']\n",
            "After lowercasing: hi.:)technical support.providing assistance to us customer through call and email:)\n",
            "After removing special chars: hitechnical supportproviding assistance to us customer through call and email\n",
            "After tokenization: ['hitechnical', 'supportproviding', 'assistance', 'to', 'us', 'customer', 'through', 'call', 'and', 'email']\n",
            "After stop word removal: ['hitechnical', 'supportproviding', 'assistance', 'us', 'customer', 'call', 'email']\n",
            "After lemmatization: ['hitechnical', 'supportproviding', 'assistance', 'u', 'customer', 'call', 'email']\n",
            "After lowercasing: if you text on your way to cup stop that should work. and that should be bus\n",
            "After removing special chars: if you text on your way to cup stop that should work and that should be bus\n",
            "After tokenization: ['if', 'you', 'text', 'on', 'your', 'way', 'to', 'cup', 'stop', 'that', 'should', 'work', 'and', 'that', 'should', 'be', 'bus']\n",
            "After stop word removal: ['text', 'way', 'cup', 'stop', 'work', 'bus']\n",
            "After lemmatization: ['text', 'way', 'cup', 'stop', 'work', 'bus']\n",
            "After lowercasing: whens your radio show?\n",
            "After removing special chars: whens your radio show\n",
            "After tokenization: ['whens', 'your', 'radio', 'show']\n",
            "After stop word removal: ['whens', 'radio', 'show']\n",
            "After lemmatization: ['whens', 'radio', 'show']\n",
            "After lowercasing: your unique user id is 1172. for removal send stop to 87239 customer services 08708034412\n",
            "After removing special chars: your unique user id is  for removal send stop to  customer services \n",
            "After tokenization: ['your', 'unique', 'user', 'id', 'is', 'for', 'removal', 'send', 'stop', 'to', 'customer', 'services']\n",
            "After stop word removal: ['unique', 'user', 'id', 'removal', 'send', 'stop', 'customer', 'services']\n",
            "After lemmatization: ['unique', 'user', 'id', 'removal', 'send', 'stop', 'customer', 'service']\n",
            "After lowercasing: i'm not sure if its still available though\n",
            "After removing special chars: im not sure if its still available though\n",
            "After tokenization: ['im', 'not', 'sure', 'if', 'its', 'still', 'available', 'though']\n",
            "After stop word removal: ['im', 'sure', 'still', 'available', 'though']\n",
            "After lemmatization: ['im', 'sure', 'still', 'available', 'though']\n",
            "After lowercasing: watever relation u built up in dis world only thing which remains atlast iz lonlines with lotz n lot memories! feeling..\n",
            "After removing special chars: watever relation u built up in dis world only thing which remains atlast iz lonlines with lotz n lot memories feeling\n",
            "After tokenization: ['watever', 'relation', 'u', 'built', 'up', 'in', 'dis', 'world', 'only', 'thing', 'which', 'remains', 'atlast', 'iz', 'lonlines', 'with', 'lotz', 'n', 'lot', 'memories', 'feeling']\n",
            "After stop word removal: ['watever', 'relation', 'u', 'built', 'dis', 'world', 'thing', 'remains', 'atlast', 'iz', 'lonlines', 'lotz', 'n', 'lot', 'memories', 'feeling']\n",
            "After lemmatization: ['watever', 'relation', 'u', 'built', 'dis', 'world', 'thing', 'remains', 'atlast', 'iz', 'lonlines', 'lotz', 'n', 'lot', 'memory', 'feeling']\n",
            "After lowercasing: cheers lou! yeah was a goodnite shame u neva came! c ya gailxx\n",
            "After removing special chars: cheers lou yeah was a goodnite shame u neva came c ya gailxx\n",
            "After tokenization: ['cheers', 'lou', 'yeah', 'was', 'a', 'goodnite', 'shame', 'u', 'neva', 'came', 'c', 'ya', 'gailxx']\n",
            "After stop word removal: ['cheers', 'lou', 'yeah', 'goodnite', 'shame', 'u', 'neva', 'came', 'c', 'ya', 'gailxx']\n",
            "After lemmatization: ['cheer', 'lou', 'yeah', 'goodnite', 'shame', 'u', 'neva', 'came', 'c', 'ya', 'gailxx']\n",
            "After lowercasing: hi..i got the money da:)\n",
            "After removing special chars: hii got the money da\n",
            "After tokenization: ['hii', 'got', 'the', 'money', 'da']\n",
            "After stop word removal: ['hii', 'got', 'money', 'da']\n",
            "After lemmatization: ['hii', 'got', 'money', 'da']\n",
            "After lowercasing: hi, mobile no.  &lt;#&gt;  has added you in their contact list on www.fullonsms.com it s a great place to send free sms to people for more visit fullonsms.com\n",
            "After removing special chars: hi mobile no  ltgt  has added you in their contact list on wwwfullonsmscom it s a great place to send free sms to people for more visit fullonsmscom\n",
            "After tokenization: ['hi', 'mobile', 'no', 'ltgt', 'has', 'added', 'you', 'in', 'their', 'contact', 'list', 'on', 'wwwfullonsmscom', 'it', 's', 'a', 'great', 'place', 'to', 'send', 'free', 'sms', 'to', 'people', 'for', 'more', 'visit', 'fullonsmscom']\n",
            "After stop word removal: ['hi', 'mobile', 'ltgt', 'added', 'contact', 'list', 'wwwfullonsmscom', 'great', 'place', 'send', 'free', 'sms', 'people', 'visit', 'fullonsmscom']\n",
            "After lemmatization: ['hi', 'mobile', 'ltgt', 'added', 'contact', 'list', 'wwwfullonsmscom', 'great', 'place', 'send', 'free', 'sm', 'people', 'visit', 'fullonsmscom']\n",
            "After lowercasing: ok then u tell me wat time u coming later lor.\n",
            "After removing special chars: ok then u tell me wat time u coming later lor\n",
            "After tokenization: ['ok', 'then', 'u', 'tell', 'me', 'wat', 'time', 'u', 'coming', 'later', 'lor']\n",
            "After stop word removal: ['ok', 'u', 'tell', 'wat', 'time', 'u', 'coming', 'later', 'lor']\n",
            "After lemmatization: ['ok', 'u', 'tell', 'wat', 'time', 'u', 'coming', 'later', 'lor']\n",
            "After lowercasing: u repeat e instructions again. wat's e road name of ur house?\n",
            "After removing special chars: u repeat e instructions again wats e road name of ur house\n",
            "After tokenization: ['u', 'repeat', 'e', 'instructions', 'again', 'wats', 'e', 'road', 'name', 'of', 'ur', 'house']\n",
            "After stop word removal: ['u', 'repeat', 'e', 'instructions', 'wats', 'e', 'road', 'name', 'ur', 'house']\n",
            "After lemmatization: ['u', 'repeat', 'e', 'instruction', 'wats', 'e', 'road', 'name', 'ur', 'house']\n",
            "After lowercasing: so many people seems to be special at first sight, but only very few will remain special to you till your last sight.. maintain them till life ends.. sh!jas\n",
            "After removing special chars: so many people seems to be special at first sight but only very few will remain special to you till your last sight maintain them till life ends shjas\n",
            "After tokenization: ['so', 'many', 'people', 'seems', 'to', 'be', 'special', 'at', 'first', 'sight', 'but', 'only', 'very', 'few', 'will', 'remain', 'special', 'to', 'you', 'till', 'your', 'last', 'sight', 'maintain', 'them', 'till', 'life', 'ends', 'shjas']\n",
            "After stop word removal: ['many', 'people', 'seems', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'ends', 'shjas']\n",
            "After lemmatization: ['many', 'people', 'seems', 'special', 'first', 'sight', 'remain', 'special', 'till', 'last', 'sight', 'maintain', 'till', 'life', 'end', 'shjas']\n",
            "After lowercasing: quite lor. but dun tell him wait he get complacent...\n",
            "After removing special chars: quite lor but dun tell him wait he get complacent\n",
            "After tokenization: ['quite', 'lor', 'but', 'dun', 'tell', 'him', 'wait', 'he', 'get', 'complacent']\n",
            "After stop word removal: ['quite', 'lor', 'dun', 'tell', 'wait', 'get', 'complacent']\n",
            "After lemmatization: ['quite', 'lor', 'dun', 'tell', 'wait', 'get', 'complacent']\n",
            "After lowercasing: sorry completely forgot * will pop em round this week if your still here?\n",
            "After removing special chars: sorry completely forgot  will pop em round this week if your still here\n",
            "After tokenization: ['sorry', 'completely', 'forgot', 'will', 'pop', 'em', 'round', 'this', 'week', 'if', 'your', 'still', 'here']\n",
            "After stop word removal: ['sorry', 'completely', 'forgot', 'pop', 'em', 'round', 'week', 'still']\n",
            "After lemmatization: ['sorry', 'completely', 'forgot', 'pop', 'em', 'round', 'week', 'still']\n",
            "After lowercasing: u r the most beautiful girl ive ever seen. u r my baby come and c me in the common room\n",
            "After removing special chars: u r the most beautiful girl ive ever seen u r my baby come and c me in the common room\n",
            "After tokenization: ['u', 'r', 'the', 'most', 'beautiful', 'girl', 'ive', 'ever', 'seen', 'u', 'r', 'my', 'baby', 'come', 'and', 'c', 'me', 'in', 'the', 'common', 'room']\n",
            "After stop word removal: ['u', 'r', 'beautiful', 'girl', 'ive', 'ever', 'seen', 'u', 'r', 'baby', 'come', 'c', 'common', 'room']\n",
            "After lemmatization: ['u', 'r', 'beautiful', 'girl', 'ive', 'ever', 'seen', 'u', 'r', 'baby', 'come', 'c', 'common', 'room']\n",
            "After lowercasing: o we cant see if we can join denis and mina? or does denis want alone time\n",
            "After removing special chars: o we cant see if we can join denis and mina or does denis want alone time\n",
            "After tokenization: ['o', 'we', 'cant', 'see', 'if', 'we', 'can', 'join', 'denis', 'and', 'mina', 'or', 'does', 'denis', 'want', 'alone', 'time']\n",
            "After stop word removal: ['cant', 'see', 'join', 'denis', 'mina', 'denis', 'want', 'alone', 'time']\n",
            "After lemmatization: ['cant', 'see', 'join', 'denis', 'mina', 'denis', 'want', 'alone', 'time']\n",
            "After lowercasing: sen told that he is going to join his uncle finance in cbe\n",
            "After removing special chars: sen told that he is going to join his uncle finance in cbe\n",
            "After tokenization: ['sen', 'told', 'that', 'he', 'is', 'going', 'to', 'join', 'his', 'uncle', 'finance', 'in', 'cbe']\n",
            "After stop word removal: ['sen', 'told', 'going', 'join', 'uncle', 'finance', 'cbe']\n",
            "After lemmatization: ['sen', 'told', 'going', 'join', 'uncle', 'finance', 'cbe']\n",
            "After lowercasing: yup... hey then one day on fri we can ask miwa and jiayin take leave go karaoke \n",
            "After removing special chars: yup hey then one day on fri we can ask miwa and jiayin take leave go karaoke \n",
            "After tokenization: ['yup', 'hey', 'then', 'one', 'day', 'on', 'fri', 'we', 'can', 'ask', 'miwa', 'and', 'jiayin', 'take', 'leave', 'go', 'karaoke']\n",
            "After stop word removal: ['yup', 'hey', 'one', 'day', 'fri', 'ask', 'miwa', 'jiayin', 'take', 'leave', 'go', 'karaoke']\n",
            "After lemmatization: ['yup', 'hey', 'one', 'day', 'fri', 'ask', 'miwa', 'jiayin', 'take', 'leave', 'go', 'karaoke']\n",
            "After lowercasing: call me, i am senthil from hsbc.\n",
            "After removing special chars: call me i am senthil from hsbc\n",
            "After tokenization: ['call', 'me', 'i', 'am', 'senthil', 'from', 'hsbc']\n",
            "After stop word removal: ['call', 'senthil', 'hsbc']\n",
            "After lemmatization: ['call', 'senthil', 'hsbc']\n",
            "After lowercasing: especially since i talk about boston all up in my personal statement, lol! i woulda changed that if i had realized it said nyc! it says boston now.\n",
            "After removing special chars: especially since i talk about boston all up in my personal statement lol i woulda changed that if i had realized it said nyc it says boston now\n",
            "After tokenization: ['especially', 'since', 'i', 'talk', 'about', 'boston', 'all', 'up', 'in', 'my', 'personal', 'statement', 'lol', 'i', 'woulda', 'changed', 'that', 'if', 'i', 'had', 'realized', 'it', 'said', 'nyc', 'it', 'says', 'boston', 'now']\n",
            "After stop word removal: ['especially', 'since', 'talk', 'boston', 'personal', 'statement', 'lol', 'woulda', 'changed', 'realized', 'said', 'nyc', 'says', 'boston']\n",
            "After lemmatization: ['especially', 'since', 'talk', 'boston', 'personal', 'statement', 'lol', 'woulda', 'changed', 'realized', 'said', 'nyc', 'say', 'boston']\n",
            "After lowercasing: indeed and by the way it was either or - not both !\n",
            "After removing special chars: indeed and by the way it was either or  not both \n",
            "After tokenization: ['indeed', 'and', 'by', 'the', 'way', 'it', 'was', 'either', 'or', 'not', 'both']\n",
            "After stop word removal: ['indeed', 'way', 'either']\n",
            "After lemmatization: ['indeed', 'way', 'either']\n",
            "After lowercasing: urgent -call 09066649731from landline. your complimentary 4* ibiza holiday or å£10,000 cash await collection sae t&cs po box 434 sk3 8wp 150ppm 18+\n",
            "After removing special chars: urgent call from landline your complimentary  ibiza holiday or  cash await collection sae tcs po box  sk wp ppm \n",
            "After tokenization: ['urgent', 'call', 'from', 'landline', 'your', 'complimentary', 'ibiza', 'holiday', 'or', 'cash', 'await', 'collection', 'sae', 'tcs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After stop word removal: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'tcs', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After lemmatization: ['urgent', 'call', 'landline', 'complimentary', 'ibiza', 'holiday', 'cash', 'await', 'collection', 'sae', 'tc', 'po', 'box', 'sk', 'wp', 'ppm']\n",
            "After lowercasing: holy living christ what is taking you so long\n",
            "After removing special chars: holy living christ what is taking you so long\n",
            "After tokenization: ['holy', 'living', 'christ', 'what', 'is', 'taking', 'you', 'so', 'long']\n",
            "After stop word removal: ['holy', 'living', 'christ', 'taking', 'long']\n",
            "After lemmatization: ['holy', 'living', 'christ', 'taking', 'long']\n",
            "After lowercasing: ìï thk of wat to eat tonight.\n",
            "After removing special chars:  thk of wat to eat tonight\n",
            "After tokenization: ['thk', 'of', 'wat', 'to', 'eat', 'tonight']\n",
            "After stop word removal: ['thk', 'wat', 'eat', 'tonight']\n",
            "After lemmatization: ['thk', 'wat', 'eat', 'tonight']\n",
            "After lowercasing: thanx. yup we coming back on sun. finish dinner going back 2 hotel now. time flies, we're tog 4 exactly a mth today. hope we'll haf many more mths to come...\n",
            "After removing special chars: thanx yup we coming back on sun finish dinner going back  hotel now time flies were tog  exactly a mth today hope well haf many more mths to come\n",
            "After tokenization: ['thanx', 'yup', 'we', 'coming', 'back', 'on', 'sun', 'finish', 'dinner', 'going', 'back', 'hotel', 'now', 'time', 'flies', 'were', 'tog', 'exactly', 'a', 'mth', 'today', 'hope', 'well', 'haf', 'many', 'more', 'mths', 'to', 'come']\n",
            "After stop word removal: ['thanx', 'yup', 'coming', 'back', 'sun', 'finish', 'dinner', 'going', 'back', 'hotel', 'time', 'flies', 'tog', 'exactly', 'mth', 'today', 'hope', 'well', 'haf', 'many', 'mths', 'come']\n",
            "After lemmatization: ['thanx', 'yup', 'coming', 'back', 'sun', 'finish', 'dinner', 'going', 'back', 'hotel', 'time', 'fly', 'tog', 'exactly', 'mth', 'today', 'hope', 'well', 'haf', 'many', 'mths', 'come']\n",
            "After lowercasing: we're on the opposite side from where we dropped you off\n",
            "After removing special chars: were on the opposite side from where we dropped you off\n",
            "After tokenization: ['were', 'on', 'the', 'opposite', 'side', 'from', 'where', 'we', 'dropped', 'you', 'off']\n",
            "After stop word removal: ['opposite', 'side', 'dropped']\n",
            "After lemmatization: ['opposite', 'side', 'dropped']\n",
            "After lowercasing: yup. izzit still raining heavily cos i'm in e mrt i can't c outside.\n",
            "After removing special chars: yup izzit still raining heavily cos im in e mrt i cant c outside\n",
            "After tokenization: ['yup', 'izzit', 'still', 'raining', 'heavily', 'cos', 'im', 'in', 'e', 'mrt', 'i', 'cant', 'c', 'outside']\n",
            "After stop word removal: ['yup', 'izzit', 'still', 'raining', 'heavily', 'cos', 'im', 'e', 'mrt', 'cant', 'c', 'outside']\n",
            "After lemmatization: ['yup', 'izzit', 'still', 'raining', 'heavily', 'co', 'im', 'e', 'mrt', 'cant', 'c', 'outside']\n",
            "After lowercasing: send me your resume:-)\n",
            "After removing special chars: send me your resume\n",
            "After tokenization: ['send', 'me', 'your', 'resume']\n",
            "After stop word removal: ['send', 'resume']\n",
            "After lemmatization: ['send', 'resume']\n",
            "After lowercasing: gd luck 4 ur exams :-)\n",
            "After removing special chars: gd luck  ur exams \n",
            "After tokenization: ['gd', 'luck', 'ur', 'exams']\n",
            "After stop word removal: ['gd', 'luck', 'ur', 'exams']\n",
            "After lemmatization: ['gd', 'luck', 'ur', 'exam']\n",
            "After lowercasing: or u ask they all if next sat can a not. if all of them can make it then i'm ok lor.\n",
            "After removing special chars: or u ask they all if next sat can a not if all of them can make it then im ok lor\n",
            "After tokenization: ['or', 'u', 'ask', 'they', 'all', 'if', 'next', 'sat', 'can', 'a', 'not', 'if', 'all', 'of', 'them', 'can', 'make', 'it', 'then', 'im', 'ok', 'lor']\n",
            "After stop word removal: ['u', 'ask', 'next', 'sat', 'make', 'im', 'ok', 'lor']\n",
            "After lemmatization: ['u', 'ask', 'next', 'sat', 'make', 'im', 'ok', 'lor']\n",
            "After lowercasing: sorry that was my uncle. i.ll keep in touch\n",
            "After removing special chars: sorry that was my uncle ill keep in touch\n",
            "After tokenization: ['sorry', 'that', 'was', 'my', 'uncle', 'ill', 'keep', 'in', 'touch']\n",
            "After stop word removal: ['sorry', 'uncle', 'ill', 'keep', 'touch']\n",
            "After lemmatization: ['sorry', 'uncle', 'ill', 'keep', 'touch']\n",
            "After lowercasing: saw guys and dolls last night with patrick swayze it was great\n",
            "After removing special chars: saw guys and dolls last night with patrick swayze it was great\n",
            "After tokenization: ['saw', 'guys', 'and', 'dolls', 'last', 'night', 'with', 'patrick', 'swayze', 'it', 'was', 'great']\n",
            "After stop word removal: ['saw', 'guys', 'dolls', 'last', 'night', 'patrick', 'swayze', 'great']\n",
            "After lemmatization: ['saw', 'guy', 'doll', 'last', 'night', 'patrick', 'swayze', 'great']\n",
            "After lowercasing: urgent this is our 2nd attempt to contact u. your å£900 prize from yesterday is still awaiting collection. to claim call now 09061702893\n",
            "After removing special chars: urgent this is our nd attempt to contact u your  prize from yesterday is still awaiting collection to claim call now \n",
            "After tokenization: ['urgent', 'this', 'is', 'our', 'nd', 'attempt', 'to', 'contact', 'u', 'your', 'prize', 'from', 'yesterday', 'is', 'still', 'awaiting', 'collection', 'to', 'claim', 'call', 'now']\n",
            "After stop word removal: ['urgent', 'nd', 'attempt', 'contact', 'u', 'prize', 'yesterday', 'still', 'awaiting', 'collection', 'claim', 'call']\n",
            "After lemmatization: ['urgent', 'nd', 'attempt', 'contact', 'u', 'prize', 'yesterday', 'still', 'awaiting', 'collection', 'claim', 'call']\n",
            "After lowercasing: santa calling! would your little ones like a call from santa xmas eve? call 09077818151 to book you time. calls1.50ppm last 3mins 30s t&c www.santacalling.com\n",
            "After removing special chars: santa calling would your little ones like a call from santa xmas eve call  to book you time callsppm last mins s tc wwwsantacallingcom\n",
            "After tokenization: ['santa', 'calling', 'would', 'your', 'little', 'ones', 'like', 'a', 'call', 'from', 'santa', 'xmas', 'eve', 'call', 'to', 'book', 'you', 'time', 'callsppm', 'last', 'mins', 's', 'tc', 'wwwsantacallingcom']\n",
            "After stop word removal: ['santa', 'calling', 'would', 'little', 'ones', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time', 'callsppm', 'last', 'mins', 'tc', 'wwwsantacallingcom']\n",
            "After lemmatization: ['santa', 'calling', 'would', 'little', 'one', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time', 'callsppm', 'last', 'min', 'tc', 'wwwsantacallingcom']\n",
            "After lowercasing: just come home. i don't want u to be miserable\n",
            "After removing special chars: just come home i dont want u to be miserable\n",
            "After tokenization: ['just', 'come', 'home', 'i', 'dont', 'want', 'u', 'to', 'be', 'miserable']\n",
            "After stop word removal: ['come', 'home', 'dont', 'want', 'u', 'miserable']\n",
            "After lemmatization: ['come', 'home', 'dont', 'want', 'u', 'miserable']\n",
            "After lowercasing: i dont know why she.s not getting your messages\n",
            "After removing special chars: i dont know why shes not getting your messages\n",
            "After tokenization: ['i', 'dont', 'know', 'why', 'shes', 'not', 'getting', 'your', 'messages']\n",
            "After stop word removal: ['dont', 'know', 'shes', 'getting', 'messages']\n",
            "After lemmatization: ['dont', 'know', 'shes', 'getting', 'message']\n",
            "After lowercasing: its cool but tyler had to take off so we're gonna buy for him and drop it off at his place later tonight. our total order is a quarter, you got enough?\n",
            "After removing special chars: its cool but tyler had to take off so were gonna buy for him and drop it off at his place later tonight our total order is a quarter you got enough\n",
            "After tokenization: ['its', 'cool', 'but', 'tyler', 'had', 'to', 'take', 'off', 'so', 'were', 'gon', 'na', 'buy', 'for', 'him', 'and', 'drop', 'it', 'off', 'at', 'his', 'place', 'later', 'tonight', 'our', 'total', 'order', 'is', 'a', 'quarter', 'you', 'got', 'enough']\n",
            "After stop word removal: ['cool', 'tyler', 'take', 'gon', 'na', 'buy', 'drop', 'place', 'later', 'tonight', 'total', 'order', 'quarter', 'got', 'enough']\n",
            "After lemmatization: ['cool', 'tyler', 'take', 'gon', 'na', 'buy', 'drop', 'place', 'later', 'tonight', 'total', 'order', 'quarter', 'got', 'enough']\n",
            "After lowercasing: the guy at the car shop who was flirting with me got my phone number from the paperwork and called and texted me. i'm nervous because of course now he may have my address. should i call his boss and tell him, knowing this may get him fired?\n",
            "After removing special chars: the guy at the car shop who was flirting with me got my phone number from the paperwork and called and texted me im nervous because of course now he may have my address should i call his boss and tell him knowing this may get him fired\n",
            "After tokenization: ['the', 'guy', 'at', 'the', 'car', 'shop', 'who', 'was', 'flirting', 'with', 'me', 'got', 'my', 'phone', 'number', 'from', 'the', 'paperwork', 'and', 'called', 'and', 'texted', 'me', 'im', 'nervous', 'because', 'of', 'course', 'now', 'he', 'may', 'have', 'my', 'address', 'should', 'i', 'call', 'his', 'boss', 'and', 'tell', 'him', 'knowing', 'this', 'may', 'get', 'him', 'fired']\n",
            "After stop word removal: ['guy', 'car', 'shop', 'flirting', 'got', 'phone', 'number', 'paperwork', 'called', 'texted', 'im', 'nervous', 'course', 'may', 'address', 'call', 'boss', 'tell', 'knowing', 'may', 'get', 'fired']\n",
            "After lemmatization: ['guy', 'car', 'shop', 'flirting', 'got', 'phone', 'number', 'paperwork', 'called', 'texted', 'im', 'nervous', 'course', 'may', 'address', 'call', 'bos', 'tell', 'knowing', 'may', 'get', 'fired']\n",
            "After lowercasing: reverse is cheating. that is not mathematics.\n",
            "After removing special chars: reverse is cheating that is not mathematics\n",
            "After tokenization: ['reverse', 'is', 'cheating', 'that', 'is', 'not', 'mathematics']\n",
            "After stop word removal: ['reverse', 'cheating', 'mathematics']\n",
            "After lemmatization: ['reverse', 'cheating', 'mathematics']\n",
            "After lowercasing: how do you plan to manage that\n",
            "After removing special chars: how do you plan to manage that\n",
            "After tokenization: ['how', 'do', 'you', 'plan', 'to', 'manage', 'that']\n",
            "After stop word removal: ['plan', 'manage']\n",
            "After lemmatization: ['plan', 'manage']\n",
            "After lowercasing: er, hello, things didnû÷t quite go to plan ûò is limping slowly home followed by aa and with exhaust hanging off\n",
            "After removing special chars: er hello things didnt quite go to plan  is limping slowly home followed by aa and with exhaust hanging off\n",
            "After tokenization: ['er', 'hello', 'things', 'didnt', 'quite', 'go', 'to', 'plan', 'is', 'limping', 'slowly', 'home', 'followed', 'by', 'aa', 'and', 'with', 'exhaust', 'hanging', 'off']\n",
            "After stop word removal: ['er', 'hello', 'things', 'didnt', 'quite', 'go', 'plan', 'limping', 'slowly', 'home', 'followed', 'aa', 'exhaust', 'hanging']\n",
            "After lemmatization: ['er', 'hello', 'thing', 'didnt', 'quite', 'go', 'plan', 'limping', 'slowly', 'home', 'followed', 'aa', 'exhaust', 'hanging']\n",
            "After lowercasing: sorry for the delay. yes masters\n",
            "After removing special chars: sorry for the delay yes masters\n",
            "After tokenization: ['sorry', 'for', 'the', 'delay', 'yes', 'masters']\n",
            "After stop word removal: ['sorry', 'delay', 'yes', 'masters']\n",
            "After lemmatization: ['sorry', 'delay', 'yes', 'master']\n",
            "After lowercasing: call me when u finish then i come n pick u.\n",
            "After removing special chars: call me when u finish then i come n pick u\n",
            "After tokenization: ['call', 'me', 'when', 'u', 'finish', 'then', 'i', 'come', 'n', 'pick', 'u']\n",
            "After stop word removal: ['call', 'u', 'finish', 'come', 'n', 'pick', 'u']\n",
            "After lemmatization: ['call', 'u', 'finish', 'come', 'n', 'pick', 'u']\n",
            "After lowercasing: private! your 2004 account statement for 078498****7 shows 786 unredeemed bonus points. to claim call 08719180219 identifier code: 45239 expires 06.05.05\n",
            "After removing special chars: private your  account statement for  shows  unredeemed bonus points to claim call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'bonus', 'points', 'to', 'claim', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'bonus', 'points', 'claim', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'bonus', 'point', 'claim', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: what's up my own oga. left my phone at home and just saw ur messages. hope you are good. have a great weekend.\n",
            "After removing special chars: whats up my own oga left my phone at home and just saw ur messages hope you are good have a great weekend\n",
            "After tokenization: ['whats', 'up', 'my', 'own', 'oga', 'left', 'my', 'phone', 'at', 'home', 'and', 'just', 'saw', 'ur', 'messages', 'hope', 'you', 'are', 'good', 'have', 'a', 'great', 'weekend']\n",
            "After stop word removal: ['whats', 'oga', 'left', 'phone', 'home', 'saw', 'ur', 'messages', 'hope', 'good', 'great', 'weekend']\n",
            "After lemmatization: ['whats', 'oga', 'left', 'phone', 'home', 'saw', 'ur', 'message', 'hope', 'good', 'great', 'weekend']\n",
            "After lowercasing: don't worry though, i understand how important it is that i be put in my place with a poorly thought out punishment in the face of the worst thing that has ever happened to me. brb gonna go kill myself\n",
            "After removing special chars: dont worry though i understand how important it is that i be put in my place with a poorly thought out punishment in the face of the worst thing that has ever happened to me brb gonna go kill myself\n",
            "After tokenization: ['dont', 'worry', 'though', 'i', 'understand', 'how', 'important', 'it', 'is', 'that', 'i', 'be', 'put', 'in', 'my', 'place', 'with', 'a', 'poorly', 'thought', 'out', 'punishment', 'in', 'the', 'face', 'of', 'the', 'worst', 'thing', 'that', 'has', 'ever', 'happened', 'to', 'me', 'brb', 'gon', 'na', 'go', 'kill', 'myself']\n",
            "After stop word removal: ['dont', 'worry', 'though', 'understand', 'important', 'put', 'place', 'poorly', 'thought', 'punishment', 'face', 'worst', 'thing', 'ever', 'happened', 'brb', 'gon', 'na', 'go', 'kill']\n",
            "After lemmatization: ['dont', 'worry', 'though', 'understand', 'important', 'put', 'place', 'poorly', 'thought', 'punishment', 'face', 'worst', 'thing', 'ever', 'happened', 'brb', 'gon', 'na', 'go', 'kill']\n",
            "After lowercasing: honey, can you pls find out how much they sell predicte in nigeria. and how many times can it be used. its very important to have a reply before monday\n",
            "After removing special chars: honey can you pls find out how much they sell predicte in nigeria and how many times can it be used its very important to have a reply before monday\n",
            "After tokenization: ['honey', 'can', 'you', 'pls', 'find', 'out', 'how', 'much', 'they', 'sell', 'predicte', 'in', 'nigeria', 'and', 'how', 'many', 'times', 'can', 'it', 'be', 'used', 'its', 'very', 'important', 'to', 'have', 'a', 'reply', 'before', 'monday']\n",
            "After stop word removal: ['honey', 'pls', 'find', 'much', 'sell', 'predicte', 'nigeria', 'many', 'times', 'used', 'important', 'reply', 'monday']\n",
            "After lemmatization: ['honey', 'pls', 'find', 'much', 'sell', 'predicte', 'nigeria', 'many', 'time', 'used', 'important', 'reply', 'monday']\n",
            "After lowercasing: e admin building there? i might b slightly earlier... i'll call u when i'm reaching...\n",
            "After removing special chars: e admin building there i might b slightly earlier ill call u when im reaching\n",
            "After tokenization: ['e', 'admin', 'building', 'there', 'i', 'might', 'b', 'slightly', 'earlier', 'ill', 'call', 'u', 'when', 'im', 'reaching']\n",
            "After stop word removal: ['e', 'admin', 'building', 'might', 'b', 'slightly', 'earlier', 'ill', 'call', 'u', 'im', 'reaching']\n",
            "After lemmatization: ['e', 'admin', 'building', 'might', 'b', 'slightly', 'earlier', 'ill', 'call', 'u', 'im', 'reaching']\n",
            "After lowercasing: fyi i'm at usf now, swing by the room whenever\n",
            "After removing special chars: fyi im at usf now swing by the room whenever\n",
            "After tokenization: ['fyi', 'im', 'at', 'usf', 'now', 'swing', 'by', 'the', 'room', 'whenever']\n",
            "After stop word removal: ['fyi', 'im', 'usf', 'swing', 'room', 'whenever']\n",
            "After lemmatization: ['fyi', 'im', 'usf', 'swing', 'room', 'whenever']\n",
            "After lowercasing: i can call in  &lt;#&gt;  min if thats ok\n",
            "After removing special chars: i can call in  ltgt  min if thats ok\n",
            "After tokenization: ['i', 'can', 'call', 'in', 'ltgt', 'min', 'if', 'thats', 'ok']\n",
            "After stop word removal: ['call', 'ltgt', 'min', 'thats', 'ok']\n",
            "After lemmatization: ['call', 'ltgt', 'min', 'thats', 'ok']\n",
            "After lowercasing: ummmmmaah many many happy returns of d day my dear sweet heart.. happy birthday dear\n",
            "After removing special chars: ummmmmaah many many happy returns of d day my dear sweet heart happy birthday dear\n",
            "After tokenization: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'of', 'd', 'day', 'my', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
            "After stop word removal: ['ummmmmaah', 'many', 'many', 'happy', 'returns', 'day', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
            "After lemmatization: ['ummmmmaah', 'many', 'many', 'happy', 'return', 'day', 'dear', 'sweet', 'heart', 'happy', 'birthday', 'dear']\n",
            "After lowercasing: ìï no home work to do meh... \n",
            "After removing special chars:  no home work to do meh \n",
            "After tokenization: ['no', 'home', 'work', 'to', 'do', 'meh']\n",
            "After stop word removal: ['home', 'work', 'meh']\n",
            "After lemmatization: ['home', 'work', 'meh']\n",
            "After lowercasing: anything is valuable in only 2 situations: first- before getting it... second- after loosing it...\n",
            "After removing special chars: anything is valuable in only  situations first before getting it second after loosing it\n",
            "After tokenization: ['anything', 'is', 'valuable', 'in', 'only', 'situations', 'first', 'before', 'getting', 'it', 'second', 'after', 'loosing', 'it']\n",
            "After stop word removal: ['anything', 'valuable', 'situations', 'first', 'getting', 'second', 'loosing']\n",
            "After lemmatization: ['anything', 'valuable', 'situation', 'first', 'getting', 'second', 'loosing']\n",
            "After lowercasing: me too. mark is taking forever to pick up my prescription and the pain is coming back.\n",
            "After removing special chars: me too mark is taking forever to pick up my prescription and the pain is coming back\n",
            "After tokenization: ['me', 'too', 'mark', 'is', 'taking', 'forever', 'to', 'pick', 'up', 'my', 'prescription', 'and', 'the', 'pain', 'is', 'coming', 'back']\n",
            "After stop word removal: ['mark', 'taking', 'forever', 'pick', 'prescription', 'pain', 'coming', 'back']\n",
            "After lemmatization: ['mark', 'taking', 'forever', 'pick', 'prescription', 'pain', 'coming', 'back']\n",
            "After lowercasing: how's ur paper?\n",
            "After removing special chars: hows ur paper\n",
            "After tokenization: ['hows', 'ur', 'paper']\n",
            "After stop word removal: ['hows', 'ur', 'paper']\n",
            "After lemmatization: ['hows', 'ur', 'paper']\n",
            "After lowercasing: got smaller capacity one? quite ex...\n",
            "After removing special chars: got smaller capacity one quite ex\n",
            "After tokenization: ['got', 'smaller', 'capacity', 'one', 'quite', 'ex']\n",
            "After stop word removal: ['got', 'smaller', 'capacity', 'one', 'quite', 'ex']\n",
            "After lemmatization: ['got', 'smaller', 'capacity', 'one', 'quite', 'ex']\n",
            "After lowercasing: check out choose your babe videos @ sms.shsex.netun fgkslpopw fgkslpo\n",
            "After removing special chars: check out choose your babe videos  smsshsexnetun fgkslpopw fgkslpo\n",
            "After tokenization: ['check', 'out', 'choose', 'your', 'babe', 'videos', 'smsshsexnetun', 'fgkslpopw', 'fgkslpo']\n",
            "After stop word removal: ['check', 'choose', 'babe', 'videos', 'smsshsexnetun', 'fgkslpopw', 'fgkslpo']\n",
            "After lemmatization: ['check', 'choose', 'babe', 'video', 'smsshsexnetun', 'fgkslpopw', 'fgkslpo']\n",
            "After lowercasing: im good! i have been thinking about you...\n",
            "After removing special chars: im good i have been thinking about you\n",
            "After tokenization: ['im', 'good', 'i', 'have', 'been', 'thinking', 'about', 'you']\n",
            "After stop word removal: ['im', 'good', 'thinking']\n",
            "After lemmatization: ['im', 'good', 'thinking']\n",
            "After lowercasing: u r a winner u ave been specially selected 2 receive å£1000 cash or a 4* holiday (flights inc) speak to a live operator 2 claim 0871277810710p/min (18 )\n",
            "After removing special chars: u r a winner u ave been specially selected  receive  cash or a  holiday flights inc speak to a live operator  claim pmin  \n",
            "After tokenization: ['u', 'r', 'a', 'winner', 'u', 'ave', 'been', 'specially', 'selected', 'receive', 'cash', 'or', 'a', 'holiday', 'flights', 'inc', 'speak', 'to', 'a', 'live', 'operator', 'claim', 'pmin']\n",
            "After stop word removal: ['u', 'r', 'winner', 'u', 'ave', 'specially', 'selected', 'receive', 'cash', 'holiday', 'flights', 'inc', 'speak', 'live', 'operator', 'claim', 'pmin']\n",
            "After lemmatization: ['u', 'r', 'winner', 'u', 'ave', 'specially', 'selected', 'receive', 'cash', 'holiday', 'flight', 'inc', 'speak', 'live', 'operator', 'claim', 'pmin']\n",
            "After lowercasing: :-) :-)\n",
            "After removing special chars:  \n",
            "After tokenization: []\n",
            "After stop word removal: []\n",
            "After lemmatization: []\n",
            "After lowercasing: not thought bout it... || drink in tap & spile at seven. || is that pub on gas st off broad st by canal. || ok?\n",
            "After removing special chars: not thought bout it  drink in tap  spile at seven  is that pub on gas st off broad st by canal  ok\n",
            "After tokenization: ['not', 'thought', 'bout', 'it', 'drink', 'in', 'tap', 'spile', 'at', 'seven', 'is', 'that', 'pub', 'on', 'gas', 'st', 'off', 'broad', 'st', 'by', 'canal', 'ok']\n",
            "After stop word removal: ['thought', 'bout', 'drink', 'tap', 'spile', 'seven', 'pub', 'gas', 'st', 'broad', 'st', 'canal', 'ok']\n",
            "After lemmatization: ['thought', 'bout', 'drink', 'tap', 'spile', 'seven', 'pub', 'gas', 'st', 'broad', 'st', 'canal', 'ok']\n",
            "After lowercasing: i am going to sleep. i am tired of travel.\n",
            "After removing special chars: i am going to sleep i am tired of travel\n",
            "After tokenization: ['i', 'am', 'going', 'to', 'sleep', 'i', 'am', 'tired', 'of', 'travel']\n",
            "After stop word removal: ['going', 'sleep', 'tired', 'travel']\n",
            "After lemmatization: ['going', 'sleep', 'tired', 'travel']\n",
            "After lowercasing: haha, just what i was thinkin\n",
            "After removing special chars: haha just what i was thinkin\n",
            "After tokenization: ['haha', 'just', 'what', 'i', 'was', 'thinkin']\n",
            "After stop word removal: ['haha', 'thinkin']\n",
            "After lemmatization: ['haha', 'thinkin']\n",
            "After lowercasing: yup but it's not giving me problems now so mayb i'll jus leave it...\n",
            "After removing special chars: yup but its not giving me problems now so mayb ill jus leave it\n",
            "After tokenization: ['yup', 'but', 'its', 'not', 'giving', 'me', 'problems', 'now', 'so', 'mayb', 'ill', 'jus', 'leave', 'it']\n",
            "After stop word removal: ['yup', 'giving', 'problems', 'mayb', 'ill', 'jus', 'leave']\n",
            "After lemmatization: ['yup', 'giving', 'problem', 'mayb', 'ill', 'jus', 'leave']\n",
            "After lowercasing: lol no. just trying to make your day a little more interesting\n",
            "After removing special chars: lol no just trying to make your day a little more interesting\n",
            "After tokenization: ['lol', 'no', 'just', 'trying', 'to', 'make', 'your', 'day', 'a', 'little', 'more', 'interesting']\n",
            "After stop word removal: ['lol', 'trying', 'make', 'day', 'little', 'interesting']\n",
            "After lemmatization: ['lol', 'trying', 'make', 'day', 'little', 'interesting']\n",
            "After lowercasing: how long before you get reply, just defer admission til next semester\n",
            "After removing special chars: how long before you get reply just defer admission til next semester\n",
            "After tokenization: ['how', 'long', 'before', 'you', 'get', 'reply', 'just', 'defer', 'admission', 'til', 'next', 'semester']\n",
            "After stop word removal: ['long', 'get', 'reply', 'defer', 'admission', 'til', 'next', 'semester']\n",
            "After lemmatization: ['long', 'get', 'reply', 'defer', 'admission', 'til', 'next', 'semester']\n",
            "After lowercasing: the word \\checkmate\\\" in chess comes from the persian phrase \\\"shah maat\\\" which means; \\\"the king is dead..\\\" goodmorning.. have a good day..:)\"\n",
            "After removing special chars: the word checkmate in chess comes from the persian phrase shah maat which means the king is dead goodmorning have a good day\n",
            "After tokenization: ['the', 'word', 'checkmate', 'in', 'chess', 'comes', 'from', 'the', 'persian', 'phrase', 'shah', 'maat', 'which', 'means', 'the', 'king', 'is', 'dead', 'goodmorning', 'have', 'a', 'good', 'day']\n",
            "After stop word removal: ['word', 'checkmate', 'chess', 'comes', 'persian', 'phrase', 'shah', 'maat', 'means', 'king', 'dead', 'goodmorning', 'good', 'day']\n",
            "After lemmatization: ['word', 'checkmate', 'chess', 'come', 'persian', 'phrase', 'shah', 'maat', 'mean', 'king', 'dead', 'goodmorning', 'good', 'day']\n",
            "After lowercasing: po de :-):):-):-):-). no need job aha.\n",
            "After removing special chars: po de  no need job aha\n",
            "After tokenization: ['po', 'de', 'no', 'need', 'job', 'aha']\n",
            "After stop word removal: ['po', 'de', 'need', 'job', 'aha']\n",
            "After lemmatization: ['po', 'de', 'need', 'job', 'aha']\n",
            "After lowercasing: rats. hey did u ever vote for the next themes?\n",
            "After removing special chars: rats hey did u ever vote for the next themes\n",
            "After tokenization: ['rats', 'hey', 'did', 'u', 'ever', 'vote', 'for', 'the', 'next', 'themes']\n",
            "After stop word removal: ['rats', 'hey', 'u', 'ever', 'vote', 'next', 'themes']\n",
            "After lemmatization: ['rat', 'hey', 'u', 'ever', 'vote', 'next', 'theme']\n",
            "After lowercasing: new mobiles from 2004, must go! txt: nokia to no: 89545 & collect yours today! from only å£1. www.4-tc.biz 2optout 087187262701.50gbp/mtmsg18 txtauction.\n",
            "After removing special chars: new mobiles from  must go txt nokia to no   collect yours today from only  wwwtcbiz optout gbpmtmsg txtauction\n",
            "After tokenization: ['new', 'mobiles', 'from', 'must', 'go', 'txt', 'nokia', 'to', 'no', 'collect', 'yours', 'today', 'from', 'only', 'wwwtcbiz', 'optout', 'gbpmtmsg', 'txtauction']\n",
            "After stop word removal: ['new', 'mobiles', 'must', 'go', 'txt', 'nokia', 'collect', 'today', 'wwwtcbiz', 'optout', 'gbpmtmsg', 'txtauction']\n",
            "After lemmatization: ['new', 'mobile', 'must', 'go', 'txt', 'nokia', 'collect', 'today', 'wwwtcbiz', 'optout', 'gbpmtmsg', 'txtauction']\n",
            "After lowercasing: i hope your pee burns tonite.\n",
            "After removing special chars: i hope your pee burns tonite\n",
            "After tokenization: ['i', 'hope', 'your', 'pee', 'burns', 'tonite']\n",
            "After stop word removal: ['hope', 'pee', 'burns', 'tonite']\n",
            "After lemmatization: ['hope', 'pee', 'burn', 'tonite']\n",
            "After lowercasing: oh rite. well im with my best mate pete, who i went out with 4 a week+ now were 2geva again. its been longer than a week.\n",
            "After removing special chars: oh rite well im with my best mate pete who i went out with  a week now were geva again its been longer than a week\n",
            "After tokenization: ['oh', 'rite', 'well', 'im', 'with', 'my', 'best', 'mate', 'pete', 'who', 'i', 'went', 'out', 'with', 'a', 'week', 'now', 'were', 'geva', 'again', 'its', 'been', 'longer', 'than', 'a', 'week']\n",
            "After stop word removal: ['oh', 'rite', 'well', 'im', 'best', 'mate', 'pete', 'went', 'week', 'geva', 'longer', 'week']\n",
            "After lemmatization: ['oh', 'rite', 'well', 'im', 'best', 'mate', 'pete', 'went', 'week', 'geva', 'longer', 'week']\n",
            "After lowercasing: yay can't wait to party together!\n",
            "After removing special chars: yay cant wait to party together\n",
            "After tokenization: ['yay', 'cant', 'wait', 'to', 'party', 'together']\n",
            "After stop word removal: ['yay', 'cant', 'wait', 'party', 'together']\n",
            "After lemmatization: ['yay', 'cant', 'wait', 'party', 'together']\n",
            "After lowercasing: ....photoshop makes my computer shut down.\n",
            "After removing special chars: photoshop makes my computer shut down\n",
            "After tokenization: ['photoshop', 'makes', 'my', 'computer', 'shut', 'down']\n",
            "After stop word removal: ['photoshop', 'makes', 'computer', 'shut']\n",
            "After lemmatization: ['photoshop', 'make', 'computer', 'shut']\n",
            "After lowercasing: all boys made fun of me today. ok i have no problem. i just sent one message just for fun\n",
            "After removing special chars: all boys made fun of me today ok i have no problem i just sent one message just for fun\n",
            "After tokenization: ['all', 'boys', 'made', 'fun', 'of', 'me', 'today', 'ok', 'i', 'have', 'no', 'problem', 'i', 'just', 'sent', 'one', 'message', 'just', 'for', 'fun']\n",
            "After stop word removal: ['boys', 'made', 'fun', 'today', 'ok', 'problem', 'sent', 'one', 'message', 'fun']\n",
            "After lemmatization: ['boy', 'made', 'fun', 'today', 'ok', 'problem', 'sent', 'one', 'message', 'fun']\n",
            "After lowercasing: that's one of the issues but california is okay. no snow so its manageable\n",
            "After removing special chars: thats one of the issues but california is okay no snow so its manageable\n",
            "After tokenization: ['thats', 'one', 'of', 'the', 'issues', 'but', 'california', 'is', 'okay', 'no', 'snow', 'so', 'its', 'manageable']\n",
            "After stop word removal: ['thats', 'one', 'issues', 'california', 'okay', 'snow', 'manageable']\n",
            "After lemmatization: ['thats', 'one', 'issue', 'california', 'okay', 'snow', 'manageable']\n",
            "After lowercasing: private! your 2003 account statement for shows 800 un-redeemed s. i. m. points. call 08715203652 identifier code: 42810 expires 29/10/0\n",
            "After removing special chars: private your  account statement for shows  unredeemed s i m points call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'point', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: hmmm.... mayb can try e shoppin area one, but forgot e name of hotel...\n",
            "After removing special chars: hmmm mayb can try e shoppin area one but forgot e name of hotel\n",
            "After tokenization: ['hmmm', 'mayb', 'can', 'try', 'e', 'shoppin', 'area', 'one', 'but', 'forgot', 'e', 'name', 'of', 'hotel']\n",
            "After stop word removal: ['hmmm', 'mayb', 'try', 'e', 'shoppin', 'area', 'one', 'forgot', 'e', 'name', 'hotel']\n",
            "After lemmatization: ['hmmm', 'mayb', 'try', 'e', 'shoppin', 'area', 'one', 'forgot', 'e', 'name', 'hotel']\n",
            "After lowercasing: awesome, that gonna be soon or later tonight?\n",
            "After removing special chars: awesome that gonna be soon or later tonight\n",
            "After tokenization: ['awesome', 'that', 'gon', 'na', 'be', 'soon', 'or', 'later', 'tonight']\n",
            "After stop word removal: ['awesome', 'gon', 'na', 'soon', 'later', 'tonight']\n",
            "After lemmatization: ['awesome', 'gon', 'na', 'soon', 'later', 'tonight']\n",
            "After lowercasing: i need details about that online job.\n",
            "After removing special chars: i need details about that online job\n",
            "After tokenization: ['i', 'need', 'details', 'about', 'that', 'online', 'job']\n",
            "After stop word removal: ['need', 'details', 'online', 'job']\n",
            "After lemmatization: ['need', 'detail', 'online', 'job']\n",
            "After lowercasing: you have won! as a valued vodafone customer our computer has picked you to win a å£150 prize. to collect is easy. just call 09061743386 \n",
            "After removing special chars: you have won as a valued vodafone customer our computer has picked you to win a  prize to collect is easy just call  \n",
            "After tokenization: ['you', 'have', 'won', 'as', 'a', 'valued', 'vodafone', 'customer', 'our', 'computer', 'has', 'picked', 'you', 'to', 'win', 'a', 'prize', 'to', 'collect', 'is', 'easy', 'just', 'call']\n",
            "After stop word removal: ['valued', 'vodafone', 'customer', 'computer', 'picked', 'win', 'prize', 'collect', 'easy', 'call']\n",
            "After lemmatization: ['valued', 'vodafone', 'customer', 'computer', 'picked', 'win', 'prize', 'collect', 'easy', 'call']\n",
            "After lowercasing: missing you too.pray inshah allah\n",
            "After removing special chars: missing you toopray inshah allah\n",
            "After tokenization: ['missing', 'you', 'toopray', 'inshah', 'allah']\n",
            "After stop word removal: ['missing', 'toopray', 'inshah', 'allah']\n",
            "After lemmatization: ['missing', 'toopray', 'inshah', 'allah']\n",
            "After lowercasing: pls help me tell ashley that i cant find her number oh\n",
            "After removing special chars: pls help me tell ashley that i cant find her number oh\n",
            "After tokenization: ['pls', 'help', 'me', 'tell', 'ashley', 'that', 'i', 'cant', 'find', 'her', 'number', 'oh']\n",
            "After stop word removal: ['pls', 'help', 'tell', 'ashley', 'cant', 'find', 'number', 'oh']\n",
            "After lemmatization: ['pls', 'help', 'tell', 'ashley', 'cant', 'find', 'number', 'oh']\n",
            "After lowercasing: i am in escape theatre now. . going to watch kavalan in a few minutes\n",
            "After removing special chars: i am in escape theatre now  going to watch kavalan in a few minutes\n",
            "After tokenization: ['i', 'am', 'in', 'escape', 'theatre', 'now', 'going', 'to', 'watch', 'kavalan', 'in', 'a', 'few', 'minutes']\n",
            "After stop word removal: ['escape', 'theatre', 'going', 'watch', 'kavalan', 'minutes']\n",
            "After lemmatization: ['escape', 'theatre', 'going', 'watch', 'kavalan', 'minute']\n",
            "After lowercasing: s.this will increase the chance of winning.\n",
            "After removing special chars: sthis will increase the chance of winning\n",
            "After tokenization: ['sthis', 'will', 'increase', 'the', 'chance', 'of', 'winning']\n",
            "After stop word removal: ['sthis', 'increase', 'chance', 'winning']\n",
            "After lemmatization: ['sthis', 'increase', 'chance', 'winning']\n",
            "After lowercasing: either way works for me. i am  &lt;#&gt;  years old. hope that doesnt bother you.\n",
            "After removing special chars: either way works for me i am  ltgt  years old hope that doesnt bother you\n",
            "After tokenization: ['either', 'way', 'works', 'for', 'me', 'i', 'am', 'ltgt', 'years', 'old', 'hope', 'that', 'doesnt', 'bother', 'you']\n",
            "After stop word removal: ['either', 'way', 'works', 'ltgt', 'years', 'old', 'hope', 'doesnt', 'bother']\n",
            "After lemmatization: ['either', 'way', 'work', 'ltgt', 'year', 'old', 'hope', 'doesnt', 'bother']\n",
            "After lowercasing: maybe you should find something else to do instead???\n",
            "After removing special chars: maybe you should find something else to do instead\n",
            "After tokenization: ['maybe', 'you', 'should', 'find', 'something', 'else', 'to', 'do', 'instead']\n",
            "After stop word removal: ['maybe', 'find', 'something', 'else', 'instead']\n",
            "After lemmatization: ['maybe', 'find', 'something', 'else', 'instead']\n",
            "After lowercasing: gain the rights of a wife.dont demand it.i am trying as husband too.lets see\n",
            "After removing special chars: gain the rights of a wifedont demand iti am trying as husband toolets see\n",
            "After tokenization: ['gain', 'the', 'rights', 'of', 'a', 'wifedont', 'demand', 'iti', 'am', 'trying', 'as', 'husband', 'toolets', 'see']\n",
            "After stop word removal: ['gain', 'rights', 'wifedont', 'demand', 'iti', 'trying', 'husband', 'toolets', 'see']\n",
            "After lemmatization: ['gain', 'right', 'wifedont', 'demand', 'iti', 'trying', 'husband', 'toolets', 'see']\n",
            "After lowercasing: i liked your new house\n",
            "After removing special chars: i liked your new house\n",
            "After tokenization: ['i', 'liked', 'your', 'new', 'house']\n",
            "After stop word removal: ['liked', 'new', 'house']\n",
            "After lemmatization: ['liked', 'new', 'house']\n",
            "After lowercasing: i'm fine. hope you are also\n",
            "After removing special chars: im fine hope you are also\n",
            "After tokenization: ['im', 'fine', 'hope', 'you', 'are', 'also']\n",
            "After stop word removal: ['im', 'fine', 'hope', 'also']\n",
            "After lemmatization: ['im', 'fine', 'hope', 'also']\n",
            "After lowercasing: also north carolina and texas atm, you would just go to the gre site and pay for the test results to be sent.\n",
            "After removing special chars: also north carolina and texas atm you would just go to the gre site and pay for the test results to be sent\n",
            "After tokenization: ['also', 'north', 'carolina', 'and', 'texas', 'atm', 'you', 'would', 'just', 'go', 'to', 'the', 'gre', 'site', 'and', 'pay', 'for', 'the', 'test', 'results', 'to', 'be', 'sent']\n",
            "After stop word removal: ['also', 'north', 'carolina', 'texas', 'atm', 'would', 'go', 'gre', 'site', 'pay', 'test', 'results', 'sent']\n",
            "After lemmatization: ['also', 'north', 'carolina', 'texas', 'atm', 'would', 'go', 'gre', 'site', 'pay', 'test', 'result', 'sent']\n",
            "After lowercasing: same to u...\n",
            "After removing special chars: same to u\n",
            "After tokenization: ['same', 'to', 'u']\n",
            "After stop word removal: ['u']\n",
            "After lemmatization: ['u']\n",
            "After lowercasing: yes baby! i need to stretch open your pussy!\n",
            "After removing special chars: yes baby i need to stretch open your pussy\n",
            "After tokenization: ['yes', 'baby', 'i', 'need', 'to', 'stretch', 'open', 'your', 'pussy']\n",
            "After stop word removal: ['yes', 'baby', 'need', 'stretch', 'open', 'pussy']\n",
            "After lemmatization: ['yes', 'baby', 'need', 'stretch', 'open', 'pussy']\n",
            "After lowercasing: thanks  and ! or bomb and date as my phone wanted to say! \n",
            "After removing special chars: thanks  and  or bomb and date as my phone wanted to say \n",
            "After tokenization: ['thanks', 'and', 'or', 'bomb', 'and', 'date', 'as', 'my', 'phone', 'wanted', 'to', 'say']\n",
            "After stop word removal: ['thanks', 'bomb', 'date', 'phone', 'wanted', 'say']\n",
            "After lemmatization: ['thanks', 'bomb', 'date', 'phone', 'wanted', 'say']\n",
            "After lowercasing: ok...\n",
            "After removing special chars: ok\n",
            "After tokenization: ['ok']\n",
            "After stop word removal: ['ok']\n",
            "After lemmatization: ['ok']\n",
            "After lowercasing: hey, a guy i know is breathing down my neck to get him some bud, anyway you'd be able to get a half track to usf tonight?\n",
            "After removing special chars: hey a guy i know is breathing down my neck to get him some bud anyway youd be able to get a half track to usf tonight\n",
            "After tokenization: ['hey', 'a', 'guy', 'i', 'know', 'is', 'breathing', 'down', 'my', 'neck', 'to', 'get', 'him', 'some', 'bud', 'anyway', 'youd', 'be', 'able', 'to', 'get', 'a', 'half', 'track', 'to', 'usf', 'tonight']\n",
            "After stop word removal: ['hey', 'guy', 'know', 'breathing', 'neck', 'get', 'bud', 'anyway', 'youd', 'able', 'get', 'half', 'track', 'usf', 'tonight']\n",
            "After lemmatization: ['hey', 'guy', 'know', 'breathing', 'neck', 'get', 'bud', 'anyway', 'youd', 'able', 'get', 'half', 'track', 'usf', 'tonight']\n",
            "After lowercasing: \\response\\\" is one of d powerful weapon 2 occupy a place in others 'heart'... so\n",
            "After removing special chars: response is one of d powerful weapon  occupy a place in others heart so\n",
            "After tokenization: ['response', 'is', 'one', 'of', 'd', 'powerful', 'weapon', 'occupy', 'a', 'place', 'in', 'others', 'heart', 'so']\n",
            "After stop word removal: ['response', 'one', 'powerful', 'weapon', 'occupy', 'place', 'others', 'heart']\n",
            "After lemmatization: ['response', 'one', 'powerful', 'weapon', 'occupy', 'place', 'others', 'heart']\n",
            "After lowercasing: nokia phone is lovly..\n",
            "After removing special chars: nokia phone is lovly\n",
            "After tokenization: ['nokia', 'phone', 'is', 'lovly']\n",
            "After stop word removal: ['nokia', 'phone', 'lovly']\n",
            "After lemmatization: ['nokia', 'phone', 'lovly']\n",
            "After lowercasing: **free message**thanks for using the auction subscription service. 18 . 150p/msgrcvd 2 skip an auction txt out. 2 unsubscribe txt stop customercare 08718726270\n",
            "After removing special chars: free messagethanks for using the auction subscription service   pmsgrcvd  skip an auction txt out  unsubscribe txt stop customercare \n",
            "After tokenization: ['free', 'messagethanks', 'for', 'using', 'the', 'auction', 'subscription', 'service', 'pmsgrcvd', 'skip', 'an', 'auction', 'txt', 'out', 'unsubscribe', 'txt', 'stop', 'customercare']\n",
            "After stop word removal: ['free', 'messagethanks', 'using', 'auction', 'subscription', 'service', 'pmsgrcvd', 'skip', 'auction', 'txt', 'unsubscribe', 'txt', 'stop', 'customercare']\n",
            "After lemmatization: ['free', 'messagethanks', 'using', 'auction', 'subscription', 'service', 'pmsgrcvd', 'skip', 'auction', 'txt', 'unsubscribe', 'txt', 'stop', 'customercare']\n",
            "After lowercasing: bored housewives! chat n date now! 0871750.77.11! bt-national rate 10p/min only from landlines!\n",
            "After removing special chars: bored housewives chat n date now  btnational rate pmin only from landlines\n",
            "After tokenization: ['bored', 'housewives', 'chat', 'n', 'date', 'now', 'btnational', 'rate', 'pmin', 'only', 'from', 'landlines']\n",
            "After stop word removal: ['bored', 'housewives', 'chat', 'n', 'date', 'btnational', 'rate', 'pmin', 'landlines']\n",
            "After lemmatization: ['bored', 'housewife', 'chat', 'n', 'date', 'btnational', 'rate', 'pmin', 'landline']\n",
            "After lowercasing: sorry da..today i wont come to play..i have driving clas..\n",
            "After removing special chars: sorry datoday i wont come to playi have driving clas\n",
            "After tokenization: ['sorry', 'datoday', 'i', 'wont', 'come', 'to', 'playi', 'have', 'driving', 'clas']\n",
            "After stop word removal: ['sorry', 'datoday', 'wont', 'come', 'playi', 'driving', 'clas']\n",
            "After lemmatization: ['sorry', 'datoday', 'wont', 'come', 'playi', 'driving', 'clas']\n",
            "After lowercasing: i'm really sorry i lit your hair on fire\n",
            "After removing special chars: im really sorry i lit your hair on fire\n",
            "After tokenization: ['im', 'really', 'sorry', 'i', 'lit', 'your', 'hair', 'on', 'fire']\n",
            "After stop word removal: ['im', 'really', 'sorry', 'lit', 'hair', 'fire']\n",
            "After lemmatization: ['im', 'really', 'sorry', 'lit', 'hair', 'fire']\n",
            "After lowercasing: oh! shit, i thought that was your trip! loooooool ... that just makes so much more sense now ... *grins* and the sofa reference was ... the \\sleep on a couch\\\" link you sent me ... wasn't that how you went on your trip ? oh ... and didn't your babe go with you for that celebration with your rents?\"\n",
            "After removing special chars: oh shit i thought that was your trip loooooool  that just makes so much more sense now  grins and the sofa reference was  the sleep on a couch link you sent me  wasnt that how you went on your trip  oh  and didnt your babe go with you for that celebration with your rents\n",
            "After tokenization: ['oh', 'shit', 'i', 'thought', 'that', 'was', 'your', 'trip', 'loooooool', 'that', 'just', 'makes', 'so', 'much', 'more', 'sense', 'now', 'grins', 'and', 'the', 'sofa', 'reference', 'was', 'the', 'sleep', 'on', 'a', 'couch', 'link', 'you', 'sent', 'me', 'wasnt', 'that', 'how', 'you', 'went', 'on', 'your', 'trip', 'oh', 'and', 'didnt', 'your', 'babe', 'go', 'with', 'you', 'for', 'that', 'celebration', 'with', 'your', 'rents']\n",
            "After stop word removal: ['oh', 'shit', 'thought', 'trip', 'loooooool', 'makes', 'much', 'sense', 'grins', 'sofa', 'reference', 'sleep', 'couch', 'link', 'sent', 'wasnt', 'went', 'trip', 'oh', 'didnt', 'babe', 'go', 'celebration', 'rents']\n",
            "After lemmatization: ['oh', 'shit', 'thought', 'trip', 'loooooool', 'make', 'much', 'sense', 'grin', 'sofa', 'reference', 'sleep', 'couch', 'link', 'sent', 'wasnt', 'went', 'trip', 'oh', 'didnt', 'babe', 'go', 'celebration', 'rent']\n",
            "After lowercasing: okey dokey swashbuckling stuff what oh.\n",
            "After removing special chars: okey dokey swashbuckling stuff what oh\n",
            "After tokenization: ['okey', 'dokey', 'swashbuckling', 'stuff', 'what', 'oh']\n",
            "After stop word removal: ['okey', 'dokey', 'swashbuckling', 'stuff', 'oh']\n",
            "After lemmatization: ['okey', 'dokey', 'swashbuckling', 'stuff', 'oh']\n",
            "After lowercasing: watching cartoon, listening music &amp; at eve had to go temple &amp; church.. what about u?\n",
            "After removing special chars: watching cartoon listening music amp at eve had to go temple amp church what about u\n",
            "After tokenization: ['watching', 'cartoon', 'listening', 'music', 'amp', 'at', 'eve', 'had', 'to', 'go', 'temple', 'amp', 'church', 'what', 'about', 'u']\n",
            "After stop word removal: ['watching', 'cartoon', 'listening', 'music', 'amp', 'eve', 'go', 'temple', 'amp', 'church', 'u']\n",
            "After lemmatization: ['watching', 'cartoon', 'listening', 'music', 'amp', 'eve', 'go', 'temple', 'amp', 'church', 'u']\n",
            "After lowercasing: 1. tension face 2. smiling face 3. waste face 4. innocent face 5.terror face 6.cruel face 7.romantic face 8.lovable face 9.decent face  &lt;#&gt; .joker face.\n",
            "After removing special chars:  tension face  smiling face  waste face  innocent face terror face cruel face romantic face lovable face decent face  ltgt joker face\n",
            "After tokenization: ['tension', 'face', 'smiling', 'face', 'waste', 'face', 'innocent', 'face', 'terror', 'face', 'cruel', 'face', 'romantic', 'face', 'lovable', 'face', 'decent', 'face', 'ltgt', 'joker', 'face']\n",
            "After stop word removal: ['tension', 'face', 'smiling', 'face', 'waste', 'face', 'innocent', 'face', 'terror', 'face', 'cruel', 'face', 'romantic', 'face', 'lovable', 'face', 'decent', 'face', 'ltgt', 'joker', 'face']\n",
            "After lemmatization: ['tension', 'face', 'smiling', 'face', 'waste', 'face', 'innocent', 'face', 'terror', 'face', 'cruel', 'face', 'romantic', 'face', 'lovable', 'face', 'decent', 'face', 'ltgt', 'joker', 'face']\n",
            "After lowercasing: dip's cell dead. so i m coming with him. u better respond else we shall come back.\n",
            "After removing special chars: dips cell dead so i m coming with him u better respond else we shall come back\n",
            "After tokenization: ['dips', 'cell', 'dead', 'so', 'i', 'm', 'coming', 'with', 'him', 'u', 'better', 'respond', 'else', 'we', 'shall', 'come', 'back']\n",
            "After stop word removal: ['dips', 'cell', 'dead', 'coming', 'u', 'better', 'respond', 'else', 'shall', 'come', 'back']\n",
            "After lemmatization: ['dip', 'cell', 'dead', 'coming', 'u', 'better', 'respond', 'else', 'shall', 'come', 'back']\n",
            "After lowercasing: well. you know what i mean. texting\n",
            "After removing special chars: well you know what i mean texting\n",
            "After tokenization: ['well', 'you', 'know', 'what', 'i', 'mean', 'texting']\n",
            "After stop word removal: ['well', 'know', 'mean', 'texting']\n",
            "After lemmatization: ['well', 'know', 'mean', 'texting']\n",
            "After lowercasing: hi dis is yijue i would be happy to work wif ì_ all for gek1510...\n",
            "After removing special chars: hi dis is yijue i would be happy to work wif  all for gek\n",
            "After tokenization: ['hi', 'dis', 'is', 'yijue', 'i', 'would', 'be', 'happy', 'to', 'work', 'wif', 'all', 'for', 'gek']\n",
            "After stop word removal: ['hi', 'dis', 'yijue', 'would', 'happy', 'work', 'wif', 'gek']\n",
            "After lemmatization: ['hi', 'dis', 'yijue', 'would', 'happy', 'work', 'wif', 'gek']\n",
            "After lowercasing: lol! oops sorry! have fun. \n",
            "After removing special chars: lol oops sorry have fun \n",
            "After tokenization: ['lol', 'oops', 'sorry', 'have', 'fun']\n",
            "After stop word removal: ['lol', 'oops', 'sorry', 'fun']\n",
            "After lemmatization: ['lol', 'oops', 'sorry', 'fun']\n",
            "After lowercasing: wat happened to the cruise thing\n",
            "After removing special chars: wat happened to the cruise thing\n",
            "After tokenization: ['wat', 'happened', 'to', 'the', 'cruise', 'thing']\n",
            "After stop word removal: ['wat', 'happened', 'cruise', 'thing']\n",
            "After lemmatization: ['wat', 'happened', 'cruise', 'thing']\n",
            "After lowercasing: i know dat feelin had it with pete! wuld get with em , nuther place nuther time mayb?\n",
            "After removing special chars: i know dat feelin had it with pete wuld get with em  nuther place nuther time mayb\n",
            "After tokenization: ['i', 'know', 'dat', 'feelin', 'had', 'it', 'with', 'pete', 'wuld', 'get', 'with', 'em', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
            "After stop word removal: ['know', 'dat', 'feelin', 'pete', 'wuld', 'get', 'em', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
            "After lemmatization: ['know', 'dat', 'feelin', 'pete', 'wuld', 'get', 'em', 'nuther', 'place', 'nuther', 'time', 'mayb']\n",
            "After lowercasing: lyricalladie(21/f) is inviting you to be her friend. reply yes-910 or no-910. see her: www.sms.ac/u/hmmross stop? send stop frnd to 62468\n",
            "After removing special chars: lyricalladief is inviting you to be her friend reply yes or no see her wwwsmsacuhmmross stop send stop frnd to \n",
            "After tokenization: ['lyricalladief', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'wwwsmsacuhmmross', 'stop', 'send', 'stop', 'frnd', 'to']\n",
            "After stop word removal: ['lyricalladief', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacuhmmross', 'stop', 'send', 'stop', 'frnd']\n",
            "After lemmatization: ['lyricalladief', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacuhmmross', 'stop', 'send', 'stop', 'frnd']\n",
            "After lowercasing: the world's most happiest frnds never have the same characters... dey just have the best understanding of their differences...\n",
            "After removing special chars: the worlds most happiest frnds never have the same characters dey just have the best understanding of their differences\n",
            "After tokenization: ['the', 'worlds', 'most', 'happiest', 'frnds', 'never', 'have', 'the', 'same', 'characters', 'dey', 'just', 'have', 'the', 'best', 'understanding', 'of', 'their', 'differences']\n",
            "After stop word removal: ['worlds', 'happiest', 'frnds', 'never', 'characters', 'dey', 'best', 'understanding', 'differences']\n",
            "After lemmatization: ['world', 'happiest', 'frnds', 'never', 'character', 'dey', 'best', 'understanding', 'difference']\n",
            "After lowercasing: no 1 polyphonic tone 4 ur mob every week! just txt pt2 to 87575. 1st tone free ! so get txtin now and tell ur friends. 150p/tone. 16 reply hl 4info\n",
            "After removing special chars: no  polyphonic tone  ur mob every week just txt pt to  st tone free  so get txtin now and tell ur friends ptone  reply hl info\n",
            "After tokenization: ['no', 'polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'just', 'txt', 'pt', 'to', 'st', 'tone', 'free', 'so', 'get', 'txtin', 'now', 'and', 'tell', 'ur', 'friends', 'ptone', 'reply', 'hl', 'info']\n",
            "After stop word removal: ['polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'pt', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friends', 'ptone', 'reply', 'hl', 'info']\n",
            "After lemmatization: ['polyphonic', 'tone', 'ur', 'mob', 'every', 'week', 'txt', 'pt', 'st', 'tone', 'free', 'get', 'txtin', 'tell', 'ur', 'friend', 'ptone', 'reply', 'hl', 'info']\n",
            "After lowercasing: yeah just open chat and click friend lists. then make the list. easy as pie\n",
            "After removing special chars: yeah just open chat and click friend lists then make the list easy as pie\n",
            "After tokenization: ['yeah', 'just', 'open', 'chat', 'and', 'click', 'friend', 'lists', 'then', 'make', 'the', 'list', 'easy', 'as', 'pie']\n",
            "After stop word removal: ['yeah', 'open', 'chat', 'click', 'friend', 'lists', 'make', 'list', 'easy', 'pie']\n",
            "After lemmatization: ['yeah', 'open', 'chat', 'click', 'friend', 'list', 'make', 'list', 'easy', 'pie']\n",
            "After lowercasing: alright tyler's got a minor crisis and has to be home sooner than he thought so be here asap\n",
            "After removing special chars: alright tylers got a minor crisis and has to be home sooner than he thought so be here asap\n",
            "After tokenization: ['alright', 'tylers', 'got', 'a', 'minor', 'crisis', 'and', 'has', 'to', 'be', 'home', 'sooner', 'than', 'he', 'thought', 'so', 'be', 'here', 'asap']\n",
            "After stop word removal: ['alright', 'tylers', 'got', 'minor', 'crisis', 'home', 'sooner', 'thought', 'asap']\n",
            "After lemmatization: ['alright', 'tyler', 'got', 'minor', 'crisis', 'home', 'sooner', 'thought', 'asap']\n",
            "After lowercasing: when/where do i pick you up\n",
            "After removing special chars: whenwhere do i pick you up\n",
            "After tokenization: ['whenwhere', 'do', 'i', 'pick', 'you', 'up']\n",
            "After stop word removal: ['whenwhere', 'pick']\n",
            "After lemmatization: ['whenwhere', 'pick']\n",
            "After lowercasing: as usual u can call me ard 10 smth.\n",
            "After removing special chars: as usual u can call me ard  smth\n",
            "After tokenization: ['as', 'usual', 'u', 'can', 'call', 'me', 'ard', 'smth']\n",
            "After stop word removal: ['usual', 'u', 'call', 'ard', 'smth']\n",
            "After lemmatization: ['usual', 'u', 'call', 'ard', 'smth']\n",
            "After lowercasing: new theory: argument wins d situation, but loses the person. so dont argue with ur friends just.. . . . kick them &amp; say, i'm always correct.!\n",
            "After removing special chars: new theory argument wins d situation but loses the person so dont argue with ur friends just    kick them amp say im always correct\n",
            "After tokenization: ['new', 'theory', 'argument', 'wins', 'd', 'situation', 'but', 'loses', 'the', 'person', 'so', 'dont', 'argue', 'with', 'ur', 'friends', 'just', 'kick', 'them', 'amp', 'say', 'im', 'always', 'correct']\n",
            "After stop word removal: ['new', 'theory', 'argument', 'wins', 'situation', 'loses', 'person', 'dont', 'argue', 'ur', 'friends', 'kick', 'amp', 'say', 'im', 'always', 'correct']\n",
            "After lemmatization: ['new', 'theory', 'argument', 'win', 'situation', 'loses', 'person', 'dont', 'argue', 'ur', 'friend', 'kick', 'amp', 'say', 'im', 'always', 'correct']\n",
            "After lowercasing: for many things its an antibiotic and it can be used for chest abdomen and gynae infections even bone infections.\n",
            "After removing special chars: for many things its an antibiotic and it can be used for chest abdomen and gynae infections even bone infections\n",
            "After tokenization: ['for', 'many', 'things', 'its', 'an', 'antibiotic', 'and', 'it', 'can', 'be', 'used', 'for', 'chest', 'abdomen', 'and', 'gynae', 'infections', 'even', 'bone', 'infections']\n",
            "After stop word removal: ['many', 'things', 'antibiotic', 'used', 'chest', 'abdomen', 'gynae', 'infections', 'even', 'bone', 'infections']\n",
            "After lemmatization: ['many', 'thing', 'antibiotic', 'used', 'chest', 'abdomen', 'gynae', 'infection', 'even', 'bone', 'infection']\n",
            "After lowercasing: poor girl can't go one day lmao\n",
            "After removing special chars: poor girl cant go one day lmao\n",
            "After tokenization: ['poor', 'girl', 'cant', 'go', 'one', 'day', 'lmao']\n",
            "After stop word removal: ['poor', 'girl', 'cant', 'go', 'one', 'day', 'lmao']\n",
            "After lemmatization: ['poor', 'girl', 'cant', 'go', 'one', 'day', 'lmao']\n",
            "After lowercasing: or just do that 6times\n",
            "After removing special chars: or just do that times\n",
            "After tokenization: ['or', 'just', 'do', 'that', 'times']\n",
            "After stop word removal: ['times']\n",
            "After lemmatization: ['time']\n",
            "After lowercasing: todays vodafone numbers ending with 4882 are selected to a receive a å£350 award. if your number matches call 09064019014 to receive your å£350 award.\n",
            "After removing special chars: todays vodafone numbers ending with  are selected to a receive a  award if your number matches call  to receive your  award\n",
            "After tokenization: ['todays', 'vodafone', 'numbers', 'ending', 'with', 'are', 'selected', 'to', 'a', 'receive', 'a', 'award', 'if', 'your', 'number', 'matches', 'call', 'to', 'receive', 'your', 'award']\n",
            "After stop word removal: ['todays', 'vodafone', 'numbers', 'ending', 'selected', 'receive', 'award', 'number', 'matches', 'call', 'receive', 'award']\n",
            "After lemmatization: ['today', 'vodafone', 'number', 'ending', 'selected', 'receive', 'award', 'number', 'match', 'call', 'receive', 'award']\n",
            "After lowercasing: you have to pls make a note of all she.s exposed to. also find out from her school if anyone else was vomiting. is there a dog or cat in the house? let me know later.\n",
            "After removing special chars: you have to pls make a note of all shes exposed to also find out from her school if anyone else was vomiting is there a dog or cat in the house let me know later\n",
            "After tokenization: ['you', 'have', 'to', 'pls', 'make', 'a', 'note', 'of', 'all', 'shes', 'exposed', 'to', 'also', 'find', 'out', 'from', 'her', 'school', 'if', 'anyone', 'else', 'was', 'vomiting', 'is', 'there', 'a', 'dog', 'or', 'cat', 'in', 'the', 'house', 'let', 'me', 'know', 'later']\n",
            "After stop word removal: ['pls', 'make', 'note', 'shes', 'exposed', 'also', 'find', 'school', 'anyone', 'else', 'vomiting', 'dog', 'cat', 'house', 'let', 'know', 'later']\n",
            "After lemmatization: ['pls', 'make', 'note', 'shes', 'exposed', 'also', 'find', 'school', 'anyone', 'else', 'vomiting', 'dog', 'cat', 'house', 'let', 'know', 'later']\n",
            "After lowercasing: japanese proverb: if one can do it, u too can do it, if none can do it,u must do it indian version: if one can do it, let him do it.. if none can do it,leave it!! and finally kerala version: if one can do it, stop him doing it.. if none can do it, make a strike against it ...\n",
            "After removing special chars: japanese proverb if one can do it u too can do it if none can do itu must do it indian version if one can do it let him do it if none can do itleave it and finally kerala version if one can do it stop him doing it if none can do it make a strike against it \n",
            "After tokenization: ['japanese', 'proverb', 'if', 'one', 'can', 'do', 'it', 'u', 'too', 'can', 'do', 'it', 'if', 'none', 'can', 'do', 'itu', 'must', 'do', 'it', 'indian', 'version', 'if', 'one', 'can', 'do', 'it', 'let', 'him', 'do', 'it', 'if', 'none', 'can', 'do', 'itleave', 'it', 'and', 'finally', 'kerala', 'version', 'if', 'one', 'can', 'do', 'it', 'stop', 'him', 'doing', 'it', 'if', 'none', 'can', 'do', 'it', 'make', 'a', 'strike', 'against', 'it']\n",
            "After stop word removal: ['japanese', 'proverb', 'one', 'u', 'none', 'itu', 'must', 'indian', 'version', 'one', 'let', 'none', 'itleave', 'finally', 'kerala', 'version', 'one', 'stop', 'none', 'make', 'strike']\n",
            "After lemmatization: ['japanese', 'proverb', 'one', 'u', 'none', 'itu', 'must', 'indian', 'version', 'one', 'let', 'none', 'itleave', 'finally', 'kerala', 'version', 'one', 'stop', 'none', 'make', 'strike']\n",
            "After lowercasing: sounds like there could be a lot of time spent in that chastity device boy ... *grins* ... or take your beatings like a good dog. going to lounge in a nice long bath now ?\n",
            "After removing special chars: sounds like there could be a lot of time spent in that chastity device boy  grins  or take your beatings like a good dog going to lounge in a nice long bath now \n",
            "After tokenization: ['sounds', 'like', 'there', 'could', 'be', 'a', 'lot', 'of', 'time', 'spent', 'in', 'that', 'chastity', 'device', 'boy', 'grins', 'or', 'take', 'your', 'beatings', 'like', 'a', 'good', 'dog', 'going', 'to', 'lounge', 'in', 'a', 'nice', 'long', 'bath', 'now']\n",
            "After stop word removal: ['sounds', 'like', 'could', 'lot', 'time', 'spent', 'chastity', 'device', 'boy', 'grins', 'take', 'beatings', 'like', 'good', 'dog', 'going', 'lounge', 'nice', 'long', 'bath']\n",
            "After lemmatization: ['sound', 'like', 'could', 'lot', 'time', 'spent', 'chastity', 'device', 'boy', 'grin', 'take', 'beating', 'like', 'good', 'dog', 'going', 'lounge', 'nice', 'long', 'bath']\n",
            "After lowercasing: its worse if if uses half way then stops. its better for him to complete it.\n",
            "After removing special chars: its worse if if uses half way then stops its better for him to complete it\n",
            "After tokenization: ['its', 'worse', 'if', 'if', 'uses', 'half', 'way', 'then', 'stops', 'its', 'better', 'for', 'him', 'to', 'complete', 'it']\n",
            "After stop word removal: ['worse', 'uses', 'half', 'way', 'stops', 'better', 'complete']\n",
            "After lemmatization: ['worse', 'us', 'half', 'way', 'stop', 'better', 'complete']\n",
            "After lowercasing: miserable. they don't tell u that the side effects of birth control are massive gut wrenching cramps for the first 2 months. i didn't sleep at all last night.\n",
            "After removing special chars: miserable they dont tell u that the side effects of birth control are massive gut wrenching cramps for the first  months i didnt sleep at all last night\n",
            "After tokenization: ['miserable', 'they', 'dont', 'tell', 'u', 'that', 'the', 'side', 'effects', 'of', 'birth', 'control', 'are', 'massive', 'gut', 'wrenching', 'cramps', 'for', 'the', 'first', 'months', 'i', 'didnt', 'sleep', 'at', 'all', 'last', 'night']\n",
            "After stop word removal: ['miserable', 'dont', 'tell', 'u', 'side', 'effects', 'birth', 'control', 'massive', 'gut', 'wrenching', 'cramps', 'first', 'months', 'didnt', 'sleep', 'last', 'night']\n",
            "After lemmatization: ['miserable', 'dont', 'tell', 'u', 'side', 'effect', 'birth', 'control', 'massive', 'gut', 'wrenching', 'cramp', 'first', 'month', 'didnt', 'sleep', 'last', 'night']\n",
            "After lowercasing: send me the new number\n",
            "After removing special chars: send me the new number\n",
            "After tokenization: ['send', 'me', 'the', 'new', 'number']\n",
            "After stop word removal: ['send', 'new', 'number']\n",
            "After lemmatization: ['send', 'new', 'number']\n",
            "After lowercasing: convey my regards to him\n",
            "After removing special chars: convey my regards to him\n",
            "After tokenization: ['convey', 'my', 'regards', 'to', 'him']\n",
            "After stop word removal: ['convey', 'regards']\n",
            "After lemmatization: ['convey', 'regard']\n",
            "After lowercasing: want the latest video handset? 750 anytime any network mins? half price line rental? reply or call 08000930705 for delivery tomorrow\n",
            "After removing special chars: want the latest video handset  anytime any network mins half price line rental reply or call  for delivery tomorrow\n",
            "After tokenization: ['want', 'the', 'latest', 'video', 'handset', 'anytime', 'any', 'network', 'mins', 'half', 'price', 'line', 'rental', 'reply', 'or', 'call', 'for', 'delivery', 'tomorrow']\n",
            "After stop word removal: ['want', 'latest', 'video', 'handset', 'anytime', 'network', 'mins', 'half', 'price', 'line', 'rental', 'reply', 'call', 'delivery', 'tomorrow']\n",
            "After lemmatization: ['want', 'latest', 'video', 'handset', 'anytime', 'network', 'min', 'half', 'price', 'line', 'rental', 'reply', 'call', 'delivery', 'tomorrow']\n",
            "After lowercasing: 2 and half years i missed your friendship:-)\n",
            "After removing special chars:  and half years i missed your friendship\n",
            "After tokenization: ['and', 'half', 'years', 'i', 'missed', 'your', 'friendship']\n",
            "After stop word removal: ['half', 'years', 'missed', 'friendship']\n",
            "After lemmatization: ['half', 'year', 'missed', 'friendship']\n",
            "After lowercasing: i cant pick the phone right now. pls send a message\n",
            "After removing special chars: i cant pick the phone right now pls send a message\n",
            "After tokenization: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
            "After stop word removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lemmatization: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lowercasing: oh for fuck's sake she's in like tallahassee\n",
            "After removing special chars: oh for fucks sake shes in like tallahassee\n",
            "After tokenization: ['oh', 'for', 'fucks', 'sake', 'shes', 'in', 'like', 'tallahassee']\n",
            "After stop word removal: ['oh', 'fucks', 'sake', 'shes', 'like', 'tallahassee']\n",
            "After lemmatization: ['oh', 'fuck', 'sake', 'shes', 'like', 'tallahassee']\n",
            "After lowercasing: haha, that was the first person i was gonna ask\n",
            "After removing special chars: haha that was the first person i was gonna ask\n",
            "After tokenization: ['haha', 'that', 'was', 'the', 'first', 'person', 'i', 'was', 'gon', 'na', 'ask']\n",
            "After stop word removal: ['haha', 'first', 'person', 'gon', 'na', 'ask']\n",
            "After lemmatization: ['haha', 'first', 'person', 'gon', 'na', 'ask']\n",
            "After lowercasing: ou are guaranteed the latest nokia phone, a 40gb ipod mp3 player or a å£500 prize! txt word: collect to no: 83355! ibhltd ldnw15h 150p/mtmsgrcvd18\n",
            "After removing special chars: ou are guaranteed the latest nokia phone a gb ipod mp player or a  prize txt word collect to no  ibhltd ldnwh pmtmsgrcvd\n",
            "After tokenization: ['ou', 'are', 'guaranteed', 'the', 'latest', 'nokia', 'phone', 'a', 'gb', 'ipod', 'mp', 'player', 'or', 'a', 'prize', 'txt', 'word', 'collect', 'to', 'no', 'ibhltd', 'ldnwh', 'pmtmsgrcvd']\n",
            "After stop word removal: ['ou', 'guaranteed', 'latest', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'ibhltd', 'ldnwh', 'pmtmsgrcvd']\n",
            "After lemmatization: ['ou', 'guaranteed', 'latest', 'nokia', 'phone', 'gb', 'ipod', 'mp', 'player', 'prize', 'txt', 'word', 'collect', 'ibhltd', 'ldnwh', 'pmtmsgrcvd']\n",
            "After lowercasing: taka lor. wat time u wan 2 come n look 4 us?\n",
            "After removing special chars: taka lor wat time u wan  come n look  us\n",
            "After tokenization: ['taka', 'lor', 'wat', 'time', 'u', 'wan', 'come', 'n', 'look', 'us']\n",
            "After stop word removal: ['taka', 'lor', 'wat', 'time', 'u', 'wan', 'come', 'n', 'look', 'us']\n",
            "After lemmatization: ['taka', 'lor', 'wat', 'time', 'u', 'wan', 'come', 'n', 'look', 'u']\n",
            "After lowercasing: * free* polyphonic ringtone text super to 87131 to get your free poly tone of the week now! 16 sn pobox202 nr31 7zs subscription 450pw\n",
            "After removing special chars:  free polyphonic ringtone text super to  to get your free poly tone of the week now  sn pobox nr zs subscription pw\n",
            "After tokenization: ['free', 'polyphonic', 'ringtone', 'text', 'super', 'to', 'to', 'get', 'your', 'free', 'poly', 'tone', 'of', 'the', 'week', 'now', 'sn', 'pobox', 'nr', 'zs', 'subscription', 'pw']\n",
            "After stop word removal: ['free', 'polyphonic', 'ringtone', 'text', 'super', 'get', 'free', 'poly', 'tone', 'week', 'sn', 'pobox', 'nr', 'zs', 'subscription', 'pw']\n",
            "After lemmatization: ['free', 'polyphonic', 'ringtone', 'text', 'super', 'get', 'free', 'poly', 'tone', 'week', 'sn', 'pobox', 'nr', 'z', 'subscription', 'pw']\n",
            "After lowercasing: \\i;m reaching in another 2 stops.\\\"\"\n",
            "After removing special chars: im reaching in another  stops\n",
            "After tokenization: ['im', 'reaching', 'in', 'another', 'stops']\n",
            "After stop word removal: ['im', 'reaching', 'another', 'stops']\n",
            "After lemmatization: ['im', 'reaching', 'another', 'stop']\n",
            "After lowercasing: no, i *didn't* mean to post it. i wrote it, and like so many other times i've ritten stuff to you, i let it sit there. it was what i was feeling at the time. i was angry. before i left, i hit send, then stop. it wasn't there. i checked on my phone when i got to my car. it wasn't there. you said you didn't sleep, you were bored. so why wouldn't that be the time to clean, fold laundry, etc.? at least make the bed?\n",
            "After removing special chars: no i didnt mean to post it i wrote it and like so many other times ive ritten stuff to you i let it sit there it was what i was feeling at the time i was angry before i left i hit send then stop it wasnt there i checked on my phone when i got to my car it wasnt there you said you didnt sleep you were bored so why wouldnt that be the time to clean fold laundry etc at least make the bed\n",
            "After tokenization: ['no', 'i', 'didnt', 'mean', 'to', 'post', 'it', 'i', 'wrote', 'it', 'and', 'like', 'so', 'many', 'other', 'times', 'ive', 'ritten', 'stuff', 'to', 'you', 'i', 'let', 'it', 'sit', 'there', 'it', 'was', 'what', 'i', 'was', 'feeling', 'at', 'the', 'time', 'i', 'was', 'angry', 'before', 'i', 'left', 'i', 'hit', 'send', 'then', 'stop', 'it', 'wasnt', 'there', 'i', 'checked', 'on', 'my', 'phone', 'when', 'i', 'got', 'to', 'my', 'car', 'it', 'wasnt', 'there', 'you', 'said', 'you', 'didnt', 'sleep', 'you', 'were', 'bored', 'so', 'why', 'wouldnt', 'that', 'be', 'the', 'time', 'to', 'clean', 'fold', 'laundry', 'etc', 'at', 'least', 'make', 'the', 'bed']\n",
            "After stop word removal: ['didnt', 'mean', 'post', 'wrote', 'like', 'many', 'times', 'ive', 'ritten', 'stuff', 'let', 'sit', 'feeling', 'time', 'angry', 'left', 'hit', 'send', 'stop', 'wasnt', 'checked', 'phone', 'got', 'car', 'wasnt', 'said', 'didnt', 'sleep', 'bored', 'wouldnt', 'time', 'clean', 'fold', 'laundry', 'etc', 'least', 'make', 'bed']\n",
            "After lemmatization: ['didnt', 'mean', 'post', 'wrote', 'like', 'many', 'time', 'ive', 'ritten', 'stuff', 'let', 'sit', 'feeling', 'time', 'angry', 'left', 'hit', 'send', 'stop', 'wasnt', 'checked', 'phone', 'got', 'car', 'wasnt', 'said', 'didnt', 'sleep', 'bored', 'wouldnt', 'time', 'clean', 'fold', 'laundry', 'etc', 'least', 'make', 'bed']\n",
            "After lowercasing: warner village 83118 c colin farrell in swat this wkend @warner village & get 1 free med. popcorn!just show msg+ticket@kiosk.valid 4-7/12. c t&c @kiosk. reply sony 4 mre film offers\n",
            "After removing special chars: warner village  c colin farrell in swat this wkend warner village  get  free med popcornjust show msgticketkioskvalid  c tc kiosk reply sony  mre film offers\n",
            "After tokenization: ['warner', 'village', 'c', 'colin', 'farrell', 'in', 'swat', 'this', 'wkend', 'warner', 'village', 'get', 'free', 'med', 'popcornjust', 'show', 'msgticketkioskvalid', 'c', 'tc', 'kiosk', 'reply', 'sony', 'mre', 'film', 'offers']\n",
            "After stop word removal: ['warner', 'village', 'c', 'colin', 'farrell', 'swat', 'wkend', 'warner', 'village', 'get', 'free', 'med', 'popcornjust', 'show', 'msgticketkioskvalid', 'c', 'tc', 'kiosk', 'reply', 'sony', 'mre', 'film', 'offers']\n",
            "After lemmatization: ['warner', 'village', 'c', 'colin', 'farrell', 'swat', 'wkend', 'warner', 'village', 'get', 'free', 'med', 'popcornjust', 'show', 'msgticketkioskvalid', 'c', 'tc', 'kiosk', 'reply', 'sony', 'mre', 'film', 'offer']\n",
            "After lowercasing: will you come online today night\n",
            "After removing special chars: will you come online today night\n",
            "After tokenization: ['will', 'you', 'come', 'online', 'today', 'night']\n",
            "After stop word removal: ['come', 'online', 'today', 'night']\n",
            "After lemmatization: ['come', 'online', 'today', 'night']\n",
            "After lowercasing: then anything special?\n",
            "After removing special chars: then anything special\n",
            "After tokenization: ['then', 'anything', 'special']\n",
            "After stop word removal: ['anything', 'special']\n",
            "After lemmatization: ['anything', 'special']\n",
            "After lowercasing: i'm in solihull, | do you want anything?\n",
            "After removing special chars: im in solihull  do you want anything\n",
            "After tokenization: ['im', 'in', 'solihull', 'do', 'you', 'want', 'anything']\n",
            "After stop word removal: ['im', 'solihull', 'want', 'anything']\n",
            "After lemmatization: ['im', 'solihull', 'want', 'anything']\n",
            "After lowercasing: will do. have a good day\n",
            "After removing special chars: will do have a good day\n",
            "After tokenization: ['will', 'do', 'have', 'a', 'good', 'day']\n",
            "After stop word removal: ['good', 'day']\n",
            "After lemmatization: ['good', 'day']\n",
            "After lowercasing: we regret to inform u that the nhs has made a mistake.u were never actually born.please report 2 yor local hospital 2b terminated.we r sorry 4 the inconvenience\n",
            "After removing special chars: we regret to inform u that the nhs has made a mistakeu were never actually bornplease report  yor local hospital b terminatedwe r sorry  the inconvenience\n",
            "After tokenization: ['we', 'regret', 'to', 'inform', 'u', 'that', 'the', 'nhs', 'has', 'made', 'a', 'mistakeu', 'were', 'never', 'actually', 'bornplease', 'report', 'yor', 'local', 'hospital', 'b', 'terminatedwe', 'r', 'sorry', 'the', 'inconvenience']\n",
            "After stop word removal: ['regret', 'inform', 'u', 'nhs', 'made', 'mistakeu', 'never', 'actually', 'bornplease', 'report', 'yor', 'local', 'hospital', 'b', 'terminatedwe', 'r', 'sorry', 'inconvenience']\n",
            "After lemmatization: ['regret', 'inform', 'u', 'nh', 'made', 'mistakeu', 'never', 'actually', 'bornplease', 'report', 'yor', 'local', 'hospital', 'b', 'terminatedwe', 'r', 'sorry', 'inconvenience']\n",
            "After lowercasing: love that holiday monday feeling even if i have to go to the dentists in an hour\n",
            "After removing special chars: love that holiday monday feeling even if i have to go to the dentists in an hour\n",
            "After tokenization: ['love', 'that', 'holiday', 'monday', 'feeling', 'even', 'if', 'i', 'have', 'to', 'go', 'to', 'the', 'dentists', 'in', 'an', 'hour']\n",
            "After stop word removal: ['love', 'holiday', 'monday', 'feeling', 'even', 'go', 'dentists', 'hour']\n",
            "After lemmatization: ['love', 'holiday', 'monday', 'feeling', 'even', 'go', 'dentist', 'hour']\n",
            "After lowercasing: i am on the way to tirupur.\n",
            "After removing special chars: i am on the way to tirupur\n",
            "After tokenization: ['i', 'am', 'on', 'the', 'way', 'to', 'tirupur']\n",
            "After stop word removal: ['way', 'tirupur']\n",
            "After lemmatization: ['way', 'tirupur']\n",
            "After lowercasing: goal! arsenal 4 (henry, 7 v liverpool 2 henry scores with a simple shot from 6 yards from a pass by bergkamp to give arsenal a 2 goal margin after 78 mins.\n",
            "After removing special chars: goal arsenal  henry  v liverpool  henry scores with a simple shot from  yards from a pass by bergkamp to give arsenal a  goal margin after  mins\n",
            "After tokenization: ['goal', 'arsenal', 'henry', 'v', 'liverpool', 'henry', 'scores', 'with', 'a', 'simple', 'shot', 'from', 'yards', 'from', 'a', 'pass', 'by', 'bergkamp', 'to', 'give', 'arsenal', 'a', 'goal', 'margin', 'after', 'mins']\n",
            "After stop word removal: ['goal', 'arsenal', 'henry', 'v', 'liverpool', 'henry', 'scores', 'simple', 'shot', 'yards', 'pass', 'bergkamp', 'give', 'arsenal', 'goal', 'margin', 'mins']\n",
            "After lemmatization: ['goal', 'arsenal', 'henry', 'v', 'liverpool', 'henry', 'score', 'simple', 'shot', 'yard', 'pas', 'bergkamp', 'give', 'arsenal', 'goal', 'margin', 'min']\n",
            "After lowercasing: you've already got a flaky parent. it'snot supposed to be the child's job to support the parent...not until they're the ride age anyway. i'm supposed to be there to support you. and now i've hurt you. unintentional. but hurt nonetheless.\n",
            "After removing special chars: youve already got a flaky parent itsnot supposed to be the childs job to support the parentnot until theyre the ride age anyway im supposed to be there to support you and now ive hurt you unintentional but hurt nonetheless\n",
            "After tokenization: ['youve', 'already', 'got', 'a', 'flaky', 'parent', 'itsnot', 'supposed', 'to', 'be', 'the', 'childs', 'job', 'to', 'support', 'the', 'parentnot', 'until', 'theyre', 'the', 'ride', 'age', 'anyway', 'im', 'supposed', 'to', 'be', 'there', 'to', 'support', 'you', 'and', 'now', 'ive', 'hurt', 'you', 'unintentional', 'but', 'hurt', 'nonetheless']\n",
            "After stop word removal: ['youve', 'already', 'got', 'flaky', 'parent', 'itsnot', 'supposed', 'childs', 'job', 'support', 'parentnot', 'theyre', 'ride', 'age', 'anyway', 'im', 'supposed', 'support', 'ive', 'hurt', 'unintentional', 'hurt', 'nonetheless']\n",
            "After lemmatization: ['youve', 'already', 'got', 'flaky', 'parent', 'itsnot', 'supposed', 'child', 'job', 'support', 'parentnot', 'theyre', 'ride', 'age', 'anyway', 'im', 'supposed', 'support', 'ive', 'hurt', 'unintentional', 'hurt', 'nonetheless']\n",
            "After lowercasing: we took hooch for a walk toaday and i fell over! splat! grazed my knees and everything! should have stayed at home! see you tomorrow! \n",
            "After removing special chars: we took hooch for a walk toaday and i fell over splat grazed my knees and everything should have stayed at home see you tomorrow \n",
            "After tokenization: ['we', 'took', 'hooch', 'for', 'a', 'walk', 'toaday', 'and', 'i', 'fell', 'over', 'splat', 'grazed', 'my', 'knees', 'and', 'everything', 'should', 'have', 'stayed', 'at', 'home', 'see', 'you', 'tomorrow']\n",
            "After stop word removal: ['took', 'hooch', 'walk', 'toaday', 'fell', 'splat', 'grazed', 'knees', 'everything', 'stayed', 'home', 'see', 'tomorrow']\n",
            "After lemmatization: ['took', 'hooch', 'walk', 'toaday', 'fell', 'splat', 'grazed', 'knee', 'everything', 'stayed', 'home', 'see', 'tomorrow']\n",
            "After lowercasing: just dropped em off, omw back now\n",
            "After removing special chars: just dropped em off omw back now\n",
            "After tokenization: ['just', 'dropped', 'em', 'off', 'omw', 'back', 'now']\n",
            "After stop word removal: ['dropped', 'em', 'omw', 'back']\n",
            "After lemmatization: ['dropped', 'em', 'omw', 'back']\n",
            "After lowercasing: this is the 2nd time we have tried 2 contact u. u have won the 750 pound prize. 2 claim is easy, call 08712101358 now! only 10p per min. bt-national-rate\n",
            "After removing special chars: this is the nd time we have tried  contact u u have won the  pound prize  claim is easy call  now only p per min btnationalrate\n",
            "After tokenization: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'min', 'btnationalrate']\n",
            "After stop word removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'min', 'btnationalrate']\n",
            "After lemmatization: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'min', 'btnationalrate']\n",
            "After lowercasing: sitting in mu waiting for everyone to get out of my suite so i can take a shower\n",
            "After removing special chars: sitting in mu waiting for everyone to get out of my suite so i can take a shower\n",
            "After tokenization: ['sitting', 'in', 'mu', 'waiting', 'for', 'everyone', 'to', 'get', 'out', 'of', 'my', 'suite', 'so', 'i', 'can', 'take', 'a', 'shower']\n",
            "After stop word removal: ['sitting', 'mu', 'waiting', 'everyone', 'get', 'suite', 'take', 'shower']\n",
            "After lemmatization: ['sitting', 'mu', 'waiting', 'everyone', 'get', 'suite', 'take', 'shower']\n",
            "After lowercasing: re your call; you didn't see my facebook huh?\n",
            "After removing special chars: re your call you didnt see my facebook huh\n",
            "After tokenization: ['re', 'your', 'call', 'you', 'didnt', 'see', 'my', 'facebook', 'huh']\n",
            "After stop word removal: ['call', 'didnt', 'see', 'facebook', 'huh']\n",
            "After lemmatization: ['call', 'didnt', 'see', 'facebook', 'huh']\n",
            "After lowercasing: g says you never answer your texts, confirm/deny\n",
            "After removing special chars: g says you never answer your texts confirmdeny\n",
            "After tokenization: ['g', 'says', 'you', 'never', 'answer', 'your', 'texts', 'confirmdeny']\n",
            "After stop word removal: ['g', 'says', 'never', 'answer', 'texts', 'confirmdeny']\n",
            "After lemmatization: ['g', 'say', 'never', 'answer', 'text', 'confirmdeny']\n",
            "After lowercasing: its so common hearin how r u? wat r u doing? how was ur day? so let me ask u something different. did u smile today? if not, do it now.... gud evng.\n",
            "After removing special chars: its so common hearin how r u wat r u doing how was ur day so let me ask u something different did u smile today if not do it now gud evng\n",
            "After tokenization: ['its', 'so', 'common', 'hearin', 'how', 'r', 'u', 'wat', 'r', 'u', 'doing', 'how', 'was', 'ur', 'day', 'so', 'let', 'me', 'ask', 'u', 'something', 'different', 'did', 'u', 'smile', 'today', 'if', 'not', 'do', 'it', 'now', 'gud', 'evng']\n",
            "After stop word removal: ['common', 'hearin', 'r', 'u', 'wat', 'r', 'u', 'ur', 'day', 'let', 'ask', 'u', 'something', 'different', 'u', 'smile', 'today', 'gud', 'evng']\n",
            "After lemmatization: ['common', 'hearin', 'r', 'u', 'wat', 'r', 'u', 'ur', 'day', 'let', 'ask', 'u', 'something', 'different', 'u', 'smile', 'today', 'gud', 'evng']\n",
            "After lowercasing: hi dear call me its urgnt. i don't know whats your problem. you don't want to work or if you have any other problem at least tell me. wating for your reply.\n",
            "After removing special chars: hi dear call me its urgnt i dont know whats your problem you dont want to work or if you have any other problem at least tell me wating for your reply\n",
            "After tokenization: ['hi', 'dear', 'call', 'me', 'its', 'urgnt', 'i', 'dont', 'know', 'whats', 'your', 'problem', 'you', 'dont', 'want', 'to', 'work', 'or', 'if', 'you', 'have', 'any', 'other', 'problem', 'at', 'least', 'tell', 'me', 'wating', 'for', 'your', 'reply']\n",
            "After stop word removal: ['hi', 'dear', 'call', 'urgnt', 'dont', 'know', 'whats', 'problem', 'dont', 'want', 'work', 'problem', 'least', 'tell', 'wating', 'reply']\n",
            "After lemmatization: ['hi', 'dear', 'call', 'urgnt', 'dont', 'know', 'whats', 'problem', 'dont', 'want', 'work', 'problem', 'least', 'tell', 'wating', 'reply']\n",
            "After lowercasing: oh yah... we never cancel leh... haha \n",
            "After removing special chars: oh yah we never cancel leh haha \n",
            "After tokenization: ['oh', 'yah', 'we', 'never', 'cancel', 'leh', 'haha']\n",
            "After stop word removal: ['oh', 'yah', 'never', 'cancel', 'leh', 'haha']\n",
            "After lemmatization: ['oh', 'yah', 'never', 'cancel', 'leh', 'haha']\n",
            "After lowercasing: we can go 4 e normal pilates after our intro...  \n",
            "After removing special chars: we can go  e normal pilates after our intro  \n",
            "After tokenization: ['we', 'can', 'go', 'e', 'normal', 'pilates', 'after', 'our', 'intro']\n",
            "After stop word removal: ['go', 'e', 'normal', 'pilates', 'intro']\n",
            "After lemmatization: ['go', 'e', 'normal', 'pilate', 'intro']\n",
            "After lowercasing: ok... let u noe when i leave my house.\n",
            "After removing special chars: ok let u noe when i leave my house\n",
            "After tokenization: ['ok', 'let', 'u', 'noe', 'when', 'i', 'leave', 'my', 'house']\n",
            "After stop word removal: ['ok', 'let', 'u', 'noe', 'leave', 'house']\n",
            "After lemmatization: ['ok', 'let', 'u', 'noe', 'leave', 'house']\n",
            "After lowercasing: oh yes, why is it like torture watching england?\n",
            "After removing special chars: oh yes why is it like torture watching england\n",
            "After tokenization: ['oh', 'yes', 'why', 'is', 'it', 'like', 'torture', 'watching', 'england']\n",
            "After stop word removal: ['oh', 'yes', 'like', 'torture', 'watching', 'england']\n",
            "After lemmatization: ['oh', 'yes', 'like', 'torture', 'watching', 'england']\n",
            "After lowercasing: wanna do some art?! :d\n",
            "After removing special chars: wanna do some art d\n",
            "After tokenization: ['wan', 'na', 'do', 'some', 'art', 'd']\n",
            "After stop word removal: ['wan', 'na', 'art']\n",
            "After lemmatization: ['wan', 'na', 'art']\n",
            "After lowercasing: just hopeing that wasnû÷t too pissed up to remember and has gone off to his sisters or something!\n",
            "After removing special chars: just hopeing that wasnt too pissed up to remember and has gone off to his sisters or something\n",
            "After tokenization: ['just', 'hopeing', 'that', 'wasnt', 'too', 'pissed', 'up', 'to', 'remember', 'and', 'has', 'gone', 'off', 'to', 'his', 'sisters', 'or', 'something']\n",
            "After stop word removal: ['hopeing', 'wasnt', 'pissed', 'remember', 'gone', 'sisters', 'something']\n",
            "After lemmatization: ['hopeing', 'wasnt', 'pissed', 'remember', 'gone', 'sister', 'something']\n",
            "After lowercasing: got what it takes 2 take part in the wrc rally in oz? u can with lucozade energy! text rally le to 61200 (25p), see packs or lucozade.co.uk/wrc & itcould be u!\n",
            "After removing special chars: got what it takes  take part in the wrc rally in oz u can with lucozade energy text rally le to  p see packs or lucozadecoukwrc  itcould be u\n",
            "After tokenization: ['got', 'what', 'it', 'takes', 'take', 'part', 'in', 'the', 'wrc', 'rally', 'in', 'oz', 'u', 'can', 'with', 'lucozade', 'energy', 'text', 'rally', 'le', 'to', 'p', 'see', 'packs', 'or', 'lucozadecoukwrc', 'itcould', 'be', 'u']\n",
            "After stop word removal: ['got', 'takes', 'take', 'part', 'wrc', 'rally', 'oz', 'u', 'lucozade', 'energy', 'text', 'rally', 'le', 'p', 'see', 'packs', 'lucozadecoukwrc', 'itcould', 'u']\n",
            "After lemmatization: ['got', 'take', 'take', 'part', 'wrc', 'rally', 'oz', 'u', 'lucozade', 'energy', 'text', 'rally', 'le', 'p', 'see', 'pack', 'lucozadecoukwrc', 'itcould', 'u']\n",
            "After lowercasing: hi, the sexychat girls are waiting for you to text them. text now for a great night chatting. send stop to stop this service\n",
            "After removing special chars: hi the sexychat girls are waiting for you to text them text now for a great night chatting send stop to stop this service\n",
            "After tokenization: ['hi', 'the', 'sexychat', 'girls', 'are', 'waiting', 'for', 'you', 'to', 'text', 'them', 'text', 'now', 'for', 'a', 'great', 'night', 'chatting', 'send', 'stop', 'to', 'stop', 'this', 'service']\n",
            "After stop word removal: ['hi', 'sexychat', 'girls', 'waiting', 'text', 'text', 'great', 'night', 'chatting', 'send', 'stop', 'stop', 'service']\n",
            "After lemmatization: ['hi', 'sexychat', 'girl', 'waiting', 'text', 'text', 'great', 'night', 'chatting', 'send', 'stop', 'stop', 'service']\n",
            "After lowercasing: good morning, my boytoy! how's those yummy lips ? where's my sexy buns now ? what do you do ? do you think of me ? do you crave me ? do you need me ?\n",
            "After removing special chars: good morning my boytoy hows those yummy lips  wheres my sexy buns now  what do you do  do you think of me  do you crave me  do you need me \n",
            "After tokenization: ['good', 'morning', 'my', 'boytoy', 'hows', 'those', 'yummy', 'lips', 'wheres', 'my', 'sexy', 'buns', 'now', 'what', 'do', 'you', 'do', 'do', 'you', 'think', 'of', 'me', 'do', 'you', 'crave', 'me', 'do', 'you', 'need', 'me']\n",
            "After stop word removal: ['good', 'morning', 'boytoy', 'hows', 'yummy', 'lips', 'wheres', 'sexy', 'buns', 'think', 'crave', 'need']\n",
            "After lemmatization: ['good', 'morning', 'boytoy', 'hows', 'yummy', 'lip', 'wheres', 'sexy', 'bun', 'think', 'crave', 'need']\n",
            "After lowercasing: match started.india  &lt;#&gt;  for 2\n",
            "After removing special chars: match startedindia  ltgt  for \n",
            "After tokenization: ['match', 'startedindia', 'ltgt', 'for']\n",
            "After stop word removal: ['match', 'startedindia', 'ltgt']\n",
            "After lemmatization: ['match', 'startedindia', 'ltgt']\n",
            "After lowercasing: once free call me sir.\n",
            "After removing special chars: once free call me sir\n",
            "After tokenization: ['once', 'free', 'call', 'me', 'sir']\n",
            "After stop word removal: ['free', 'call', 'sir']\n",
            "After lemmatization: ['free', 'call', 'sir']\n",
            "After lowercasing: hey do you want anything to buy:)\n",
            "After removing special chars: hey do you want anything to buy\n",
            "After tokenization: ['hey', 'do', 'you', 'want', 'anything', 'to', 'buy']\n",
            "After stop word removal: ['hey', 'want', 'anything', 'buy']\n",
            "After lemmatization: ['hey', 'want', 'anything', 'buy']\n",
            "After lowercasing: hey babe, how's it going ? did you ever figure out where your going for new years ?\n",
            "After removing special chars: hey babe hows it going  did you ever figure out where your going for new years \n",
            "After tokenization: ['hey', 'babe', 'hows', 'it', 'going', 'did', 'you', 'ever', 'figure', 'out', 'where', 'your', 'going', 'for', 'new', 'years']\n",
            "After stop word removal: ['hey', 'babe', 'hows', 'going', 'ever', 'figure', 'going', 'new', 'years']\n",
            "After lemmatization: ['hey', 'babe', 'hows', 'going', 'ever', 'figure', 'going', 'new', 'year']\n",
            "After lowercasing: k..k.:)congratulation ..\n",
            "After removing special chars: kkcongratulation \n",
            "After tokenization: ['kkcongratulation']\n",
            "After stop word removal: ['kkcongratulation']\n",
            "After lemmatization: ['kkcongratulation']\n",
            "After lowercasing: g wants to know where the fuck you are\n",
            "After removing special chars: g wants to know where the fuck you are\n",
            "After tokenization: ['g', 'wants', 'to', 'know', 'where', 'the', 'fuck', 'you', 'are']\n",
            "After stop word removal: ['g', 'wants', 'know', 'fuck']\n",
            "After lemmatization: ['g', 'want', 'know', 'fuck']\n",
            "After lowercasing: no it was cancelled yeah baby! well that sounds important so i understand my darlin give me a ring later on this fone love kate x\n",
            "After removing special chars: no it was cancelled yeah baby well that sounds important so i understand my darlin give me a ring later on this fone love kate x\n",
            "After tokenization: ['no', 'it', 'was', 'cancelled', 'yeah', 'baby', 'well', 'that', 'sounds', 'important', 'so', 'i', 'understand', 'my', 'darlin', 'give', 'me', 'a', 'ring', 'later', 'on', 'this', 'fone', 'love', 'kate', 'x']\n",
            "After stop word removal: ['cancelled', 'yeah', 'baby', 'well', 'sounds', 'important', 'understand', 'darlin', 'give', 'ring', 'later', 'fone', 'love', 'kate', 'x']\n",
            "After lemmatization: ['cancelled', 'yeah', 'baby', 'well', 'sound', 'important', 'understand', 'darlin', 'give', 'ring', 'later', 'fone', 'love', 'kate', 'x']\n",
            "After lowercasing: tomarrow i want to got to court. at  &lt;decimal&gt; . so you come to bus stand at 9.\n",
            "After removing special chars: tomarrow i want to got to court at  ltdecimalgt  so you come to bus stand at \n",
            "After tokenization: ['tomarrow', 'i', 'want', 'to', 'got', 'to', 'court', 'at', 'ltdecimalgt', 'so', 'you', 'come', 'to', 'bus', 'stand', 'at']\n",
            "After stop word removal: ['tomarrow', 'want', 'got', 'court', 'ltdecimalgt', 'come', 'bus', 'stand']\n",
            "After lemmatization: ['tomarrow', 'want', 'got', 'court', 'ltdecimalgt', 'come', 'bus', 'stand']\n",
            "After lowercasing: ìï go home liao? ask dad to pick me up at 6...\n",
            "After removing special chars:  go home liao ask dad to pick me up at \n",
            "After tokenization: ['go', 'home', 'liao', 'ask', 'dad', 'to', 'pick', 'me', 'up', 'at']\n",
            "After stop word removal: ['go', 'home', 'liao', 'ask', 'dad', 'pick']\n",
            "After lemmatization: ['go', 'home', 'liao', 'ask', 'dad', 'pick']\n",
            "After lowercasing: omg you can make a wedding chapel in frontierville? why do they get all the good stuff?\n",
            "After removing special chars: omg you can make a wedding chapel in frontierville why do they get all the good stuff\n",
            "After tokenization: ['omg', 'you', 'can', 'make', 'a', 'wedding', 'chapel', 'in', 'frontierville', 'why', 'do', 'they', 'get', 'all', 'the', 'good', 'stuff']\n",
            "After stop word removal: ['omg', 'make', 'wedding', 'chapel', 'frontierville', 'get', 'good', 'stuff']\n",
            "After lemmatization: ['omg', 'make', 'wedding', 'chapel', 'frontierville', 'get', 'good', 'stuff']\n",
            "After lowercasing: i'm eatin now lor, but goin back to work soon... e mountain deer show huh... i watch b4 liao, very nice...\n",
            "After removing special chars: im eatin now lor but goin back to work soon e mountain deer show huh i watch b liao very nice\n",
            "After tokenization: ['im', 'eatin', 'now', 'lor', 'but', 'goin', 'back', 'to', 'work', 'soon', 'e', 'mountain', 'deer', 'show', 'huh', 'i', 'watch', 'b', 'liao', 'very', 'nice']\n",
            "After stop word removal: ['im', 'eatin', 'lor', 'goin', 'back', 'work', 'soon', 'e', 'mountain', 'deer', 'show', 'huh', 'watch', 'b', 'liao', 'nice']\n",
            "After lemmatization: ['im', 'eatin', 'lor', 'goin', 'back', 'work', 'soon', 'e', 'mountain', 'deer', 'show', 'huh', 'watch', 'b', 'liao', 'nice']\n",
            "After lowercasing: check mail.i have mailed varma and kept copy to you regarding membership.take care.insha allah.\n",
            "After removing special chars: check maili have mailed varma and kept copy to you regarding membershiptake careinsha allah\n",
            "After tokenization: ['check', 'maili', 'have', 'mailed', 'varma', 'and', 'kept', 'copy', 'to', 'you', 'regarding', 'membershiptake', 'careinsha', 'allah']\n",
            "After stop word removal: ['check', 'maili', 'mailed', 'varma', 'kept', 'copy', 'regarding', 'membershiptake', 'careinsha', 'allah']\n",
            "After lemmatization: ['check', 'maili', 'mailed', 'varma', 'kept', 'copy', 'regarding', 'membershiptake', 'careinsha', 'allah']\n",
            "After lowercasing: wrong phone! this phone! i answer this one but assume the other is people i don't well\n",
            "After removing special chars: wrong phone this phone i answer this one but assume the other is people i dont well\n",
            "After tokenization: ['wrong', 'phone', 'this', 'phone', 'i', 'answer', 'this', 'one', 'but', 'assume', 'the', 'other', 'is', 'people', 'i', 'dont', 'well']\n",
            "After stop word removal: ['wrong', 'phone', 'phone', 'answer', 'one', 'assume', 'people', 'dont', 'well']\n",
            "After lemmatization: ['wrong', 'phone', 'phone', 'answer', 'one', 'assume', 'people', 'dont', 'well']\n",
            "After lowercasing: anyway i don't think i can secure anything up here, lemme know if you want me to drive down south and chill\n",
            "After removing special chars: anyway i dont think i can secure anything up here lemme know if you want me to drive down south and chill\n",
            "After tokenization: ['anyway', 'i', 'dont', 'think', 'i', 'can', 'secure', 'anything', 'up', 'here', 'lem', 'me', 'know', 'if', 'you', 'want', 'me', 'to', 'drive', 'down', 'south', 'and', 'chill']\n",
            "After stop word removal: ['anyway', 'dont', 'think', 'secure', 'anything', 'lem', 'know', 'want', 'drive', 'south', 'chill']\n",
            "After lemmatization: ['anyway', 'dont', 'think', 'secure', 'anything', 'lem', 'know', 'want', 'drive', 'south', 'chill']\n",
            "After lowercasing: i'm already back home so no probably not\n",
            "After removing special chars: im already back home so no probably not\n",
            "After tokenization: ['im', 'already', 'back', 'home', 'so', 'no', 'probably', 'not']\n",
            "After stop word removal: ['im', 'already', 'back', 'home', 'probably']\n",
            "After lemmatization: ['im', 'already', 'back', 'home', 'probably']\n",
            "After lowercasing: great news! call freefone 08006344447 to claim your guaranteed å£1000 cash or å£2000 gift. speak to a live operator now!\n",
            "After removing special chars: great news call freefone  to claim your guaranteed  cash or  gift speak to a live operator now\n",
            "After tokenization: ['great', 'news', 'call', 'freefone', 'to', 'claim', 'your', 'guaranteed', 'cash', 'or', 'gift', 'speak', 'to', 'a', 'live', 'operator', 'now']\n",
            "After stop word removal: ['great', 'news', 'call', 'freefone', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'operator']\n",
            "After lemmatization: ['great', 'news', 'call', 'freefone', 'claim', 'guaranteed', 'cash', 'gift', 'speak', 'live', 'operator']\n",
            "After lowercasing: hi this is amy, we will be sending you a free phone number in a couple of days, which will give you an access to all the adult parties...\n",
            "After removing special chars: hi this is amy we will be sending you a free phone number in a couple of days which will give you an access to all the adult parties\n",
            "After tokenization: ['hi', 'this', 'is', 'amy', 'we', 'will', 'be', 'sending', 'you', 'a', 'free', 'phone', 'number', 'in', 'a', 'couple', 'of', 'days', 'which', 'will', 'give', 'you', 'an', 'access', 'to', 'all', 'the', 'adult', 'parties']\n",
            "After stop word removal: ['hi', 'amy', 'sending', 'free', 'phone', 'number', 'couple', 'days', 'give', 'access', 'adult', 'parties']\n",
            "After lemmatization: ['hi', 'amy', 'sending', 'free', 'phone', 'number', 'couple', 'day', 'give', 'access', 'adult', 'party']\n",
            "After lowercasing: i am in bus on the way to calicut\n",
            "After removing special chars: i am in bus on the way to calicut\n",
            "After tokenization: ['i', 'am', 'in', 'bus', 'on', 'the', 'way', 'to', 'calicut']\n",
            "After stop word removal: ['bus', 'way', 'calicut']\n",
            "After lemmatization: ['bus', 'way', 'calicut']\n",
            "After lowercasing: hi its me you are probably having too much fun to get this message but i thought id txt u cos im bored! and james has been farting at me all night\n",
            "After removing special chars: hi its me you are probably having too much fun to get this message but i thought id txt u cos im bored and james has been farting at me all night\n",
            "After tokenization: ['hi', 'its', 'me', 'you', 'are', 'probably', 'having', 'too', 'much', 'fun', 'to', 'get', 'this', 'message', 'but', 'i', 'thought', 'id', 'txt', 'u', 'cos', 'im', 'bored', 'and', 'james', 'has', 'been', 'farting', 'at', 'me', 'all', 'night']\n",
            "After stop word removal: ['hi', 'probably', 'much', 'fun', 'get', 'message', 'thought', 'id', 'txt', 'u', 'cos', 'im', 'bored', 'james', 'farting', 'night']\n",
            "After lemmatization: ['hi', 'probably', 'much', 'fun', 'get', 'message', 'thought', 'id', 'txt', 'u', 'co', 'im', 'bored', 'james', 'farting', 'night']\n",
            "After lowercasing: hi baby im sat on the bloody bus at the mo and i wont be home until about 7:30 wanna do somethin later? call me later ortxt back jess xx\n",
            "After removing special chars: hi baby im sat on the bloody bus at the mo and i wont be home until about  wanna do somethin later call me later ortxt back jess xx\n",
            "After tokenization: ['hi', 'baby', 'im', 'sat', 'on', 'the', 'bloody', 'bus', 'at', 'the', 'mo', 'and', 'i', 'wont', 'be', 'home', 'until', 'about', 'wan', 'na', 'do', 'somethin', 'later', 'call', 'me', 'later', 'ortxt', 'back', 'jess', 'xx']\n",
            "After stop word removal: ['hi', 'baby', 'im', 'sat', 'bloody', 'bus', 'mo', 'wont', 'home', 'wan', 'na', 'somethin', 'later', 'call', 'later', 'ortxt', 'back', 'jess', 'xx']\n",
            "After lemmatization: ['hi', 'baby', 'im', 'sat', 'bloody', 'bus', 'mo', 'wont', 'home', 'wan', 'na', 'somethin', 'later', 'call', 'later', 'ortxt', 'back', 'jess', 'xx']\n",
            "After lowercasing: welcome to select, an o2 service with added benefits. you can now call our specially trained advisors free from your mobile by dialling 402.\n",
            "After removing special chars: welcome to select an o service with added benefits you can now call our specially trained advisors free from your mobile by dialling \n",
            "After tokenization: ['welcome', 'to', 'select', 'an', 'o', 'service', 'with', 'added', 'benefits', 'you', 'can', 'now', 'call', 'our', 'specially', 'trained', 'advisors', 'free', 'from', 'your', 'mobile', 'by', 'dialling']\n",
            "After stop word removal: ['welcome', 'select', 'service', 'added', 'benefits', 'call', 'specially', 'trained', 'advisors', 'free', 'mobile', 'dialling']\n",
            "After lemmatization: ['welcome', 'select', 'service', 'added', 'benefit', 'call', 'specially', 'trained', 'advisor', 'free', 'mobile', 'dialling']\n",
            "After lowercasing: i lost 4 pounds since my doc visit last week woot woot! now i'm gonna celebrate by stuffing my face!\n",
            "After removing special chars: i lost  pounds since my doc visit last week woot woot now im gonna celebrate by stuffing my face\n",
            "After tokenization: ['i', 'lost', 'pounds', 'since', 'my', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'now', 'im', 'gon', 'na', 'celebrate', 'by', 'stuffing', 'my', 'face']\n",
            "After stop word removal: ['lost', 'pounds', 'since', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'im', 'gon', 'na', 'celebrate', 'stuffing', 'face']\n",
            "After lemmatization: ['lost', 'pound', 'since', 'doc', 'visit', 'last', 'week', 'woot', 'woot', 'im', 'gon', 'na', 'celebrate', 'stuffing', 'face']\n",
            "After lowercasing: u coming back 4 dinner rite? dad ask me so i re confirm wif u...\n",
            "After removing special chars: u coming back  dinner rite dad ask me so i re confirm wif u\n",
            "After tokenization: ['u', 'coming', 'back', 'dinner', 'rite', 'dad', 'ask', 'me', 'so', 'i', 're', 'confirm', 'wif', 'u']\n",
            "After stop word removal: ['u', 'coming', 'back', 'dinner', 'rite', 'dad', 'ask', 'confirm', 'wif', 'u']\n",
            "After lemmatization: ['u', 'coming', 'back', 'dinner', 'rite', 'dad', 'ask', 'confirm', 'wif', 'u']\n",
            "After lowercasing: doing my masters. when will you buy a bb cos i have for sale and how's bf\n",
            "After removing special chars: doing my masters when will you buy a bb cos i have for sale and hows bf\n",
            "After tokenization: ['doing', 'my', 'masters', 'when', 'will', 'you', 'buy', 'a', 'bb', 'cos', 'i', 'have', 'for', 'sale', 'and', 'hows', 'bf']\n",
            "After stop word removal: ['masters', 'buy', 'bb', 'cos', 'sale', 'hows', 'bf']\n",
            "After lemmatization: ['master', 'buy', 'bb', 'co', 'sale', 'hows', 'bf']\n",
            "After lowercasing: ahhhh...just woken up!had a bad dream about u tho,so i dont like u right now :) i didnt know anything about comedy night but i guess im up for it.\n",
            "After removing special chars: ahhhhjust woken uphad a bad dream about u thoso i dont like u right now  i didnt know anything about comedy night but i guess im up for it\n",
            "After tokenization: ['ahhhhjust', 'woken', 'uphad', 'a', 'bad', 'dream', 'about', 'u', 'thoso', 'i', 'dont', 'like', 'u', 'right', 'now', 'i', 'didnt', 'know', 'anything', 'about', 'comedy', 'night', 'but', 'i', 'guess', 'im', 'up', 'for', 'it']\n",
            "After stop word removal: ['ahhhhjust', 'woken', 'uphad', 'bad', 'dream', 'u', 'thoso', 'dont', 'like', 'u', 'right', 'didnt', 'know', 'anything', 'comedy', 'night', 'guess', 'im']\n",
            "After lemmatization: ['ahhhhjust', 'woken', 'uphad', 'bad', 'dream', 'u', 'thoso', 'dont', 'like', 'u', 'right', 'didnt', 'know', 'anything', 'comedy', 'night', 'guess', 'im']\n",
            "After lowercasing: i'm vivek:)i got call from your number.\n",
            "After removing special chars: im viveki got call from your number\n",
            "After tokenization: ['im', 'viveki', 'got', 'call', 'from', 'your', 'number']\n",
            "After stop word removal: ['im', 'viveki', 'got', 'call', 'number']\n",
            "After lemmatization: ['im', 'viveki', 'got', 'call', 'number']\n",
            "After lowercasing: why didn't u call on your lunch?\n",
            "After removing special chars: why didnt u call on your lunch\n",
            "After tokenization: ['why', 'didnt', 'u', 'call', 'on', 'your', 'lunch']\n",
            "After stop word removal: ['didnt', 'u', 'call', 'lunch']\n",
            "After lemmatization: ['didnt', 'u', 'call', 'lunch']\n",
            "After lowercasing: what i mean was i left too early to check, cos i'm working a 9-6.\n",
            "After removing special chars: what i mean was i left too early to check cos im working a \n",
            "After tokenization: ['what', 'i', 'mean', 'was', 'i', 'left', 'too', 'early', 'to', 'check', 'cos', 'im', 'working', 'a']\n",
            "After stop word removal: ['mean', 'left', 'early', 'check', 'cos', 'im', 'working']\n",
            "After lemmatization: ['mean', 'left', 'early', 'check', 'co', 'im', 'working']\n",
            "After lowercasing: i want  &lt;#&gt;  rs da:)do you have it?\n",
            "After removing special chars: i want  ltgt  rs dado you have it\n",
            "After tokenization: ['i', 'want', 'ltgt', 'rs', 'dado', 'you', 'have', 'it']\n",
            "After stop word removal: ['want', 'ltgt', 'rs', 'dado']\n",
            "After lemmatization: ['want', 'ltgt', 'r', 'dado']\n",
            "After lowercasing: a bit of ur smile is my hppnss, a drop of ur tear is my sorrow, a part of ur heart is my life, a heart like mine wil care for u, forevr as my goodfriend\n",
            "After removing special chars: a bit of ur smile is my hppnss a drop of ur tear is my sorrow a part of ur heart is my life a heart like mine wil care for u forevr as my goodfriend\n",
            "After tokenization: ['a', 'bit', 'of', 'ur', 'smile', 'is', 'my', 'hppnss', 'a', 'drop', 'of', 'ur', 'tear', 'is', 'my', 'sorrow', 'a', 'part', 'of', 'ur', 'heart', 'is', 'my', 'life', 'a', 'heart', 'like', 'mine', 'wil', 'care', 'for', 'u', 'forevr', 'as', 'my', 'goodfriend']\n",
            "After stop word removal: ['bit', 'ur', 'smile', 'hppnss', 'drop', 'ur', 'tear', 'sorrow', 'part', 'ur', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'u', 'forevr', 'goodfriend']\n",
            "After lemmatization: ['bit', 'ur', 'smile', 'hppnss', 'drop', 'ur', 'tear', 'sorrow', 'part', 'ur', 'heart', 'life', 'heart', 'like', 'mine', 'wil', 'care', 'u', 'forevr', 'goodfriend']\n",
            "After lowercasing: yup ok...\n",
            "After removing special chars: yup ok\n",
            "After tokenization: ['yup', 'ok']\n",
            "After stop word removal: ['yup', 'ok']\n",
            "After lemmatization: ['yup', 'ok']\n",
            "After lowercasing: i want to see your pretty pussy...\n",
            "After removing special chars: i want to see your pretty pussy\n",
            "After tokenization: ['i', 'want', 'to', 'see', 'your', 'pretty', 'pussy']\n",
            "After stop word removal: ['want', 'see', 'pretty', 'pussy']\n",
            "After lemmatization: ['want', 'see', 'pretty', 'pussy']\n",
            "After lowercasing: dear voucher holder have your next meal on us. use the following link on your pc 2 enjoy a 2 4 1 dining experiencehttp://www.vouch4me.com/etlp/dining.asp\n",
            "After removing special chars: dear voucher holder have your next meal on us use the following link on your pc  enjoy a    dining experiencehttpwwwvouchmecometlpdiningasp\n",
            "After tokenization: ['dear', 'voucher', 'holder', 'have', 'your', 'next', 'meal', 'on', 'us', 'use', 'the', 'following', 'link', 'on', 'your', 'pc', 'enjoy', 'a', 'dining', 'experiencehttpwwwvouchmecometlpdiningasp']\n",
            "After stop word removal: ['dear', 'voucher', 'holder', 'next', 'meal', 'us', 'use', 'following', 'link', 'pc', 'enjoy', 'dining', 'experiencehttpwwwvouchmecometlpdiningasp']\n",
            "After lemmatization: ['dear', 'voucher', 'holder', 'next', 'meal', 'u', 'use', 'following', 'link', 'pc', 'enjoy', 'dining', 'experiencehttpwwwvouchmecometlpdiningasp']\n",
            "After lowercasing: a few people are at the game, i'm at the mall with iouri and kaila\n",
            "After removing special chars: a few people are at the game im at the mall with iouri and kaila\n",
            "After tokenization: ['a', 'few', 'people', 'are', 'at', 'the', 'game', 'im', 'at', 'the', 'mall', 'with', 'iouri', 'and', 'kaila']\n",
            "After stop word removal: ['people', 'game', 'im', 'mall', 'iouri', 'kaila']\n",
            "After lemmatization: ['people', 'game', 'im', 'mall', 'iouri', 'kaila']\n",
            "After lowercasing: urgent! we are trying to contact u. todays draw shows that you have won a å£2000 prize guaranteed. call 09058094507 from land line. claim 3030. valid 12hrs only\n",
            "After removing special chars: urgent we are trying to contact u todays draw shows that you have won a  prize guaranteed call  from land line claim  valid hrs only\n",
            "After tokenization: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'u', 'todays', 'draw', 'shows', 'that', 'you', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only']\n",
            "After stop word removal: ['urgent', 'trying', 'contact', 'u', 'todays', 'draw', 'shows', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs']\n",
            "After lemmatization: ['urgent', 'trying', 'contact', 'u', 'today', 'draw', 'show', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr']\n",
            "After lowercasing: you can donate å£2.50 to unicef's asian tsunami disaster support fund by texting donate to 864233. å£2.50 will be added to your next bill\n",
            "After removing special chars: you can donate  to unicefs asian tsunami disaster support fund by texting donate to   will be added to your next bill\n",
            "After tokenization: ['you', 'can', 'donate', 'to', 'unicefs', 'asian', 'tsunami', 'disaster', 'support', 'fund', 'by', 'texting', 'donate', 'to', 'will', 'be', 'added', 'to', 'your', 'next', 'bill']\n",
            "After stop word removal: ['donate', 'unicefs', 'asian', 'tsunami', 'disaster', 'support', 'fund', 'texting', 'donate', 'added', 'next', 'bill']\n",
            "After lemmatization: ['donate', 'unicef', 'asian', 'tsunami', 'disaster', 'support', 'fund', 'texting', 'donate', 'added', 'next', 'bill']\n",
            "After lowercasing: future is not what we planned for tomorrow.....! it is the result of what we do today...! do the best in present... enjoy the future.\n",
            "After removing special chars: future is not what we planned for tomorrow it is the result of what we do today do the best in present enjoy the future\n",
            "After tokenization: ['future', 'is', 'not', 'what', 'we', 'planned', 'for', 'tomorrow', 'it', 'is', 'the', 'result', 'of', 'what', 'we', 'do', 'today', 'do', 'the', 'best', 'in', 'present', 'enjoy', 'the', 'future']\n",
            "After stop word removal: ['future', 'planned', 'tomorrow', 'result', 'today', 'best', 'present', 'enjoy', 'future']\n",
            "After lemmatization: ['future', 'planned', 'tomorrow', 'result', 'today', 'best', 'present', 'enjoy', 'future']\n",
            "After lowercasing: i will cme i want to go to hos 2morow. after that i wil cme. this what i got from her dear what to do. she didnt say any time\n",
            "After removing special chars: i will cme i want to go to hos morow after that i wil cme this what i got from her dear what to do she didnt say any time\n",
            "After tokenization: ['i', 'will', 'cme', 'i', 'want', 'to', 'go', 'to', 'hos', 'morow', 'after', 'that', 'i', 'wil', 'cme', 'this', 'what', 'i', 'got', 'from', 'her', 'dear', 'what', 'to', 'do', 'she', 'didnt', 'say', 'any', 'time']\n",
            "After stop word removal: ['cme', 'want', 'go', 'hos', 'morow', 'wil', 'cme', 'got', 'dear', 'didnt', 'say', 'time']\n",
            "After lemmatization: ['cme', 'want', 'go', 'ho', 'morow', 'wil', 'cme', 'got', 'dear', 'didnt', 'say', 'time']\n",
            "After lowercasing: we are supposed to meet to discuss abt our trip... thought xuhui told you? in the afternoon. thought we can go for lesson after that\n",
            "After removing special chars: we are supposed to meet to discuss abt our trip thought xuhui told you in the afternoon thought we can go for lesson after that\n",
            "After tokenization: ['we', 'are', 'supposed', 'to', 'meet', 'to', 'discuss', 'abt', 'our', 'trip', 'thought', 'xuhui', 'told', 'you', 'in', 'the', 'afternoon', 'thought', 'we', 'can', 'go', 'for', 'lesson', 'after', 'that']\n",
            "After stop word removal: ['supposed', 'meet', 'discuss', 'abt', 'trip', 'thought', 'xuhui', 'told', 'afternoon', 'thought', 'go', 'lesson']\n",
            "After lemmatization: ['supposed', 'meet', 'discus', 'abt', 'trip', 'thought', 'xuhui', 'told', 'afternoon', 'thought', 'go', 'lesson']\n",
            "After lowercasing: hey come online! use msn... we are all there\n",
            "After removing special chars: hey come online use msn we are all there\n",
            "After tokenization: ['hey', 'come', 'online', 'use', 'msn', 'we', 'are', 'all', 'there']\n",
            "After stop word removal: ['hey', 'come', 'online', 'use', 'msn']\n",
            "After lemmatization: ['hey', 'come', 'online', 'use', 'msn']\n",
            "After lowercasing: i'm fine. hope you are good. do take care.\n",
            "After removing special chars: im fine hope you are good do take care\n",
            "After tokenization: ['im', 'fine', 'hope', 'you', 'are', 'good', 'do', 'take', 'care']\n",
            "After stop word removal: ['im', 'fine', 'hope', 'good', 'take', 'care']\n",
            "After lemmatization: ['im', 'fine', 'hope', 'good', 'take', 'care']\n",
            "After lowercasing: oops i was in the shower when u called. hey a parking garage collapsed at university hospital. see i'm not crazy. stuff like that does happen.\n",
            "After removing special chars: oops i was in the shower when u called hey a parking garage collapsed at university hospital see im not crazy stuff like that does happen\n",
            "After tokenization: ['oops', 'i', 'was', 'in', 'the', 'shower', 'when', 'u', 'called', 'hey', 'a', 'parking', 'garage', 'collapsed', 'at', 'university', 'hospital', 'see', 'im', 'not', 'crazy', 'stuff', 'like', 'that', 'does', 'happen']\n",
            "After stop word removal: ['oops', 'shower', 'u', 'called', 'hey', 'parking', 'garage', 'collapsed', 'university', 'hospital', 'see', 'im', 'crazy', 'stuff', 'like', 'happen']\n",
            "After lemmatization: ['oops', 'shower', 'u', 'called', 'hey', 'parking', 'garage', 'collapsed', 'university', 'hospital', 'see', 'im', 'crazy', 'stuff', 'like', 'happen']\n",
            "After lowercasing: aiyo u so poor thing... then u dun wan 2 eat? u bathe already?\n",
            "After removing special chars: aiyo u so poor thing then u dun wan  eat u bathe already\n",
            "After tokenization: ['aiyo', 'u', 'so', 'poor', 'thing', 'then', 'u', 'dun', 'wan', 'eat', 'u', 'bathe', 'already']\n",
            "After stop word removal: ['aiyo', 'u', 'poor', 'thing', 'u', 'dun', 'wan', 'eat', 'u', 'bathe', 'already']\n",
            "After lemmatization: ['aiyo', 'u', 'poor', 'thing', 'u', 'dun', 'wan', 'eat', 'u', 'bathe', 'already']\n",
            "After lowercasing: yar... i tot u knew dis would happen long ago already.\n",
            "After removing special chars: yar i tot u knew dis would happen long ago already\n",
            "After tokenization: ['yar', 'i', 'tot', 'u', 'knew', 'dis', 'would', 'happen', 'long', 'ago', 'already']\n",
            "After stop word removal: ['yar', 'tot', 'u', 'knew', 'dis', 'would', 'happen', 'long', 'ago', 'already']\n",
            "After lemmatization: ['yar', 'tot', 'u', 'knew', 'dis', 'would', 'happen', 'long', 'ago', 'already']\n",
            "After lowercasing: you are gorgeous! keep those pix cumming :) thank you!\n",
            "After removing special chars: you are gorgeous keep those pix cumming  thank you\n",
            "After tokenization: ['you', 'are', 'gorgeous', 'keep', 'those', 'pix', 'cumming', 'thank', 'you']\n",
            "After stop word removal: ['gorgeous', 'keep', 'pix', 'cumming', 'thank']\n",
            "After lemmatization: ['gorgeous', 'keep', 'pix', 'cumming', 'thank']\n",
            "After lowercasing: a boy was late 2 home. his father: \\power of frndship\\\"\"\n",
            "After removing special chars: a boy was late  home his father power of frndship\n",
            "After tokenization: ['a', 'boy', 'was', 'late', 'home', 'his', 'father', 'power', 'of', 'frndship']\n",
            "After stop word removal: ['boy', 'late', 'home', 'father', 'power', 'frndship']\n",
            "After lemmatization: ['boy', 'late', 'home', 'father', 'power', 'frndship']\n",
            "After lowercasing: jade its paul. y didnåõt u txt me? do u remember me from barmed? i want 2 talk 2 u! txt me\n",
            "After removing special chars: jade its paul y didnt u txt me do u remember me from barmed i want  talk  u txt me\n",
            "After tokenization: ['jade', 'its', 'paul', 'y', 'didnt', 'u', 'txt', 'me', 'do', 'u', 'remember', 'me', 'from', 'barmed', 'i', 'want', 'talk', 'u', 'txt', 'me']\n",
            "After stop word removal: ['jade', 'paul', 'didnt', 'u', 'txt', 'u', 'remember', 'barmed', 'want', 'talk', 'u', 'txt']\n",
            "After lemmatization: ['jade', 'paul', 'didnt', 'u', 'txt', 'u', 'remember', 'barmed', 'want', 'talk', 'u', 'txt']\n",
            "After lowercasing: spending new years with my brother and his family. lets plan to meet next week. are you ready to be spoiled? :)\n",
            "After removing special chars: spending new years with my brother and his family lets plan to meet next week are you ready to be spoiled \n",
            "After tokenization: ['spending', 'new', 'years', 'with', 'my', 'brother', 'and', 'his', 'family', 'lets', 'plan', 'to', 'meet', 'next', 'week', 'are', 'you', 'ready', 'to', 'be', 'spoiled']\n",
            "After stop word removal: ['spending', 'new', 'years', 'brother', 'family', 'lets', 'plan', 'meet', 'next', 'week', 'ready', 'spoiled']\n",
            "After lemmatization: ['spending', 'new', 'year', 'brother', 'family', 'let', 'plan', 'meet', 'next', 'week', 'ready', 'spoiled']\n",
            "After lowercasing: so what u doing today?\n",
            "After removing special chars: so what u doing today\n",
            "After tokenization: ['so', 'what', 'u', 'doing', 'today']\n",
            "After stop word removal: ['u', 'today']\n",
            "After lemmatization: ['u', 'today']\n",
            "After lowercasing: i said its okay. sorry\n",
            "After removing special chars: i said its okay sorry\n",
            "After tokenization: ['i', 'said', 'its', 'okay', 'sorry']\n",
            "After stop word removal: ['said', 'okay', 'sorry']\n",
            "After lemmatization: ['said', 'okay', 'sorry']\n",
            "After lowercasing: slept? i thinkthis time ( &lt;#&gt;  pm) is not dangerous\n",
            "After removing special chars: slept i thinkthis time  ltgt  pm is not dangerous\n",
            "After tokenization: ['slept', 'i', 'thinkthis', 'time', 'ltgt', 'pm', 'is', 'not', 'dangerous']\n",
            "After stop word removal: ['slept', 'thinkthis', 'time', 'ltgt', 'pm', 'dangerous']\n",
            "After lemmatization: ['slept', 'thinkthis', 'time', 'ltgt', 'pm', 'dangerous']\n",
            "After lowercasing: networking job is there.\n",
            "After removing special chars: networking job is there\n",
            "After tokenization: ['networking', 'job', 'is', 'there']\n",
            "After stop word removal: ['networking', 'job']\n",
            "After lemmatization: ['networking', 'job']\n",
            "After lowercasing: goldviking (29/m) is inviting you to be his friend. reply yes-762 or no-762 see him: www.sms.ac/u/goldviking stop? send stop frnd to 62468\n",
            "After removing special chars: goldviking m is inviting you to be his friend reply yes or no see him wwwsmsacugoldviking stop send stop frnd to \n",
            "After tokenization: ['goldviking', 'm', 'is', 'inviting', 'you', 'to', 'be', 'his', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'him', 'wwwsmsacugoldviking', 'stop', 'send', 'stop', 'frnd', 'to']\n",
            "After stop word removal: ['goldviking', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacugoldviking', 'stop', 'send', 'stop', 'frnd']\n",
            "After lemmatization: ['goldviking', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacugoldviking', 'stop', 'send', 'stop', 'frnd']\n",
            "After lowercasing: dont let studying stress you out. l8r.\n",
            "After removing special chars: dont let studying stress you out lr\n",
            "After tokenization: ['dont', 'let', 'studying', 'stress', 'you', 'out', 'lr']\n",
            "After stop word removal: ['dont', 'let', 'studying', 'stress', 'lr']\n",
            "After lemmatization: ['dont', 'let', 'studying', 'stress', 'lr']\n",
            "After lowercasing: that's y u haf 2 keep me busy...\n",
            "After removing special chars: thats y u haf  keep me busy\n",
            "After tokenization: ['thats', 'y', 'u', 'haf', 'keep', 'me', 'busy']\n",
            "After stop word removal: ['thats', 'u', 'haf', 'keep', 'busy']\n",
            "After lemmatization: ['thats', 'u', 'haf', 'keep', 'busy']\n",
            "After lowercasing: no rushing. i'm not working. i'm in school so if we rush we go hungry.\n",
            "After removing special chars: no rushing im not working im in school so if we rush we go hungry\n",
            "After tokenization: ['no', 'rushing', 'im', 'not', 'working', 'im', 'in', 'school', 'so', 'if', 'we', 'rush', 'we', 'go', 'hungry']\n",
            "After stop word removal: ['rushing', 'im', 'working', 'im', 'school', 'rush', 'go', 'hungry']\n",
            "After lemmatization: ['rushing', 'im', 'working', 'im', 'school', 'rush', 'go', 'hungry']\n",
            "After lowercasing: which channel:-):-):):-).\n",
            "After removing special chars: which channel\n",
            "After tokenization: ['which', 'channel']\n",
            "After stop word removal: ['channel']\n",
            "After lemmatization: ['channel']\n",
            "After lowercasing: so your telling me i coulda been your real valentine and i wasn't? u never pick me for nothing!!\n",
            "After removing special chars: so your telling me i coulda been your real valentine and i wasnt u never pick me for nothing\n",
            "After tokenization: ['so', 'your', 'telling', 'me', 'i', 'coulda', 'been', 'your', 'real', 'valentine', 'and', 'i', 'wasnt', 'u', 'never', 'pick', 'me', 'for', 'nothing']\n",
            "After stop word removal: ['telling', 'coulda', 'real', 'valentine', 'wasnt', 'u', 'never', 'pick', 'nothing']\n",
            "After lemmatization: ['telling', 'coulda', 'real', 'valentine', 'wasnt', 'u', 'never', 'pick', 'nothing']\n",
            "After lowercasing: phony å£350 award - todays voda numbers ending xxxx are selected to receive a å£350 award. if you have a match please call 08712300220 quoting claim code 3100 standard rates app\n",
            "After removing special chars: phony  award  todays voda numbers ending xxxx are selected to receive a  award if you have a match please call  quoting claim code  standard rates app\n",
            "After tokenization: ['phony', 'award', 'todays', 'voda', 'numbers', 'ending', 'xxxx', 'are', 'selected', 'to', 'receive', 'a', 'award', 'if', 'you', 'have', 'a', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
            "After stop word removal: ['phony', 'award', 'todays', 'voda', 'numbers', 'ending', 'xxxx', 'selected', 'receive', 'award', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rates', 'app']\n",
            "After lemmatization: ['phony', 'award', 'today', 'voda', 'number', 'ending', 'xxxx', 'selected', 'receive', 'award', 'match', 'please', 'call', 'quoting', 'claim', 'code', 'standard', 'rate', 'app']\n",
            "After lowercasing: we made it! eta at taunton is 12:30 as planned, hope thatû÷s still okday?! good to see you! :-xx\n",
            "After removing special chars: we made it eta at taunton is  as planned hope thats still okday good to see you xx\n",
            "After tokenization: ['we', 'made', 'it', 'eta', 'at', 'taunton', 'is', 'as', 'planned', 'hope', 'thats', 'still', 'okday', 'good', 'to', 'see', 'you', 'xx']\n",
            "After stop word removal: ['made', 'eta', 'taunton', 'planned', 'hope', 'thats', 'still', 'okday', 'good', 'see', 'xx']\n",
            "After lemmatization: ['made', 'eta', 'taunton', 'planned', 'hope', 'thats', 'still', 'okday', 'good', 'see', 'xx']\n",
            "After lowercasing: i'm hungry buy smth home...\n",
            "After removing special chars: im hungry buy smth home\n",
            "After tokenization: ['im', 'hungry', 'buy', 'smth', 'home']\n",
            "After stop word removal: ['im', 'hungry', 'buy', 'smth', 'home']\n",
            "After lemmatization: ['im', 'hungry', 'buy', 'smth', 'home']\n",
            "After lowercasing: \\hey kate\n",
            "After removing special chars: hey kate\n",
            "After tokenization: ['hey', 'kate']\n",
            "After stop word removal: ['hey', 'kate']\n",
            "After lemmatization: ['hey', 'kate']\n",
            "After lowercasing: my drive can only be read. i need to write\n",
            "After removing special chars: my drive can only be read i need to write\n",
            "After tokenization: ['my', 'drive', 'can', 'only', 'be', 'read', 'i', 'need', 'to', 'write']\n",
            "After stop word removal: ['drive', 'read', 'need', 'write']\n",
            "After lemmatization: ['drive', 'read', 'need', 'write']\n",
            "After lowercasing: just looked it up and addie goes back monday, sucks to be her\n",
            "After removing special chars: just looked it up and addie goes back monday sucks to be her\n",
            "After tokenization: ['just', 'looked', 'it', 'up', 'and', 'addie', 'goes', 'back', 'monday', 'sucks', 'to', 'be', 'her']\n",
            "After stop word removal: ['looked', 'addie', 'goes', 'back', 'monday', 'sucks']\n",
            "After lemmatization: ['looked', 'addie', 'go', 'back', 'monday', 'suck']\n",
            "After lowercasing: happy new year. hope you are having a good semester\n",
            "After removing special chars: happy new year hope you are having a good semester\n",
            "After tokenization: ['happy', 'new', 'year', 'hope', 'you', 'are', 'having', 'a', 'good', 'semester']\n",
            "After stop word removal: ['happy', 'new', 'year', 'hope', 'good', 'semester']\n",
            "After lemmatization: ['happy', 'new', 'year', 'hope', 'good', 'semester']\n",
            "After lowercasing: esplanade lor. where else...\n",
            "After removing special chars: esplanade lor where else\n",
            "After tokenization: ['esplanade', 'lor', 'where', 'else']\n",
            "After stop word removal: ['esplanade', 'lor', 'else']\n",
            "After lemmatization: ['esplanade', 'lor', 'else']\n",
            "After lowercasing: can you talk with me..\n",
            "After removing special chars: can you talk with me\n",
            "After tokenization: ['can', 'you', 'talk', 'with', 'me']\n",
            "After stop word removal: ['talk']\n",
            "After lemmatization: ['talk']\n",
            "After lowercasing: hmph. go head, big baller.\n",
            "After removing special chars: hmph go head big baller\n",
            "After tokenization: ['hmph', 'go', 'head', 'big', 'baller']\n",
            "After stop word removal: ['hmph', 'go', 'head', 'big', 'baller']\n",
            "After lemmatization: ['hmph', 'go', 'head', 'big', 'baller']\n",
            "After lowercasing: well its not like you actually called someone a punto. that woulda been worse.\n",
            "After removing special chars: well its not like you actually called someone a punto that woulda been worse\n",
            "After tokenization: ['well', 'its', 'not', 'like', 'you', 'actually', 'called', 'someone', 'a', 'punto', 'that', 'woulda', 'been', 'worse']\n",
            "After stop word removal: ['well', 'like', 'actually', 'called', 'someone', 'punto', 'woulda', 'worse']\n",
            "After lemmatization: ['well', 'like', 'actually', 'called', 'someone', 'punto', 'woulda', 'worse']\n",
            "After lowercasing: nope. since ayo travelled, he has forgotten his guy\n",
            "After removing special chars: nope since ayo travelled he has forgotten his guy\n",
            "After tokenization: ['nope', 'since', 'ayo', 'travelled', 'he', 'has', 'forgotten', 'his', 'guy']\n",
            "After stop word removal: ['nope', 'since', 'ayo', 'travelled', 'forgotten', 'guy']\n",
            "After lemmatization: ['nope', 'since', 'ayo', 'travelled', 'forgotten', 'guy']\n",
            "After lowercasing: you still around? looking to pick up later\n",
            "After removing special chars: you still around looking to pick up later\n",
            "After tokenization: ['you', 'still', 'around', 'looking', 'to', 'pick', 'up', 'later']\n",
            "After stop word removal: ['still', 'around', 'looking', 'pick', 'later']\n",
            "After lemmatization: ['still', 'around', 'looking', 'pick', 'later']\n",
            "After lowercasing: cds 4u: congratulations ur awarded å£500 of cd gift vouchers or å£125 gift guaranteed & freeentry 2 å£100 wkly draw xt music to 87066 tncs www.ldew.com1win150ppmx3age16 \n",
            "After removing special chars: cds u congratulations ur awarded  of cd gift vouchers or  gift guaranteed  freeentry   wkly draw xt music to  tncs wwwldewcomwinppmxage \n",
            "After tokenization: ['cds', 'u', 'congratulations', 'ur', 'awarded', 'of', 'cd', 'gift', 'vouchers', 'or', 'gift', 'guaranteed', 'freeentry', 'wkly', 'draw', 'xt', 'music', 'to', 'tncs', 'wwwldewcomwinppmxage']\n",
            "After stop word removal: ['cds', 'u', 'congratulations', 'ur', 'awarded', 'cd', 'gift', 'vouchers', 'gift', 'guaranteed', 'freeentry', 'wkly', 'draw', 'xt', 'music', 'tncs', 'wwwldewcomwinppmxage']\n",
            "After lemmatization: ['cd', 'u', 'congratulation', 'ur', 'awarded', 'cd', 'gift', 'voucher', 'gift', 'guaranteed', 'freeentry', 'wkly', 'draw', 'xt', 'music', 'tncs', 'wwwldewcomwinppmxage']\n",
            "After lowercasing: there's someone here that has a year  &lt;#&gt;  toyota camry like mr olayiwola's own. mileage is  &lt;#&gt; k.its clean but i need to know how much will it sell for. if i can raise the dough for it how soon after landing will it sell. holla back.\n",
            "After removing special chars: theres someone here that has a year  ltgt  toyota camry like mr olayiwolas own mileage is  ltgt kits clean but i need to know how much will it sell for if i can raise the dough for it how soon after landing will it sell holla back\n",
            "After tokenization: ['theres', 'someone', 'here', 'that', 'has', 'a', 'year', 'ltgt', 'toyota', 'camry', 'like', 'mr', 'olayiwolas', 'own', 'mileage', 'is', 'ltgt', 'kits', 'clean', 'but', 'i', 'need', 'to', 'know', 'how', 'much', 'will', 'it', 'sell', 'for', 'if', 'i', 'can', 'raise', 'the', 'dough', 'for', 'it', 'how', 'soon', 'after', 'landing', 'will', 'it', 'sell', 'holla', 'back']\n",
            "After stop word removal: ['theres', 'someone', 'year', 'ltgt', 'toyota', 'camry', 'like', 'mr', 'olayiwolas', 'mileage', 'ltgt', 'kits', 'clean', 'need', 'know', 'much', 'sell', 'raise', 'dough', 'soon', 'landing', 'sell', 'holla', 'back']\n",
            "After lemmatization: ['there', 'someone', 'year', 'ltgt', 'toyota', 'camry', 'like', 'mr', 'olayiwolas', 'mileage', 'ltgt', 'kit', 'clean', 'need', 'know', 'much', 'sell', 'raise', 'dough', 'soon', 'landing', 'sell', 'holla', 'back']\n",
            "After lowercasing: guess which pub im in? im as happy as a pig in clover or whatever the saying is! \n",
            "After removing special chars: guess which pub im in im as happy as a pig in clover or whatever the saying is \n",
            "After tokenization: ['guess', 'which', 'pub', 'im', 'in', 'im', 'as', 'happy', 'as', 'a', 'pig', 'in', 'clover', 'or', 'whatever', 'the', 'saying', 'is']\n",
            "After stop word removal: ['guess', 'pub', 'im', 'im', 'happy', 'pig', 'clover', 'whatever', 'saying']\n",
            "After lemmatization: ['guess', 'pub', 'im', 'im', 'happy', 'pig', 'clover', 'whatever', 'saying']\n",
            "After lowercasing: ill b down soon\n",
            "After removing special chars: ill b down soon\n",
            "After tokenization: ['ill', 'b', 'down', 'soon']\n",
            "After stop word removal: ['ill', 'b', 'soon']\n",
            "After lemmatization: ['ill', 'b', 'soon']\n",
            "After lowercasing: oh k. . i will come tomorrow\n",
            "After removing special chars: oh k  i will come tomorrow\n",
            "After tokenization: ['oh', 'k', 'i', 'will', 'come', 'tomorrow']\n",
            "After stop word removal: ['oh', 'k', 'come', 'tomorrow']\n",
            "After lemmatization: ['oh', 'k', 'come', 'tomorrow']\n",
            "After lowercasing: go fool dont cheat others ok\n",
            "After removing special chars: go fool dont cheat others ok\n",
            "After tokenization: ['go', 'fool', 'dont', 'cheat', 'others', 'ok']\n",
            "After stop word removal: ['go', 'fool', 'dont', 'cheat', 'others', 'ok']\n",
            "After lemmatization: ['go', 'fool', 'dont', 'cheat', 'others', 'ok']\n",
            "After lowercasing: my mobile number.pls sms ur mail id.convey regards to achan,amma.rakhesh.qatar\n",
            "After removing special chars: my mobile numberpls sms ur mail idconvey regards to achanammarakheshqatar\n",
            "After tokenization: ['my', 'mobile', 'numberpls', 'sms', 'ur', 'mail', 'idconvey', 'regards', 'to', 'achanammarakheshqatar']\n",
            "After stop word removal: ['mobile', 'numberpls', 'sms', 'ur', 'mail', 'idconvey', 'regards', 'achanammarakheshqatar']\n",
            "After lemmatization: ['mobile', 'numberpls', 'sm', 'ur', 'mail', 'idconvey', 'regard', 'achanammarakheshqatar']\n",
            "After lowercasing: by the way, 'rencontre' is to meet again. mountains dont....\n",
            "After removing special chars: by the way rencontre is to meet again mountains dont\n",
            "After tokenization: ['by', 'the', 'way', 'rencontre', 'is', 'to', 'meet', 'again', 'mountains', 'dont']\n",
            "After stop word removal: ['way', 'rencontre', 'meet', 'mountains', 'dont']\n",
            "After lemmatization: ['way', 'rencontre', 'meet', 'mountain', 'dont']\n",
            "After lowercasing: you have won a guaranteed å£1000 cash or a å£2000 prize. to claim yr prize call our customer service representative on 08714712412 between 10am-7pm cost 10p\n",
            "After removing special chars: you have won a guaranteed  cash or a  prize to claim yr prize call our customer service representative on  between ampm cost p\n",
            "After tokenization: ['you', 'have', 'won', 'a', 'guaranteed', 'cash', 'or', 'a', 'prize', 'to', 'claim', 'yr', 'prize', 'call', 'our', 'customer', 'service', 'representative', 'on', 'between', 'ampm', 'cost', 'p']\n",
            "After stop word removal: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative', 'ampm', 'cost', 'p']\n",
            "After lemmatization: ['guaranteed', 'cash', 'prize', 'claim', 'yr', 'prize', 'call', 'customer', 'service', 'representative', 'ampm', 'cost', 'p']\n",
            "After lowercasing: u attend ur driving lesson how many times a wk n which day?\n",
            "After removing special chars: u attend ur driving lesson how many times a wk n which day\n",
            "After tokenization: ['u', 'attend', 'ur', 'driving', 'lesson', 'how', 'many', 'times', 'a', 'wk', 'n', 'which', 'day']\n",
            "After stop word removal: ['u', 'attend', 'ur', 'driving', 'lesson', 'many', 'times', 'wk', 'n', 'day']\n",
            "After lemmatization: ['u', 'attend', 'ur', 'driving', 'lesson', 'many', 'time', 'wk', 'n', 'day']\n",
            "After lowercasing: uncle g, just checking up on you. do have a rewarding month\n",
            "After removing special chars: uncle g just checking up on you do have a rewarding month\n",
            "After tokenization: ['uncle', 'g', 'just', 'checking', 'up', 'on', 'you', 'do', 'have', 'a', 'rewarding', 'month']\n",
            "After stop word removal: ['uncle', 'g', 'checking', 'rewarding', 'month']\n",
            "After lemmatization: ['uncle', 'g', 'checking', 'rewarding', 'month']\n",
            "After lowercasing: hello boytoy ! geeee ... i'm missing you today. i like to send you a tm and remind you i'm thinking of you ... and you are loved ... *loving kiss*\n",
            "After removing special chars: hello boytoy  geeee  im missing you today i like to send you a tm and remind you im thinking of you  and you are loved  loving kiss\n",
            "After tokenization: ['hello', 'boytoy', 'geeee', 'im', 'missing', 'you', 'today', 'i', 'like', 'to', 'send', 'you', 'a', 'tm', 'and', 'remind', 'you', 'im', 'thinking', 'of', 'you', 'and', 'you', 'are', 'loved', 'loving', 'kiss']\n",
            "After stop word removal: ['hello', 'boytoy', 'geeee', 'im', 'missing', 'today', 'like', 'send', 'tm', 'remind', 'im', 'thinking', 'loved', 'loving', 'kiss']\n",
            "After lemmatization: ['hello', 'boytoy', 'geeee', 'im', 'missing', 'today', 'like', 'send', 'tm', 'remind', 'im', 'thinking', 'loved', 'loving', 'kiss']\n",
            "After lowercasing: i think the other two still need to get cash but we can def be ready by 9\n",
            "After removing special chars: i think the other two still need to get cash but we can def be ready by \n",
            "After tokenization: ['i', 'think', 'the', 'other', 'two', 'still', 'need', 'to', 'get', 'cash', 'but', 'we', 'can', 'def', 'be', 'ready', 'by']\n",
            "After stop word removal: ['think', 'two', 'still', 'need', 'get', 'cash', 'def', 'ready']\n",
            "After lemmatization: ['think', 'two', 'still', 'need', 'get', 'cash', 'def', 'ready']\n",
            "After lowercasing: hey gals...u all wanna meet 4 dinner at nìâte? \n",
            "After removing special chars: hey galsu all wanna meet  dinner at nte \n",
            "After tokenization: ['hey', 'galsu', 'all', 'wan', 'na', 'meet', 'dinner', 'at', 'nte']\n",
            "After stop word removal: ['hey', 'galsu', 'wan', 'na', 'meet', 'dinner', 'nte']\n",
            "After lemmatization: ['hey', 'galsu', 'wan', 'na', 'meet', 'dinner', 'nte']\n",
            "After lowercasing: dear 0776xxxxxxx u've been invited to xchat. this is our final attempt to contact u! txt chat to 86688 150p/msgrcvdhg/suite342/2lands/row/w1j6hl ldn 18yrs\n",
            "After removing special chars: dear xxxxxxx uve been invited to xchat this is our final attempt to contact u txt chat to  pmsgrcvdhgsuitelandsrowwjhl ldn yrs\n",
            "After tokenization: ['dear', 'xxxxxxx', 'uve', 'been', 'invited', 'to', 'xchat', 'this', 'is', 'our', 'final', 'attempt', 'to', 'contact', 'u', 'txt', 'chat', 'to', 'pmsgrcvdhgsuitelandsrowwjhl', 'ldn', 'yrs']\n",
            "After stop word removal: ['dear', 'xxxxxxx', 'uve', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat', 'pmsgrcvdhgsuitelandsrowwjhl', 'ldn', 'yrs']\n",
            "After lemmatization: ['dear', 'xxxxxxx', 'uve', 'invited', 'xchat', 'final', 'attempt', 'contact', 'u', 'txt', 'chat', 'pmsgrcvdhgsuitelandsrowwjhl', 'ldn', 'yr']\n",
            "After lowercasing: babe ! what are you doing ? where are you ? who are you talking to ? do you think of me ? are you being a good boy? are you missing me? do you love me ?\n",
            "After removing special chars: babe  what are you doing  where are you  who are you talking to  do you think of me  are you being a good boy are you missing me do you love me \n",
            "After tokenization: ['babe', 'what', 'are', 'you', 'doing', 'where', 'are', 'you', 'who', 'are', 'you', 'talking', 'to', 'do', 'you', 'think', 'of', 'me', 'are', 'you', 'being', 'a', 'good', 'boy', 'are', 'you', 'missing', 'me', 'do', 'you', 'love', 'me']\n",
            "After stop word removal: ['babe', 'talking', 'think', 'good', 'boy', 'missing', 'love']\n",
            "After lemmatization: ['babe', 'talking', 'think', 'good', 'boy', 'missing', 'love']\n",
            "After lowercasing: great! how is the office today?\n",
            "After removing special chars: great how is the office today\n",
            "After tokenization: ['great', 'how', 'is', 'the', 'office', 'today']\n",
            "After stop word removal: ['great', 'office', 'today']\n",
            "After lemmatization: ['great', 'office', 'today']\n",
            "After lowercasing: it's cool, we can last a little while. getting more any time soon?\n",
            "After removing special chars: its cool we can last a little while getting more any time soon\n",
            "After tokenization: ['its', 'cool', 'we', 'can', 'last', 'a', 'little', 'while', 'getting', 'more', 'any', 'time', 'soon']\n",
            "After stop word removal: ['cool', 'last', 'little', 'getting', 'time', 'soon']\n",
            "After lemmatization: ['cool', 'last', 'little', 'getting', 'time', 'soon']\n",
            "After lowercasing: :-( sad puppy noise\n",
            "After removing special chars:  sad puppy noise\n",
            "After tokenization: ['sad', 'puppy', 'noise']\n",
            "After stop word removal: ['sad', 'puppy', 'noise']\n",
            "After lemmatization: ['sad', 'puppy', 'noise']\n",
            "After lowercasing: yes its possible but dint try. pls dont tell to any one k\n",
            "After removing special chars: yes its possible but dint try pls dont tell to any one k\n",
            "After tokenization: ['yes', 'its', 'possible', 'but', 'dint', 'try', 'pls', 'dont', 'tell', 'to', 'any', 'one', 'k']\n",
            "After stop word removal: ['yes', 'possible', 'dint', 'try', 'pls', 'dont', 'tell', 'one', 'k']\n",
            "After lemmatization: ['yes', 'possible', 'dint', 'try', 'pls', 'dont', 'tell', 'one', 'k']\n",
            "After lowercasing: anyway holla at me whenever you're around because i need an excuse to go creep on people in sarasota\n",
            "After removing special chars: anyway holla at me whenever youre around because i need an excuse to go creep on people in sarasota\n",
            "After tokenization: ['anyway', 'holla', 'at', 'me', 'whenever', 'youre', 'around', 'because', 'i', 'need', 'an', 'excuse', 'to', 'go', 'creep', 'on', 'people', 'in', 'sarasota']\n",
            "After stop word removal: ['anyway', 'holla', 'whenever', 'youre', 'around', 'need', 'excuse', 'go', 'creep', 'people', 'sarasota']\n",
            "After lemmatization: ['anyway', 'holla', 'whenever', 'youre', 'around', 'need', 'excuse', 'go', 'creep', 'people', 'sarasota']\n",
            "After lowercasing: where you. what happen\n",
            "After removing special chars: where you what happen\n",
            "After tokenization: ['where', 'you', 'what', 'happen']\n",
            "After stop word removal: ['happen']\n",
            "After lemmatization: ['happen']\n",
            "After lowercasing: i was gonna ask you lol but i think its at 7\n",
            "After removing special chars: i was gonna ask you lol but i think its at \n",
            "After tokenization: ['i', 'was', 'gon', 'na', 'ask', 'you', 'lol', 'but', 'i', 'think', 'its', 'at']\n",
            "After stop word removal: ['gon', 'na', 'ask', 'lol', 'think']\n",
            "After lemmatization: ['gon', 'na', 'ask', 'lol', 'think']\n",
            "After lowercasing: ur cash-balance is currently 500 pounds - to maximize ur cash-in now send go to 86688 only 150p/meg. cc: 08718720201 hg/suite342/2lands row/w1j6hl\n",
            "After removing special chars: ur cashbalance is currently  pounds  to maximize ur cashin now send go to  only pmeg cc  hgsuitelands rowwjhl\n",
            "After tokenization: ['ur', 'cashbalance', 'is', 'currently', 'pounds', 'to', 'maximize', 'ur', 'cashin', 'now', 'send', 'go', 'to', 'only', 'pmeg', 'cc', 'hgsuitelands', 'rowwjhl']\n",
            "After stop word removal: ['ur', 'cashbalance', 'currently', 'pounds', 'maximize', 'ur', 'cashin', 'send', 'go', 'pmeg', 'cc', 'hgsuitelands', 'rowwjhl']\n",
            "After lemmatization: ['ur', 'cashbalance', 'currently', 'pound', 'maximize', 'ur', 'cashin', 'send', 'go', 'pmeg', 'cc', 'hgsuitelands', 'rowwjhl']\n",
            "After lowercasing: private! your 2003 account statement for shows 800 un-redeemed s.i.m. points. call 08715203685 identifier code:4xx26 expires 13/10/04\n",
            "After removing special chars: private your  account statement for shows  unredeemed sim points call  identifier codexx expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'sim', 'points', 'call', 'identifier', 'codexx', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'sim', 'points', 'call', 'identifier', 'codexx', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'sim', 'point', 'call', 'identifier', 'codexx', 'expires']\n",
            "After lowercasing: go chase after her and run her over while she's crossing the street\n",
            "After removing special chars: go chase after her and run her over while shes crossing the street\n",
            "After tokenization: ['go', 'chase', 'after', 'her', 'and', 'run', 'her', 'over', 'while', 'shes', 'crossing', 'the', 'street']\n",
            "After stop word removal: ['go', 'chase', 'run', 'shes', 'crossing', 'street']\n",
            "After lemmatization: ['go', 'chase', 'run', 'shes', 'crossing', 'street']\n",
            "After lowercasing: i'd like to tell you my deepest darkest fantasies. call me 09094646631 just 60p/min. to stop texts call 08712460324 (nat rate)\n",
            "After removing special chars: id like to tell you my deepest darkest fantasies call me  just pmin to stop texts call  nat rate\n",
            "After tokenization: ['id', 'like', 'to', 'tell', 'you', 'my', 'deepest', 'darkest', 'fantasies', 'call', 'me', 'just', 'pmin', 'to', 'stop', 'texts', 'call', 'nat', 'rate']\n",
            "After stop word removal: ['id', 'like', 'tell', 'deepest', 'darkest', 'fantasies', 'call', 'pmin', 'stop', 'texts', 'call', 'nat', 'rate']\n",
            "After lemmatization: ['id', 'like', 'tell', 'deepest', 'darkest', 'fantasy', 'call', 'pmin', 'stop', 'text', 'call', 'nat', 'rate']\n",
            "After lowercasing: is there coming friday is leave for pongal?do you get any news from your work place.\n",
            "After removing special chars: is there coming friday is leave for pongaldo you get any news from your work place\n",
            "After tokenization: ['is', 'there', 'coming', 'friday', 'is', 'leave', 'for', 'pongaldo', 'you', 'get', 'any', 'news', 'from', 'your', 'work', 'place']\n",
            "After stop word removal: ['coming', 'friday', 'leave', 'pongaldo', 'get', 'news', 'work', 'place']\n",
            "After lemmatization: ['coming', 'friday', 'leave', 'pongaldo', 'get', 'news', 'work', 'place']\n",
            "After lowercasing: hey... very inconvenient for your sis a not huh?\n",
            "After removing special chars: hey very inconvenient for your sis a not huh\n",
            "After tokenization: ['hey', 'very', 'inconvenient', 'for', 'your', 'sis', 'a', 'not', 'huh']\n",
            "After stop word removal: ['hey', 'inconvenient', 'sis', 'huh']\n",
            "After lemmatization: ['hey', 'inconvenient', 'si', 'huh']\n",
            "After lowercasing: ok i vl..do u know i got adsense approved..\n",
            "After removing special chars: ok i vldo u know i got adsense approved\n",
            "After tokenization: ['ok', 'i', 'vldo', 'u', 'know', 'i', 'got', 'adsense', 'approved']\n",
            "After stop word removal: ['ok', 'vldo', 'u', 'know', 'got', 'adsense', 'approved']\n",
            "After lemmatization: ['ok', 'vldo', 'u', 'know', 'got', 'adsense', 'approved']\n",
            "After lowercasing: * was really good to see you the other day dudette, been missing you!\n",
            "After removing special chars:  was really good to see you the other day dudette been missing you\n",
            "After tokenization: ['was', 'really', 'good', 'to', 'see', 'you', 'the', 'other', 'day', 'dudette', 'been', 'missing', 'you']\n",
            "After stop word removal: ['really', 'good', 'see', 'day', 'dudette', 'missing']\n",
            "After lemmatization: ['really', 'good', 'see', 'day', 'dudette', 'missing']\n",
            "After lowercasing: i want to go to perumbavoor\n",
            "After removing special chars: i want to go to perumbavoor\n",
            "After tokenization: ['i', 'want', 'to', 'go', 'to', 'perumbavoor']\n",
            "After stop word removal: ['want', 'go', 'perumbavoor']\n",
            "After lemmatization: ['want', 'go', 'perumbavoor']\n",
            "After lowercasing: how many times i told in the stage all use to laugh. you not listen aha.\n",
            "After removing special chars: how many times i told in the stage all use to laugh you not listen aha\n",
            "After tokenization: ['how', 'many', 'times', 'i', 'told', 'in', 'the', 'stage', 'all', 'use', 'to', 'laugh', 'you', 'not', 'listen', 'aha']\n",
            "After stop word removal: ['many', 'times', 'told', 'stage', 'use', 'laugh', 'listen', 'aha']\n",
            "After lemmatization: ['many', 'time', 'told', 'stage', 'use', 'laugh', 'listen', 'aha']\n",
            "After lowercasing: you won't believe it but it's true. it's incredible txts! reply g now to learn truly amazing things that will blow your mind. from o2fwd only 18p/txt\n",
            "After removing special chars: you wont believe it but its true its incredible txts reply g now to learn truly amazing things that will blow your mind from ofwd only ptxt\n",
            "After tokenization: ['you', 'wont', 'believe', 'it', 'but', 'its', 'true', 'its', 'incredible', 'txts', 'reply', 'g', 'now', 'to', 'learn', 'truly', 'amazing', 'things', 'that', 'will', 'blow', 'your', 'mind', 'from', 'ofwd', 'only', 'ptxt']\n",
            "After stop word removal: ['wont', 'believe', 'true', 'incredible', 'txts', 'reply', 'g', 'learn', 'truly', 'amazing', 'things', 'blow', 'mind', 'ofwd', 'ptxt']\n",
            "After lemmatization: ['wont', 'believe', 'true', 'incredible', 'txts', 'reply', 'g', 'learn', 'truly', 'amazing', 'thing', 'blow', 'mind', 'ofwd', 'ptxt']\n",
            "After lowercasing: (you didn't hear it from me)\n",
            "After removing special chars: you didnt hear it from me\n",
            "After tokenization: ['you', 'didnt', 'hear', 'it', 'from', 'me']\n",
            "After stop word removal: ['didnt', 'hear']\n",
            "After lemmatization: ['didnt', 'hear']\n",
            "After lowercasing: thanks for being there for me just to talk to on saturday. you are very dear to me. i cherish having you as a brother and role model.\n",
            "After removing special chars: thanks for being there for me just to talk to on saturday you are very dear to me i cherish having you as a brother and role model\n",
            "After tokenization: ['thanks', 'for', 'being', 'there', 'for', 'me', 'just', 'to', 'talk', 'to', 'on', 'saturday', 'you', 'are', 'very', 'dear', 'to', 'me', 'i', 'cherish', 'having', 'you', 'as', 'a', 'brother', 'and', 'role', 'model']\n",
            "After stop word removal: ['thanks', 'talk', 'saturday', 'dear', 'cherish', 'brother', 'role', 'model']\n",
            "After lemmatization: ['thanks', 'talk', 'saturday', 'dear', 'cherish', 'brother', 'role', 'model']\n",
            "After lowercasing: pls clarify back if an open return ticket that i have can be preponed for me to go back to kerala.\n",
            "After removing special chars: pls clarify back if an open return ticket that i have can be preponed for me to go back to kerala\n",
            "After tokenization: ['pls', 'clarify', 'back', 'if', 'an', 'open', 'return', 'ticket', 'that', 'i', 'have', 'can', 'be', 'preponed', 'for', 'me', 'to', 'go', 'back', 'to', 'kerala']\n",
            "After stop word removal: ['pls', 'clarify', 'back', 'open', 'return', 'ticket', 'preponed', 'go', 'back', 'kerala']\n",
            "After lemmatization: ['pls', 'clarify', 'back', 'open', 'return', 'ticket', 'preponed', 'go', 'back', 'kerala']\n",
            "After lowercasing: natalie (20/f) is inviting you to be her friend. reply yes-165 or no-165 see her: www.sms.ac/u/natalie2k9 stop? send stop frnd to 62468\n",
            "After removing special chars: natalie f is inviting you to be her friend reply yes or no see her wwwsmsacunataliek stop send stop frnd to \n",
            "After tokenization: ['natalie', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'wwwsmsacunataliek', 'stop', 'send', 'stop', 'frnd', 'to']\n",
            "After stop word removal: ['natalie', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacunataliek', 'stop', 'send', 'stop', 'frnd']\n",
            "After lemmatization: ['natalie', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacunataliek', 'stop', 'send', 'stop', 'frnd']\n",
            "After lowercasing: she ran off with a younger man. we will make pretty babies together :)\n",
            "After removing special chars: she ran off with a younger man we will make pretty babies together \n",
            "After tokenization: ['she', 'ran', 'off', 'with', 'a', 'younger', 'man', 'we', 'will', 'make', 'pretty', 'babies', 'together']\n",
            "After stop word removal: ['ran', 'younger', 'man', 'make', 'pretty', 'babies', 'together']\n",
            "After lemmatization: ['ran', 'younger', 'man', 'make', 'pretty', 'baby', 'together']\n",
            "After lowercasing: jamster! to get your free wallpaper text heart to 88888 now! t&c apply. 16 only. need help? call 08701213186.\n",
            "After removing special chars: jamster to get your free wallpaper text heart to  now tc apply  only need help call \n",
            "After tokenization: ['jamster', 'to', 'get', 'your', 'free', 'wallpaper', 'text', 'heart', 'to', 'now', 'tc', 'apply', 'only', 'need', 'help', 'call']\n",
            "After stop word removal: ['jamster', 'get', 'free', 'wallpaper', 'text', 'heart', 'tc', 'apply', 'need', 'help', 'call']\n",
            "After lemmatization: ['jamster', 'get', 'free', 'wallpaper', 'text', 'heart', 'tc', 'apply', 'need', 'help', 'call']\n",
            "After lowercasing: o ic lol. should play 9 doors sometime yo\n",
            "After removing special chars: o ic lol should play  doors sometime yo\n",
            "After tokenization: ['o', 'ic', 'lol', 'should', 'play', 'doors', 'sometime', 'yo']\n",
            "After stop word removal: ['ic', 'lol', 'play', 'doors', 'sometime', 'yo']\n",
            "After lemmatization: ['ic', 'lol', 'play', 'door', 'sometime', 'yo']\n",
            "After lowercasing: dunno, my dad said he coming home 2 bring us out 4 lunch. yup i go w u lor. i call u when i reach school lor...\n",
            "After removing special chars: dunno my dad said he coming home  bring us out  lunch yup i go w u lor i call u when i reach school lor\n",
            "After tokenization: ['dunno', 'my', 'dad', 'said', 'he', 'coming', 'home', 'bring', 'us', 'out', 'lunch', 'yup', 'i', 'go', 'w', 'u', 'lor', 'i', 'call', 'u', 'when', 'i', 'reach', 'school', 'lor']\n",
            "After stop word removal: ['dunno', 'dad', 'said', 'coming', 'home', 'bring', 'us', 'lunch', 'yup', 'go', 'w', 'u', 'lor', 'call', 'u', 'reach', 'school', 'lor']\n",
            "After lemmatization: ['dunno', 'dad', 'said', 'coming', 'home', 'bring', 'u', 'lunch', 'yup', 'go', 'w', 'u', 'lor', 'call', 'u', 'reach', 'school', 'lor']\n",
            "After lowercasing: we have sent jd for customer service cum accounts executive to ur mail id, for details contact us\n",
            "After removing special chars: we have sent jd for customer service cum accounts executive to ur mail id for details contact us\n",
            "After tokenization: ['we', 'have', 'sent', 'jd', 'for', 'customer', 'service', 'cum', 'accounts', 'executive', 'to', 'ur', 'mail', 'id', 'for', 'details', 'contact', 'us']\n",
            "After stop word removal: ['sent', 'jd', 'customer', 'service', 'cum', 'accounts', 'executive', 'ur', 'mail', 'id', 'details', 'contact', 'us']\n",
            "After lemmatization: ['sent', 'jd', 'customer', 'service', 'cum', 'account', 'executive', 'ur', 'mail', 'id', 'detail', 'contact', 'u']\n",
            "After lowercasing: desires- u going to doctor 4 liver. and get a bit stylish. get ur hair managed. thats it.\n",
            "After removing special chars: desires u going to doctor  liver and get a bit stylish get ur hair managed thats it\n",
            "After tokenization: ['desires', 'u', 'going', 'to', 'doctor', 'liver', 'and', 'get', 'a', 'bit', 'stylish', 'get', 'ur', 'hair', 'managed', 'thats', 'it']\n",
            "After stop word removal: ['desires', 'u', 'going', 'doctor', 'liver', 'get', 'bit', 'stylish', 'get', 'ur', 'hair', 'managed', 'thats']\n",
            "After lemmatization: ['desire', 'u', 'going', 'doctor', 'liver', 'get', 'bit', 'stylish', 'get', 'ur', 'hair', 'managed', 'thats']\n",
            "After lowercasing: hmmm.still we dont have opener?\n",
            "After removing special chars: hmmmstill we dont have opener\n",
            "After tokenization: ['hmmmstill', 'we', 'dont', 'have', 'opener']\n",
            "After stop word removal: ['hmmmstill', 'dont', 'opener']\n",
            "After lemmatization: ['hmmmstill', 'dont', 'opener']\n",
            "After lowercasing: yeah so basically any time next week you can get away from your mom &amp; get up before 3\n",
            "After removing special chars: yeah so basically any time next week you can get away from your mom amp get up before \n",
            "After tokenization: ['yeah', 'so', 'basically', 'any', 'time', 'next', 'week', 'you', 'can', 'get', 'away', 'from', 'your', 'mom', 'amp', 'get', 'up', 'before']\n",
            "After stop word removal: ['yeah', 'basically', 'time', 'next', 'week', 'get', 'away', 'mom', 'amp', 'get']\n",
            "After lemmatization: ['yeah', 'basically', 'time', 'next', 'week', 'get', 'away', 'mom', 'amp', 'get']\n",
            "After lowercasing: edison has rightly said, \\a fool can ask more questions than a wise man can answer\\\" now you know why all of us are speechless during viva.. gm\n",
            "After removing special chars: edison has rightly said a fool can ask more questions than a wise man can answer now you know why all of us are speechless during viva gm\n",
            "After tokenization: ['edison', 'has', 'rightly', 'said', 'a', 'fool', 'can', 'ask', 'more', 'questions', 'than', 'a', 'wise', 'man', 'can', 'answer', 'now', 'you', 'know', 'why', 'all', 'of', 'us', 'are', 'speechless', 'during', 'viva', 'gm']\n",
            "After stop word removal: ['edison', 'rightly', 'said', 'fool', 'ask', 'questions', 'wise', 'man', 'answer', 'know', 'us', 'speechless', 'viva', 'gm']\n",
            "After lemmatization: ['edison', 'rightly', 'said', 'fool', 'ask', 'question', 'wise', 'man', 'answer', 'know', 'u', 'speechless', 'viva', 'gm']\n",
            "After lowercasing: i will vote for wherever my heart guides me\n",
            "After removing special chars: i will vote for wherever my heart guides me\n",
            "After tokenization: ['i', 'will', 'vote', 'for', 'wherever', 'my', 'heart', 'guides', 'me']\n",
            "After stop word removal: ['vote', 'wherever', 'heart', 'guides']\n",
            "After lemmatization: ['vote', 'wherever', 'heart', 'guide']\n",
            "After lowercasing: with my sis lor... we juz watched italian job.\n",
            "After removing special chars: with my sis lor we juz watched italian job\n",
            "After tokenization: ['with', 'my', 'sis', 'lor', 'we', 'juz', 'watched', 'italian', 'job']\n",
            "After stop word removal: ['sis', 'lor', 'juz', 'watched', 'italian', 'job']\n",
            "After lemmatization: ['si', 'lor', 'juz', 'watched', 'italian', 'job']\n",
            "After lowercasing: tick, tick, tick .... where are you ? i could die of loneliness you know ! *pouts* *stomps feet* i need you ...\n",
            "After removing special chars: tick tick tick  where are you  i could die of loneliness you know  pouts stomps feet i need you \n",
            "After tokenization: ['tick', 'tick', 'tick', 'where', 'are', 'you', 'i', 'could', 'die', 'of', 'loneliness', 'you', 'know', 'pouts', 'stomps', 'feet', 'i', 'need', 'you']\n",
            "After stop word removal: ['tick', 'tick', 'tick', 'could', 'die', 'loneliness', 'know', 'pouts', 'stomps', 'feet', 'need']\n",
            "After lemmatization: ['tick', 'tick', 'tick', 'could', 'die', 'loneliness', 'know', 'pout', 'stomp', 'foot', 'need']\n",
            "After lowercasing: lmao you know me so well...\n",
            "After removing special chars: lmao you know me so well\n",
            "After tokenization: ['lmao', 'you', 'know', 'me', 'so', 'well']\n",
            "After stop word removal: ['lmao', 'know', 'well']\n",
            "After lemmatization: ['lmao', 'know', 'well']\n",
            "After lowercasing: double mins & double txt & 1/2 price linerental on latest orange bluetooth mobiles. call mobileupd8 for the very latest offers. 08000839402 or call2optout/lf56\n",
            "After removing special chars: double mins  double txt   price linerental on latest orange bluetooth mobiles call mobileupd for the very latest offers  or calloptoutlf\n",
            "After tokenization: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'on', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'for', 'the', 'very', 'latest', 'offers', 'or', 'calloptoutlf']\n",
            "After stop word removal: ['double', 'mins', 'double', 'txt', 'price', 'linerental', 'latest', 'orange', 'bluetooth', 'mobiles', 'call', 'mobileupd', 'latest', 'offers', 'calloptoutlf']\n",
            "After lemmatization: ['double', 'min', 'double', 'txt', 'price', 'linerental', 'latest', 'orange', 'bluetooth', 'mobile', 'call', 'mobileupd', 'latest', 'offer', 'calloptoutlf']\n",
            "After lowercasing: am on a train back from northampton so i'm afraid not! i'm staying skyving off today ho ho! will be around wednesday though. do you fancy the comedy club this week by the way?\n",
            "After removing special chars: am on a train back from northampton so im afraid not im staying skyving off today ho ho will be around wednesday though do you fancy the comedy club this week by the way\n",
            "After tokenization: ['am', 'on', 'a', 'train', 'back', 'from', 'northampton', 'so', 'im', 'afraid', 'not', 'im', 'staying', 'skyving', 'off', 'today', 'ho', 'ho', 'will', 'be', 'around', 'wednesday', 'though', 'do', 'you', 'fancy', 'the', 'comedy', 'club', 'this', 'week', 'by', 'the', 'way']\n",
            "After stop word removal: ['train', 'back', 'northampton', 'im', 'afraid', 'im', 'staying', 'skyving', 'today', 'ho', 'ho', 'around', 'wednesday', 'though', 'fancy', 'comedy', 'club', 'week', 'way']\n",
            "After lemmatization: ['train', 'back', 'northampton', 'im', 'afraid', 'im', 'staying', 'skyving', 'today', 'ho', 'ho', 'around', 'wednesday', 'though', 'fancy', 'comedy', 'club', 'week', 'way']\n",
            "After lowercasing: goodnight da thangam i really miss u dear.\n",
            "After removing special chars: goodnight da thangam i really miss u dear\n",
            "After tokenization: ['goodnight', 'da', 'thangam', 'i', 'really', 'miss', 'u', 'dear']\n",
            "After stop word removal: ['goodnight', 'da', 'thangam', 'really', 'miss', 'u', 'dear']\n",
            "After lemmatization: ['goodnight', 'da', 'thangam', 'really', 'miss', 'u', 'dear']\n",
            "After lowercasing: hey next sun 1030 there's a basic yoga course... at bugis... we can go for that... pilates intro next sat.... tell me what time you r free\n",
            "After removing special chars: hey next sun  theres a basic yoga course at bugis we can go for that pilates intro next sat tell me what time you r free\n",
            "After tokenization: ['hey', 'next', 'sun', 'theres', 'a', 'basic', 'yoga', 'course', 'at', 'bugis', 'we', 'can', 'go', 'for', 'that', 'pilates', 'intro', 'next', 'sat', 'tell', 'me', 'what', 'time', 'you', 'r', 'free']\n",
            "After stop word removal: ['hey', 'next', 'sun', 'theres', 'basic', 'yoga', 'course', 'bugis', 'go', 'pilates', 'intro', 'next', 'sat', 'tell', 'time', 'r', 'free']\n",
            "After lemmatization: ['hey', 'next', 'sun', 'there', 'basic', 'yoga', 'course', 'bugis', 'go', 'pilate', 'intro', 'next', 'sat', 'tell', 'time', 'r', 'free']\n",
            "After lowercasing: geeeee ... your internet is really bad today, eh ?\n",
            "After removing special chars: geeeee  your internet is really bad today eh \n",
            "After tokenization: ['geeeee', 'your', 'internet', 'is', 'really', 'bad', 'today', 'eh']\n",
            "After stop word removal: ['geeeee', 'internet', 'really', 'bad', 'today', 'eh']\n",
            "After lemmatization: ['geeeee', 'internet', 'really', 'bad', 'today', 'eh']\n",
            "After lowercasing: free video camera phones with half price line rental for 12 mths and 500 cross ntwk mins 100 txts. call mobileupd8 08001950382 or call2optout/674\n",
            "After removing special chars: free video camera phones with half price line rental for  mths and  cross ntwk mins  txts call mobileupd  or calloptout\n",
            "After tokenization: ['free', 'video', 'camera', 'phones', 'with', 'half', 'price', 'line', 'rental', 'for', 'mths', 'and', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'or', 'calloptout']\n",
            "After stop word removal: ['free', 'video', 'camera', 'phones', 'half', 'price', 'line', 'rental', 'mths', 'cross', 'ntwk', 'mins', 'txts', 'call', 'mobileupd', 'calloptout']\n",
            "After lemmatization: ['free', 'video', 'camera', 'phone', 'half', 'price', 'line', 'rental', 'mths', 'cross', 'ntwk', 'min', 'txts', 'call', 'mobileupd', 'calloptout']\n",
            "After lowercasing: i think i am disturbing her da\n",
            "After removing special chars: i think i am disturbing her da\n",
            "After tokenization: ['i', 'think', 'i', 'am', 'disturbing', 'her', 'da']\n",
            "After stop word removal: ['think', 'disturbing', 'da']\n",
            "After lemmatization: ['think', 'disturbing', 'da']\n",
            "After lowercasing: sorry, i'll call you  later. i am in meeting sir.\n",
            "After removing special chars: sorry ill call you  later i am in meeting sir\n",
            "After tokenization: ['sorry', 'ill', 'call', 'you', 'later', 'i', 'am', 'in', 'meeting', 'sir']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later', 'meeting', 'sir']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later', 'meeting', 'sir']\n",
            "After lowercasing: havent stuck at orchard in my dad's car. going 4 dinner now. u leh? so r they free tonight?\n",
            "After removing special chars: havent stuck at orchard in my dads car going  dinner now u leh so r they free tonight\n",
            "After tokenization: ['havent', 'stuck', 'at', 'orchard', 'in', 'my', 'dads', 'car', 'going', 'dinner', 'now', 'u', 'leh', 'so', 'r', 'they', 'free', 'tonight']\n",
            "After stop word removal: ['havent', 'stuck', 'orchard', 'dads', 'car', 'going', 'dinner', 'u', 'leh', 'r', 'free', 'tonight']\n",
            "After lemmatization: ['havent', 'stuck', 'orchard', 'dad', 'car', 'going', 'dinner', 'u', 'leh', 'r', 'free', 'tonight']\n",
            "After lowercasing: ok i also wan 2 watch e 9 pm show...\n",
            "After removing special chars: ok i also wan  watch e  pm show\n",
            "After tokenization: ['ok', 'i', 'also', 'wan', 'watch', 'e', 'pm', 'show']\n",
            "After stop word removal: ['ok', 'also', 'wan', 'watch', 'e', 'pm', 'show']\n",
            "After lemmatization: ['ok', 'also', 'wan', 'watch', 'e', 'pm', 'show']\n",
            "After lowercasing: i dunno lei... like dun haf...\n",
            "After removing special chars: i dunno lei like dun haf\n",
            "After tokenization: ['i', 'dunno', 'lei', 'like', 'dun', 'haf']\n",
            "After stop word removal: ['dunno', 'lei', 'like', 'dun', 'haf']\n",
            "After lemmatization: ['dunno', 'lei', 'like', 'dun', 'haf']\n",
            "After lowercasing: but your brother transfered only  &lt;#&gt;  +  &lt;#&gt; . pa.\n",
            "After removing special chars: but your brother transfered only  ltgt    ltgt  pa\n",
            "After tokenization: ['but', 'your', 'brother', 'transfered', 'only', 'ltgt', 'ltgt', 'pa']\n",
            "After stop word removal: ['brother', 'transfered', 'ltgt', 'ltgt', 'pa']\n",
            "After lemmatization: ['brother', 'transfered', 'ltgt', 'ltgt', 'pa']\n",
            "After lowercasing: i calls you later. afternoon onwords mtnl service get problem in south mumbai. i can hear you but you cann't listen me.\n",
            "After removing special chars: i calls you later afternoon onwords mtnl service get problem in south mumbai i can hear you but you cannt listen me\n",
            "After tokenization: ['i', 'calls', 'you', 'later', 'afternoon', 'onwords', 'mtnl', 'service', 'get', 'problem', 'in', 'south', 'mumbai', 'i', 'can', 'hear', 'you', 'but', 'you', 'cannt', 'listen', 'me']\n",
            "After stop word removal: ['calls', 'later', 'afternoon', 'onwords', 'mtnl', 'service', 'get', 'problem', 'south', 'mumbai', 'hear', 'cannt', 'listen']\n",
            "After lemmatization: ['call', 'later', 'afternoon', 'onwords', 'mtnl', 'service', 'get', 'problem', 'south', 'mumbai', 'hear', 'cannt', 'listen']\n",
            "After lowercasing: 83039 62735=å£450 uk break accommodationvouchers terms & conditions apply. 2 claim you mustprovide your claim number which is 15541 \n",
            "After removing special chars:   uk break accommodationvouchers terms  conditions apply  claim you mustprovide your claim number which is  \n",
            "After tokenization: ['uk', 'break', 'accommodationvouchers', 'terms', 'conditions', 'apply', 'claim', 'you', 'mustprovide', 'your', 'claim', 'number', 'which', 'is']\n",
            "After stop word removal: ['uk', 'break', 'accommodationvouchers', 'terms', 'conditions', 'apply', 'claim', 'mustprovide', 'claim', 'number']\n",
            "After lemmatization: ['uk', 'break', 'accommodationvouchers', 'term', 'condition', 'apply', 'claim', 'mustprovide', 'claim', 'number']\n",
            "After lowercasing: talk to g and x about that\n",
            "After removing special chars: talk to g and x about that\n",
            "After tokenization: ['talk', 'to', 'g', 'and', 'x', 'about', 'that']\n",
            "After stop word removal: ['talk', 'g', 'x']\n",
            "After lemmatization: ['talk', 'g', 'x']\n",
            "After lowercasing: hai dear friends... this is my new &amp; present number..:) by rajitha raj (ranju)\n",
            "After removing special chars: hai dear friends this is my new amp present number by rajitha raj ranju\n",
            "After tokenization: ['hai', 'dear', 'friends', 'this', 'is', 'my', 'new', 'amp', 'present', 'number', 'by', 'rajitha', 'raj', 'ranju']\n",
            "After stop word removal: ['hai', 'dear', 'friends', 'new', 'amp', 'present', 'number', 'rajitha', 'raj', 'ranju']\n",
            "After lemmatization: ['hai', 'dear', 'friend', 'new', 'amp', 'present', 'number', 'rajitha', 'raj', 'ranju']\n",
            "After lowercasing: 5p 4 alfie moon's children in need song on ur mob. tell ur m8s. txt tone charity to 8007 for nokias or poly charity for polys: zed 08701417012 profit 2 charity.\n",
            "After removing special chars: p  alfie moons children in need song on ur mob tell ur ms txt tone charity to  for nokias or poly charity for polys zed  profit  charity\n",
            "After tokenization: ['p', 'alfie', 'moons', 'children', 'in', 'need', 'song', 'on', 'ur', 'mob', 'tell', 'ur', 'ms', 'txt', 'tone', 'charity', 'to', 'for', 'nokias', 'or', 'poly', 'charity', 'for', 'polys', 'zed', 'profit', 'charity']\n",
            "After stop word removal: ['p', 'alfie', 'moons', 'children', 'need', 'song', 'ur', 'mob', 'tell', 'ur', 'ms', 'txt', 'tone', 'charity', 'nokias', 'poly', 'charity', 'polys', 'zed', 'profit', 'charity']\n",
            "After lemmatization: ['p', 'alfie', 'moon', 'child', 'need', 'song', 'ur', 'mob', 'tell', 'ur', 'm', 'txt', 'tone', 'charity', 'nokias', 'poly', 'charity', 'polys', 'zed', 'profit', 'charity']\n",
            "After lowercasing: as in different styles?\n",
            "After removing special chars: as in different styles\n",
            "After tokenization: ['as', 'in', 'different', 'styles']\n",
            "After stop word removal: ['different', 'styles']\n",
            "After lemmatization: ['different', 'style']\n",
            "After lowercasing: win a å£200 shopping spree every week starting now. 2 play text store to 88039. skilgme. tscs08714740323 1winawk! age16 å£1.50perweeksub.\n",
            "After removing special chars: win a  shopping spree every week starting now  play text store to  skilgme tscs winawk age perweeksub\n",
            "After tokenization: ['win', 'a', 'shopping', 'spree', 'every', 'week', 'starting', 'now', 'play', 'text', 'store', 'to', 'skilgme', 'tscs', 'winawk', 'age', 'perweeksub']\n",
            "After stop word removal: ['win', 'shopping', 'spree', 'every', 'week', 'starting', 'play', 'text', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perweeksub']\n",
            "After lemmatization: ['win', 'shopping', 'spree', 'every', 'week', 'starting', 'play', 'text', 'store', 'skilgme', 'tscs', 'winawk', 'age', 'perweeksub']\n",
            "After lowercasing: gud ni8 dear..slp well..take care..swt dreams..muah..\n",
            "After removing special chars: gud ni dearslp welltake careswt dreamsmuah\n",
            "After tokenization: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After stop word removal: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After lemmatization: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After lowercasing: i want to sent  &lt;#&gt; mesages today. thats y. sorry if i hurts\n",
            "After removing special chars: i want to sent  ltgt mesages today thats y sorry if i hurts\n",
            "After tokenization: ['i', 'want', 'to', 'sent', 'ltgt', 'mesages', 'today', 'thats', 'y', 'sorry', 'if', 'i', 'hurts']\n",
            "After stop word removal: ['want', 'sent', 'ltgt', 'mesages', 'today', 'thats', 'sorry', 'hurts']\n",
            "After lemmatization: ['want', 'sent', 'ltgt', 'mesages', 'today', 'thats', 'sorry', 'hurt']\n",
            "After lowercasing: this is the 2nd attempt to contract u, you have won this weeks top prize of either å£1000 cash or å£200 prize. just call 09066361921\n",
            "After removing special chars: this is the nd attempt to contract u you have won this weeks top prize of either  cash or  prize just call \n",
            "After tokenization: ['this', 'is', 'the', 'nd', 'attempt', 'to', 'contract', 'u', 'you', 'have', 'won', 'this', 'weeks', 'top', 'prize', 'of', 'either', 'cash', 'or', 'prize', 'just', 'call']\n",
            "After stop word removal: ['nd', 'attempt', 'contract', 'u', 'weeks', 'top', 'prize', 'either', 'cash', 'prize', 'call']\n",
            "After lemmatization: ['nd', 'attempt', 'contract', 'u', 'week', 'top', 'prize', 'either', 'cash', 'prize', 'call']\n",
            "After lowercasing: well, i'm glad you didn't find it totally disagreeable ... lol\n",
            "After removing special chars: well im glad you didnt find it totally disagreeable  lol\n",
            "After tokenization: ['well', 'im', 'glad', 'you', 'didnt', 'find', 'it', 'totally', 'disagreeable', 'lol']\n",
            "After stop word removal: ['well', 'im', 'glad', 'didnt', 'find', 'totally', 'disagreeable', 'lol']\n",
            "After lemmatization: ['well', 'im', 'glad', 'didnt', 'find', 'totally', 'disagreeable', 'lol']\n",
            "After lowercasing: guy, no flash me now. if you go call me, call me. how madam. take care oh.\n",
            "After removing special chars: guy no flash me now if you go call me call me how madam take care oh\n",
            "After tokenization: ['guy', 'no', 'flash', 'me', 'now', 'if', 'you', 'go', 'call', 'me', 'call', 'me', 'how', 'madam', 'take', 'care', 'oh']\n",
            "After stop word removal: ['guy', 'flash', 'go', 'call', 'call', 'madam', 'take', 'care', 'oh']\n",
            "After lemmatization: ['guy', 'flash', 'go', 'call', 'call', 'madam', 'take', 'care', 'oh']\n",
            "After lowercasing: do you want a new nokia 3510i colour phone deliveredtomorrow? with 300 free minutes to any mobile + 100 free texts + free camcorder reply or call 08000930705.\n",
            "After removing special chars: do you want a new nokia i colour phone deliveredtomorrow with  free minutes to any mobile   free texts  free camcorder reply or call \n",
            "After tokenization: ['do', 'you', 'want', 'a', 'new', 'nokia', 'i', 'colour', 'phone', 'deliveredtomorrow', 'with', 'free', 'minutes', 'to', 'any', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'or', 'call']\n",
            "After stop word removal: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minutes', 'mobile', 'free', 'texts', 'free', 'camcorder', 'reply', 'call']\n",
            "After lemmatization: ['want', 'new', 'nokia', 'colour', 'phone', 'deliveredtomorrow', 'free', 'minute', 'mobile', 'free', 'text', 'free', 'camcorder', 'reply', 'call']\n",
            "After lowercasing: mark works tomorrow. he gets out at 5. his work is by your house so he can meet u afterwards.\n",
            "After removing special chars: mark works tomorrow he gets out at  his work is by your house so he can meet u afterwards\n",
            "After tokenization: ['mark', 'works', 'tomorrow', 'he', 'gets', 'out', 'at', 'his', 'work', 'is', 'by', 'your', 'house', 'so', 'he', 'can', 'meet', 'u', 'afterwards']\n",
            "After stop word removal: ['mark', 'works', 'tomorrow', 'gets', 'work', 'house', 'meet', 'u', 'afterwards']\n",
            "After lemmatization: ['mark', 'work', 'tomorrow', 'get', 'work', 'house', 'meet', 'u', 'afterwards']\n",
            "After lowercasing: \\keep ur problems in ur heart\n",
            "After removing special chars: keep ur problems in ur heart\n",
            "After tokenization: ['keep', 'ur', 'problems', 'in', 'ur', 'heart']\n",
            "After stop word removal: ['keep', 'ur', 'problems', 'ur', 'heart']\n",
            "After lemmatization: ['keep', 'ur', 'problem', 'ur', 'heart']\n",
            "After lowercasing: yeah, give me a call if you've got a minute\n",
            "After removing special chars: yeah give me a call if youve got a minute\n",
            "After tokenization: ['yeah', 'give', 'me', 'a', 'call', 'if', 'youve', 'got', 'a', 'minute']\n",
            "After stop word removal: ['yeah', 'give', 'call', 'youve', 'got', 'minute']\n",
            "After lemmatization: ['yeah', 'give', 'call', 'youve', 'got', 'minute']\n",
            "After lowercasing: \\hi babe uawake?feellikw shit.justfound out via aletter thatmum gotmarried 4thnov.behind ourbacks åð fuckinnice!selfish\n",
            "After removing special chars: hi babe uawakefeellikw shitjustfound out via aletter thatmum gotmarried thnovbehind ourbacks  fuckinniceselfish\n",
            "After tokenization: ['hi', 'babe', 'uawakefeellikw', 'shitjustfound', 'out', 'via', 'aletter', 'thatmum', 'gotmarried', 'thnovbehind', 'ourbacks', 'fuckinniceselfish']\n",
            "After stop word removal: ['hi', 'babe', 'uawakefeellikw', 'shitjustfound', 'via', 'aletter', 'thatmum', 'gotmarried', 'thnovbehind', 'ourbacks', 'fuckinniceselfish']\n",
            "After lemmatization: ['hi', 'babe', 'uawakefeellikw', 'shitjustfound', 'via', 'aletter', 'thatmum', 'gotmarried', 'thnovbehind', 'ourbacks', 'fuckinniceselfish']\n",
            "After lowercasing: amazing : if you rearrange these letters it gives the same meaning... dormitory = dirty room astronomer = moon starer the eyes = they see election results = lies lets recount mother-in-law = woman hitler eleven plus two =twelve plus one its amazing... !:-)\n",
            "After removing special chars: amazing  if you rearrange these letters it gives the same meaning dormitory  dirty room astronomer  moon starer the eyes  they see election results  lies lets recount motherinlaw  woman hitler eleven plus two twelve plus one its amazing \n",
            "After tokenization: ['amazing', 'if', 'you', 'rearrange', 'these', 'letters', 'it', 'gives', 'the', 'same', 'meaning', 'dormitory', 'dirty', 'room', 'astronomer', 'moon', 'starer', 'the', 'eyes', 'they', 'see', 'election', 'results', 'lies', 'lets', 'recount', 'motherinlaw', 'woman', 'hitler', 'eleven', 'plus', 'two', 'twelve', 'plus', 'one', 'its', 'amazing']\n",
            "After stop word removal: ['amazing', 'rearrange', 'letters', 'gives', 'meaning', 'dormitory', 'dirty', 'room', 'astronomer', 'moon', 'starer', 'eyes', 'see', 'election', 'results', 'lies', 'lets', 'recount', 'motherinlaw', 'woman', 'hitler', 'eleven', 'plus', 'two', 'twelve', 'plus', 'one', 'amazing']\n",
            "After lemmatization: ['amazing', 'rearrange', 'letter', 'give', 'meaning', 'dormitory', 'dirty', 'room', 'astronomer', 'moon', 'starer', 'eye', 'see', 'election', 'result', 'lie', 'let', 'recount', 'motherinlaw', 'woman', 'hitler', 'eleven', 'plus', 'two', 'twelve', 'plus', 'one', 'amazing']\n",
            "After lowercasing: aiya we discuss later lar... pick ì_ up at 4 is it?\n",
            "After removing special chars: aiya we discuss later lar pick  up at  is it\n",
            "After tokenization: ['aiya', 'we', 'discuss', 'later', 'lar', 'pick', 'up', 'at', 'is', 'it']\n",
            "After stop word removal: ['aiya', 'discuss', 'later', 'lar', 'pick']\n",
            "After lemmatization: ['aiya', 'discus', 'later', 'lar', 'pick']\n",
            "After lowercasing: hey happy birthday...\n",
            "After removing special chars: hey happy birthday\n",
            "After tokenization: ['hey', 'happy', 'birthday']\n",
            "After stop word removal: ['hey', 'happy', 'birthday']\n",
            "After lemmatization: ['hey', 'happy', 'birthday']\n",
            "After lowercasing: sorry i missed your call. can you please call back.\n",
            "After removing special chars: sorry i missed your call can you please call back\n",
            "After tokenization: ['sorry', 'i', 'missed', 'your', 'call', 'can', 'you', 'please', 'call', 'back']\n",
            "After stop word removal: ['sorry', 'missed', 'call', 'please', 'call', 'back']\n",
            "After lemmatization: ['sorry', 'missed', 'call', 'please', 'call', 'back']\n",
            "After lowercasing: omg if its not one thing its another. my cat has worms :/ when does this bad day end?\n",
            "After removing special chars: omg if its not one thing its another my cat has worms  when does this bad day end\n",
            "After tokenization: ['omg', 'if', 'its', 'not', 'one', 'thing', 'its', 'another', 'my', 'cat', 'has', 'worms', 'when', 'does', 'this', 'bad', 'day', 'end']\n",
            "After stop word removal: ['omg', 'one', 'thing', 'another', 'cat', 'worms', 'bad', 'day', 'end']\n",
            "After lemmatization: ['omg', 'one', 'thing', 'another', 'cat', 'worm', 'bad', 'day', 'end']\n",
            "After lowercasing: good morning, im suffering from fever and dysentry ..will not be able to come to office today.\n",
            "After removing special chars: good morning im suffering from fever and dysentry will not be able to come to office today\n",
            "After tokenization: ['good', 'morning', 'im', 'suffering', 'from', 'fever', 'and', 'dysentry', 'will', 'not', 'be', 'able', 'to', 'come', 'to', 'office', 'today']\n",
            "After stop word removal: ['good', 'morning', 'im', 'suffering', 'fever', 'dysentry', 'able', 'come', 'office', 'today']\n",
            "After lemmatization: ['good', 'morning', 'im', 'suffering', 'fever', 'dysentry', 'able', 'come', 'office', 'today']\n",
            "After lowercasing: i wont do anything de.\n",
            "After removing special chars: i wont do anything de\n",
            "After tokenization: ['i', 'wont', 'do', 'anything', 'de']\n",
            "After stop word removal: ['wont', 'anything', 'de']\n",
            "After lemmatization: ['wont', 'anything', 'de']\n",
            "After lowercasing: what type of stuff do you sing?\n",
            "After removing special chars: what type of stuff do you sing\n",
            "After tokenization: ['what', 'type', 'of', 'stuff', 'do', 'you', 'sing']\n",
            "After stop word removal: ['type', 'stuff', 'sing']\n",
            "After lemmatization: ['type', 'stuff', 'sing']\n",
            "After lowercasing: st andre, virgil's cream\n",
            "After removing special chars: st andre virgils cream\n",
            "After tokenization: ['st', 'andre', 'virgils', 'cream']\n",
            "After stop word removal: ['st', 'andre', 'virgils', 'cream']\n",
            "After lemmatization: ['st', 'andre', 'virgil', 'cream']\n",
            "After lowercasing: no no. i will check all rooms befor activities\n",
            "After removing special chars: no no i will check all rooms befor activities\n",
            "After tokenization: ['no', 'no', 'i', 'will', 'check', 'all', 'rooms', 'befor', 'activities']\n",
            "After stop word removal: ['check', 'rooms', 'befor', 'activities']\n",
            "After lemmatization: ['check', 'room', 'befor', 'activity']\n",
            "After lowercasing: my fri ah... okie lor,goin 4 my drivin den go shoppin after tt...\n",
            "After removing special chars: my fri ah okie lorgoin  my drivin den go shoppin after tt\n",
            "After tokenization: ['my', 'fri', 'ah', 'okie', 'lorgoin', 'my', 'drivin', 'den', 'go', 'shoppin', 'after', 'tt']\n",
            "After stop word removal: ['fri', 'ah', 'okie', 'lorgoin', 'drivin', 'den', 'go', 'shoppin', 'tt']\n",
            "After lemmatization: ['fri', 'ah', 'okie', 'lorgoin', 'drivin', 'den', 'go', 'shoppin', 'tt']\n",
            "After lowercasing: gokila is talking with you aha:)\n",
            "After removing special chars: gokila is talking with you aha\n",
            "After tokenization: ['gokila', 'is', 'talking', 'with', 'you', 'aha']\n",
            "After stop word removal: ['gokila', 'talking', 'aha']\n",
            "After lemmatization: ['gokila', 'talking', 'aha']\n",
            "After lowercasing: hi shanil,rakhesh here.thanks,i have exchanged the uncut diamond stuff.leaving back. excellent service by dino and prem.\n",
            "After removing special chars: hi shanilrakhesh herethanksi have exchanged the uncut diamond stuffleaving back excellent service by dino and prem\n",
            "After tokenization: ['hi', 'shanilrakhesh', 'herethanksi', 'have', 'exchanged', 'the', 'uncut', 'diamond', 'stuffleaving', 'back', 'excellent', 'service', 'by', 'dino', 'and', 'prem']\n",
            "After stop word removal: ['hi', 'shanilrakhesh', 'herethanksi', 'exchanged', 'uncut', 'diamond', 'stuffleaving', 'back', 'excellent', 'service', 'dino', 'prem']\n",
            "After lemmatization: ['hi', 'shanilrakhesh', 'herethanksi', 'exchanged', 'uncut', 'diamond', 'stuffleaving', 'back', 'excellent', 'service', 'dino', 'prem']\n",
            "After lowercasing: k.k.this month kotees birthday know?\n",
            "After removing special chars: kkthis month kotees birthday know\n",
            "After tokenization: ['kkthis', 'month', 'kotees', 'birthday', 'know']\n",
            "After stop word removal: ['kkthis', 'month', 'kotees', 'birthday', 'know']\n",
            "After lemmatization: ['kkthis', 'month', 'kotees', 'birthday', 'know']\n",
            "After lowercasing: but i'm really really broke oh. no amount is too small even  &lt;#&gt; \n",
            "After removing special chars: but im really really broke oh no amount is too small even  ltgt \n",
            "After tokenization: ['but', 'im', 'really', 'really', 'broke', 'oh', 'no', 'amount', 'is', 'too', 'small', 'even', 'ltgt']\n",
            "After stop word removal: ['im', 'really', 'really', 'broke', 'oh', 'amount', 'small', 'even', 'ltgt']\n",
            "After lemmatization: ['im', 'really', 'really', 'broke', 'oh', 'amount', 'small', 'even', 'ltgt']\n",
            "After lowercasing: sorry about that this is my mates phone and i didnt write it love kate\n",
            "After removing special chars: sorry about that this is my mates phone and i didnt write it love kate\n",
            "After tokenization: ['sorry', 'about', 'that', 'this', 'is', 'my', 'mates', 'phone', 'and', 'i', 'didnt', 'write', 'it', 'love', 'kate']\n",
            "After stop word removal: ['sorry', 'mates', 'phone', 'didnt', 'write', 'love', 'kate']\n",
            "After lemmatization: ['sorry', 'mate', 'phone', 'didnt', 'write', 'love', 'kate']\n",
            "After lowercasing: themob>hit the link to get a premium pink panther game, the new no. 1 from sugababes, a crazy zebra animation or a badass hoody wallpaper-all 4 free!\n",
            "After removing special chars: themobhit the link to get a premium pink panther game the new no  from sugababes a crazy zebra animation or a badass hoody wallpaperall  free\n",
            "After tokenization: ['themobhit', 'the', 'link', 'to', 'get', 'a', 'premium', 'pink', 'panther', 'game', 'the', 'new', 'no', 'from', 'sugababes', 'a', 'crazy', 'zebra', 'animation', 'or', 'a', 'badass', 'hoody', 'wallpaperall', 'free']\n",
            "After stop word removal: ['themobhit', 'link', 'get', 'premium', 'pink', 'panther', 'game', 'new', 'sugababes', 'crazy', 'zebra', 'animation', 'badass', 'hoody', 'wallpaperall', 'free']\n",
            "After lemmatization: ['themobhit', 'link', 'get', 'premium', 'pink', 'panther', 'game', 'new', 'sugababes', 'crazy', 'zebra', 'animation', 'badass', 'hoody', 'wallpaperall', 'free']\n",
            "After lowercasing: ah, well that confuses things, doesnt it? i thought was friends with now. maybe i did the wrong thing but i already sort of invited -tho he may not come cos of money.\n",
            "After removing special chars: ah well that confuses things doesnt it i thought was friends with now maybe i did the wrong thing but i already sort of invited tho he may not come cos of money\n",
            "After tokenization: ['ah', 'well', 'that', 'confuses', 'things', 'doesnt', 'it', 'i', 'thought', 'was', 'friends', 'with', 'now', 'maybe', 'i', 'did', 'the', 'wrong', 'thing', 'but', 'i', 'already', 'sort', 'of', 'invited', 'tho', 'he', 'may', 'not', 'come', 'cos', 'of', 'money']\n",
            "After stop word removal: ['ah', 'well', 'confuses', 'things', 'doesnt', 'thought', 'friends', 'maybe', 'wrong', 'thing', 'already', 'sort', 'invited', 'tho', 'may', 'come', 'cos', 'money']\n",
            "After lemmatization: ['ah', 'well', 'confuses', 'thing', 'doesnt', 'thought', 'friend', 'maybe', 'wrong', 'thing', 'already', 'sort', 'invited', 'tho', 'may', 'come', 'co', 'money']\n",
            "After lowercasing: aight, call me once you're close\n",
            "After removing special chars: aight call me once youre close\n",
            "After tokenization: ['aight', 'call', 'me', 'once', 'youre', 'close']\n",
            "After stop word removal: ['aight', 'call', 'youre', 'close']\n",
            "After lemmatization: ['aight', 'call', 'youre', 'close']\n",
            "After lowercasing: nope thats fine. i might have a nap tho! \n",
            "After removing special chars: nope thats fine i might have a nap tho \n",
            "After tokenization: ['nope', 'thats', 'fine', 'i', 'might', 'have', 'a', 'nap', 'tho']\n",
            "After stop word removal: ['nope', 'thats', 'fine', 'might', 'nap', 'tho']\n",
            "After lemmatization: ['nope', 'thats', 'fine', 'might', 'nap', 'tho']\n",
            "After lowercasing: this msg is for your mobile content order it has been resent as previous attempt failed due to network error queries to customersqueries@netvision.uk.com\n",
            "After removing special chars: this msg is for your mobile content order it has been resent as previous attempt failed due to network error queries to customersqueriesnetvisionukcom\n",
            "After tokenization: ['this', 'msg', 'is', 'for', 'your', 'mobile', 'content', 'order', 'it', 'has', 'been', 'resent', 'as', 'previous', 'attempt', 'failed', 'due', 'to', 'network', 'error', 'queries', 'to', 'customersqueriesnetvisionukcom']\n",
            "After stop word removal: ['msg', 'mobile', 'content', 'order', 'resent', 'previous', 'attempt', 'failed', 'due', 'network', 'error', 'queries', 'customersqueriesnetvisionukcom']\n",
            "After lemmatization: ['msg', 'mobile', 'content', 'order', 'resent', 'previous', 'attempt', 'failed', 'due', 'network', 'error', 'query', 'customersqueriesnetvisionukcom']\n",
            "After lowercasing: in other news after hassling me to get him weed for a week andres has no money. haughaighgtujhyguj\n",
            "After removing special chars: in other news after hassling me to get him weed for a week andres has no money haughaighgtujhyguj\n",
            "After tokenization: ['in', 'other', 'news', 'after', 'hassling', 'me', 'to', 'get', 'him', 'weed', 'for', 'a', 'week', 'andres', 'has', 'no', 'money', 'haughaighgtujhyguj']\n",
            "After stop word removal: ['news', 'hassling', 'get', 'weed', 'week', 'andres', 'money', 'haughaighgtujhyguj']\n",
            "After lemmatization: ['news', 'hassling', 'get', 'weed', 'week', 'andres', 'money', 'haughaighgtujhyguj']\n",
            "After lowercasing: a boy loved a gal. he propsd bt she didnt mind. he gv lv lttrs, bt her frnds threw thm. again d boy decided 2 aproach d gal , dt time a truck was speeding towards d gal. wn it was about 2 hit d girl,d boy ran like hell n saved her. she asked 'hw cn u run so fast?' d boy replied \\boost is d secret of my energy\\\" n instantly d girl shouted \\\"our energy\\\" n thy lived happily 2gthr drinking boost evrydy moral of d story:- i hv free msgs:d;): gud ni8\"\n",
            "After removing special chars: a boy loved a gal he propsd bt she didnt mind he gv lv lttrs bt her frnds threw thm again d boy decided  aproach d gal  dt time a truck was speeding towards d gal wn it was about  hit d girld boy ran like hell n saved her she asked hw cn u run so fast d boy replied boost is d secret of my energy n instantly d girl shouted our energy n thy lived happily gthr drinking boost evrydy moral of d story i hv free msgsd gud ni\n",
            "After tokenization: ['a', 'boy', 'loved', 'a', 'gal', 'he', 'propsd', 'bt', 'she', 'didnt', 'mind', 'he', 'gv', 'lv', 'lttrs', 'bt', 'her', 'frnds', 'threw', 'thm', 'again', 'd', 'boy', 'decided', 'aproach', 'd', 'gal', 'dt', 'time', 'a', 'truck', 'was', 'speeding', 'towards', 'd', 'gal', 'wn', 'it', 'was', 'about', 'hit', 'd', 'girld', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'her', 'she', 'asked', 'hw', 'cn', 'u', 'run', 'so', 'fast', 'd', 'boy', 'replied', 'boost', 'is', 'd', 'secret', 'of', 'my', 'energy', 'n', 'instantly', 'd', 'girl', 'shouted', 'our', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'of', 'd', 'story', 'i', 'hv', 'free', 'msgsd', 'gud', 'ni']\n",
            "After stop word removal: ['boy', 'loved', 'gal', 'propsd', 'bt', 'didnt', 'mind', 'gv', 'lv', 'lttrs', 'bt', 'frnds', 'threw', 'thm', 'boy', 'decided', 'aproach', 'gal', 'dt', 'time', 'truck', 'speeding', 'towards', 'gal', 'wn', 'hit', 'girld', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'asked', 'hw', 'cn', 'u', 'run', 'fast', 'boy', 'replied', 'boost', 'secret', 'energy', 'n', 'instantly', 'girl', 'shouted', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'story', 'hv', 'free', 'msgsd', 'gud', 'ni']\n",
            "After lemmatization: ['boy', 'loved', 'gal', 'propsd', 'bt', 'didnt', 'mind', 'gv', 'lv', 'lttrs', 'bt', 'frnds', 'threw', 'thm', 'boy', 'decided', 'aproach', 'gal', 'dt', 'time', 'truck', 'speeding', 'towards', 'gal', 'wn', 'hit', 'girld', 'boy', 'ran', 'like', 'hell', 'n', 'saved', 'asked', 'hw', 'cn', 'u', 'run', 'fast', 'boy', 'replied', 'boost', 'secret', 'energy', 'n', 'instantly', 'girl', 'shouted', 'energy', 'n', 'thy', 'lived', 'happily', 'gthr', 'drinking', 'boost', 'evrydy', 'moral', 'story', 'hv', 'free', 'msgsd', 'gud', 'ni']\n",
            "After lowercasing: i wnt to buy a bmw car urgently..its vry urgent.but hv a shortage of  &lt;#&gt; lacs.there is no source to arng dis amt. &lt;#&gt; lacs..thats my prob\n",
            "After removing special chars: i wnt to buy a bmw car urgentlyits vry urgentbut hv a shortage of  ltgt lacsthere is no source to arng dis amt ltgt lacsthats my prob\n",
            "After tokenization: ['i', 'wnt', 'to', 'buy', 'a', 'bmw', 'car', 'urgentlyits', 'vry', 'urgentbut', 'hv', 'a', 'shortage', 'of', 'ltgt', 'lacsthere', 'is', 'no', 'source', 'to', 'arng', 'dis', 'amt', 'ltgt', 'lacsthats', 'my', 'prob']\n",
            "After stop word removal: ['wnt', 'buy', 'bmw', 'car', 'urgentlyits', 'vry', 'urgentbut', 'hv', 'shortage', 'ltgt', 'lacsthere', 'source', 'arng', 'dis', 'amt', 'ltgt', 'lacsthats', 'prob']\n",
            "After lemmatization: ['wnt', 'buy', 'bmw', 'car', 'urgentlyits', 'vry', 'urgentbut', 'hv', 'shortage', 'ltgt', 'lacsthere', 'source', 'arng', 'dis', 'amt', 'ltgt', 'lacsthats', 'prob']\n",
            "After lowercasing: ding me on ya break fassyole! blacko from londn\n",
            "After removing special chars: ding me on ya break fassyole blacko from londn\n",
            "After tokenization: ['ding', 'me', 'on', 'ya', 'break', 'fassyole', 'blacko', 'from', 'londn']\n",
            "After stop word removal: ['ding', 'ya', 'break', 'fassyole', 'blacko', 'londn']\n",
            "After lemmatization: ['ding', 'ya', 'break', 'fassyole', 'blacko', 'londn']\n",
            "After lowercasing: i really need 2 kiss u i miss u my baby from ur baby 4eva\n",
            "After removing special chars: i really need  kiss u i miss u my baby from ur baby eva\n",
            "After tokenization: ['i', 'really', 'need', 'kiss', 'u', 'i', 'miss', 'u', 'my', 'baby', 'from', 'ur', 'baby', 'eva']\n",
            "After stop word removal: ['really', 'need', 'kiss', 'u', 'miss', 'u', 'baby', 'ur', 'baby', 'eva']\n",
            "After lemmatization: ['really', 'need', 'kiss', 'u', 'miss', 'u', 'baby', 'ur', 'baby', 'eva']\n",
            "After lowercasing: the sign of maturity is not when we start saying big things.. but actually it is, when we start understanding small things... *have a nice evening* bslvyl\n",
            "After removing special chars: the sign of maturity is not when we start saying big things but actually it is when we start understanding small things have a nice evening bslvyl\n",
            "After tokenization: ['the', 'sign', 'of', 'maturity', 'is', 'not', 'when', 'we', 'start', 'saying', 'big', 'things', 'but', 'actually', 'it', 'is', 'when', 'we', 'start', 'understanding', 'small', 'things', 'have', 'a', 'nice', 'evening', 'bslvyl']\n",
            "After stop word removal: ['sign', 'maturity', 'start', 'saying', 'big', 'things', 'actually', 'start', 'understanding', 'small', 'things', 'nice', 'evening', 'bslvyl']\n",
            "After lemmatization: ['sign', 'maturity', 'start', 'saying', 'big', 'thing', 'actually', 'start', 'understanding', 'small', 'thing', 'nice', 'evening', 'bslvyl']\n",
            "After lowercasing: oh you got many responsibilities.\n",
            "After removing special chars: oh you got many responsibilities\n",
            "After tokenization: ['oh', 'you', 'got', 'many', 'responsibilities']\n",
            "After stop word removal: ['oh', 'got', 'many', 'responsibilities']\n",
            "After lemmatization: ['oh', 'got', 'many', 'responsibility']\n",
            "After lowercasing: you have 1 new message. please call 08715205273\n",
            "After removing special chars: you have  new message please call \n",
            "After tokenization: ['you', 'have', 'new', 'message', 'please', 'call']\n",
            "After stop word removal: ['new', 'message', 'please', 'call']\n",
            "After lemmatization: ['new', 'message', 'please', 'call']\n",
            "After lowercasing: i've reached sch already...\n",
            "After removing special chars: ive reached sch already\n",
            "After tokenization: ['ive', 'reached', 'sch', 'already']\n",
            "After stop word removal: ['ive', 'reached', 'sch', 'already']\n",
            "After lemmatization: ['ive', 'reached', 'sch', 'already']\n",
            "After lowercasing: december only! had your mobile 11mths+? you are entitled to update to the latest colour camera mobile for free! call the mobile update vco free on 08002986906 \n",
            "After removing special chars: december only had your mobile mths you are entitled to update to the latest colour camera mobile for free call the mobile update vco free on  \n",
            "After tokenization: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'vco', 'free', 'on']\n",
            "After stop word removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'vco', 'free']\n",
            "After lemmatization: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'vco', 'free']\n",
            "After lowercasing: u definitely need a module from e humanities dis sem izzit? u wan 2 take other modules 1st?\n",
            "After removing special chars: u definitely need a module from e humanities dis sem izzit u wan  take other modules st\n",
            "After tokenization: ['u', 'definitely', 'need', 'a', 'module', 'from', 'e', 'humanities', 'dis', 'sem', 'izzit', 'u', 'wan', 'take', 'other', 'modules', 'st']\n",
            "After stop word removal: ['u', 'definitely', 'need', 'module', 'e', 'humanities', 'dis', 'sem', 'izzit', 'u', 'wan', 'take', 'modules', 'st']\n",
            "After lemmatization: ['u', 'definitely', 'need', 'module', 'e', 'humanity', 'dis', 'sem', 'izzit', 'u', 'wan', 'take', 'module', 'st']\n",
            "After lowercasing: argh why the fuck is nobody in town ;_;\n",
            "After removing special chars: argh why the fuck is nobody in town \n",
            "After tokenization: ['argh', 'why', 'the', 'fuck', 'is', 'nobody', 'in', 'town']\n",
            "After stop word removal: ['argh', 'fuck', 'nobody', 'town']\n",
            "After lemmatization: ['argh', 'fuck', 'nobody', 'town']\n",
            "After lowercasing: get 3 lions england tone, reply lionm 4 mono or lionp 4 poly. 4 more go 2 www.ringtones.co.uk, the original n best. tones 3gbp network operator rates apply.\n",
            "After removing special chars: get  lions england tone reply lionm  mono or lionp  poly  more go  wwwringtonescouk the original n best tones gbp network operator rates apply\n",
            "After tokenization: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'or', 'lionp', 'poly', 'more', 'go', 'wwwringtonescouk', 'the', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
            "After stop word removal: ['get', 'lions', 'england', 'tone', 'reply', 'lionm', 'mono', 'lionp', 'poly', 'go', 'wwwringtonescouk', 'original', 'n', 'best', 'tones', 'gbp', 'network', 'operator', 'rates', 'apply']\n",
            "After lemmatization: ['get', 'lion', 'england', 'tone', 'reply', 'lionm', 'mono', 'lionp', 'poly', 'go', 'wwwringtonescouk', 'original', 'n', 'best', 'tone', 'gbp', 'network', 'operator', 'rate', 'apply']\n",
            "After lowercasing: thanks. fills me with complete calm and reassurance! \n",
            "After removing special chars: thanks fills me with complete calm and reassurance \n",
            "After tokenization: ['thanks', 'fills', 'me', 'with', 'complete', 'calm', 'and', 'reassurance']\n",
            "After stop word removal: ['thanks', 'fills', 'complete', 'calm', 'reassurance']\n",
            "After lemmatization: ['thanks', 'fill', 'complete', 'calm', 'reassurance']\n",
            "After lowercasing: aslamalaikkum....insha allah tohar beeen muht albi mufti mahfuuz...meaning same here....\n",
            "After removing special chars: aslamalaikkuminsha allah tohar beeen muht albi mufti mahfuuzmeaning same here\n",
            "After tokenization: ['aslamalaikkuminsha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuzmeaning', 'same', 'here']\n",
            "After stop word removal: ['aslamalaikkuminsha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuzmeaning']\n",
            "After lemmatization: ['aslamalaikkuminsha', 'allah', 'tohar', 'beeen', 'muht', 'albi', 'mufti', 'mahfuuzmeaning']\n",
            "After lowercasing: are you driving or training?\n",
            "After removing special chars: are you driving or training\n",
            "After tokenization: ['are', 'you', 'driving', 'or', 'training']\n",
            "After stop word removal: ['driving', 'training']\n",
            "After lemmatization: ['driving', 'training']\n",
            "After lowercasing: lol for real. she told my dad i have cancer\n",
            "After removing special chars: lol for real she told my dad i have cancer\n",
            "After tokenization: ['lol', 'for', 'real', 'she', 'told', 'my', 'dad', 'i', 'have', 'cancer']\n",
            "After stop word removal: ['lol', 'real', 'told', 'dad', 'cancer']\n",
            "After lemmatization: ['lol', 'real', 'told', 'dad', 'cancer']\n",
            "After lowercasing: private! your 2003 account statement for 078\n",
            "After removing special chars: private your  account statement for \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for']\n",
            "After stop word removal: ['private', 'account', 'statement']\n",
            "After lemmatization: ['private', 'account', 'statement']\n",
            "After lowercasing: oops i did have it,  &lt;#&gt; ?\n",
            "After removing special chars: oops i did have it  ltgt \n",
            "After tokenization: ['oops', 'i', 'did', 'have', 'it', 'ltgt']\n",
            "After stop word removal: ['oops', 'ltgt']\n",
            "After lemmatization: ['oops', 'ltgt']\n",
            "After lowercasing: \\not enufcredeit tocall.shall ileave uni at 6 +get a bus to yor house?\\\"\"\n",
            "After removing special chars: not enufcredeit tocallshall ileave uni at  get a bus to yor house\n",
            "After tokenization: ['not', 'enufcredeit', 'tocallshall', 'ileave', 'uni', 'at', 'get', 'a', 'bus', 'to', 'yor', 'house']\n",
            "After stop word removal: ['enufcredeit', 'tocallshall', 'ileave', 'uni', 'get', 'bus', 'yor', 'house']\n",
            "After lemmatization: ['enufcredeit', 'tocallshall', 'ileave', 'uni', 'get', 'bus', 'yor', 'house']\n",
            "After lowercasing: hi chikku, send some nice msgs\n",
            "After removing special chars: hi chikku send some nice msgs\n",
            "After tokenization: ['hi', 'chikku', 'send', 'some', 'nice', 'msgs']\n",
            "After stop word removal: ['hi', 'chikku', 'send', 'nice', 'msgs']\n",
            "After lemmatization: ['hi', 'chikku', 'send', 'nice', 'msg']\n",
            "After lowercasing: he is impossible to argue with and he always treats me like his sub, like he never released me ... which he did and i will remind him of that if necessary\n",
            "After removing special chars: he is impossible to argue with and he always treats me like his sub like he never released me  which he did and i will remind him of that if necessary\n",
            "After tokenization: ['he', 'is', 'impossible', 'to', 'argue', 'with', 'and', 'he', 'always', 'treats', 'me', 'like', 'his', 'sub', 'like', 'he', 'never', 'released', 'me', 'which', 'he', 'did', 'and', 'i', 'will', 'remind', 'him', 'of', 'that', 'if', 'necessary']\n",
            "After stop word removal: ['impossible', 'argue', 'always', 'treats', 'like', 'sub', 'like', 'never', 'released', 'remind', 'necessary']\n",
            "After lemmatization: ['impossible', 'argue', 'always', 'treat', 'like', 'sub', 'like', 'never', 'released', 'remind', 'necessary']\n",
            "After lowercasing: after my work ah... den 6 plus lor... u workin oso rite... den go orchard lor, no other place to go liao...\n",
            "After removing special chars: after my work ah den  plus lor u workin oso rite den go orchard lor no other place to go liao\n",
            "After tokenization: ['after', 'my', 'work', 'ah', 'den', 'plus', 'lor', 'u', 'workin', 'oso', 'rite', 'den', 'go', 'orchard', 'lor', 'no', 'other', 'place', 'to', 'go', 'liao']\n",
            "After stop word removal: ['work', 'ah', 'den', 'plus', 'lor', 'u', 'workin', 'oso', 'rite', 'den', 'go', 'orchard', 'lor', 'place', 'go', 'liao']\n",
            "After lemmatization: ['work', 'ah', 'den', 'plus', 'lor', 'u', 'workin', 'oso', 'rite', 'den', 'go', 'orchard', 'lor', 'place', 'go', 'liao']\n",
            "After lowercasing: to the wonderful okors, have a great month. we cherish you guys and wish you well each day. mojibiola\n",
            "After removing special chars: to the wonderful okors have a great month we cherish you guys and wish you well each day mojibiola\n",
            "After tokenization: ['to', 'the', 'wonderful', 'okors', 'have', 'a', 'great', 'month', 'we', 'cherish', 'you', 'guys', 'and', 'wish', 'you', 'well', 'each', 'day', 'mojibiola']\n",
            "After stop word removal: ['wonderful', 'okors', 'great', 'month', 'cherish', 'guys', 'wish', 'well', 'day', 'mojibiola']\n",
            "After lemmatization: ['wonderful', 'okors', 'great', 'month', 'cherish', 'guy', 'wish', 'well', 'day', 'mojibiola']\n",
            "After lowercasing: cuz ibored. and don wanna study\n",
            "After removing special chars: cuz ibored and don wanna study\n",
            "After tokenization: ['cuz', 'ibored', 'and', 'don', 'wan', 'na', 'study']\n",
            "After stop word removal: ['cuz', 'ibored', 'wan', 'na', 'study']\n",
            "After lemmatization: ['cuz', 'ibored', 'wan', 'na', 'study']\n",
            "After lowercasing: wot about on wed nite i am 3 then but only til 9!\n",
            "After removing special chars: wot about on wed nite i am  then but only til \n",
            "After tokenization: ['wot', 'about', 'on', 'wed', 'nite', 'i', 'am', 'then', 'but', 'only', 'til']\n",
            "After stop word removal: ['wot', 'wed', 'nite', 'til']\n",
            "After lemmatization: ['wot', 'wed', 'nite', 'til']\n",
            "After lowercasing: rose for red,red for blood,blood for heart,heart for u. but u for me.... send tis to all ur friends.. including me.. if u like me.. if u get back, 1-u r poor in relation! 2-u need some 1 to support 3-u r frnd 2 many 4-some1 luvs u 5+- some1 is praying god to marry u.:-) try it....\n",
            "After removing special chars: rose for redred for bloodblood for heartheart for u but u for me send tis to all ur friends including me if u like me if u get back u r poor in relation u need some  to support u r frnd  many some luvs u  some is praying god to marry u try it\n",
            "After tokenization: ['rose', 'for', 'redred', 'for', 'bloodblood', 'for', 'heartheart', 'for', 'u', 'but', 'u', 'for', 'me', 'send', 'tis', 'to', 'all', 'ur', 'friends', 'including', 'me', 'if', 'u', 'like', 'me', 'if', 'u', 'get', 'back', 'u', 'r', 'poor', 'in', 'relation', 'u', 'need', 'some', 'to', 'support', 'u', 'r', 'frnd', 'many', 'some', 'luvs', 'u', 'some', 'is', 'praying', 'god', 'to', 'marry', 'u', 'try', 'it']\n",
            "After stop word removal: ['rose', 'redred', 'bloodblood', 'heartheart', 'u', 'u', 'send', 'tis', 'ur', 'friends', 'including', 'u', 'like', 'u', 'get', 'back', 'u', 'r', 'poor', 'relation', 'u', 'need', 'support', 'u', 'r', 'frnd', 'many', 'luvs', 'u', 'praying', 'god', 'marry', 'u', 'try']\n",
            "After lemmatization: ['rose', 'redred', 'bloodblood', 'heartheart', 'u', 'u', 'send', 'ti', 'ur', 'friend', 'including', 'u', 'like', 'u', 'get', 'back', 'u', 'r', 'poor', 'relation', 'u', 'need', 'support', 'u', 'r', 'frnd', 'many', 'luvs', 'u', 'praying', 'god', 'marry', 'u', 'try']\n",
            "After lowercasing: any way where are you and what doing.\n",
            "After removing special chars: any way where are you and what doing\n",
            "After tokenization: ['any', 'way', 'where', 'are', 'you', 'and', 'what', 'doing']\n",
            "After stop word removal: ['way']\n",
            "After lemmatization: ['way']\n",
            "After lowercasing: that sucks. i'll go over so u can do my hair. you'll do it free right?\n",
            "After removing special chars: that sucks ill go over so u can do my hair youll do it free right\n",
            "After tokenization: ['that', 'sucks', 'ill', 'go', 'over', 'so', 'u', 'can', 'do', 'my', 'hair', 'youll', 'do', 'it', 'free', 'right']\n",
            "After stop word removal: ['sucks', 'ill', 'go', 'u', 'hair', 'youll', 'free', 'right']\n",
            "After lemmatization: ['suck', 'ill', 'go', 'u', 'hair', 'youll', 'free', 'right']\n",
            "After lowercasing: it's still not working. and this time i also tried adding zeros. that was the savings. the checking is  &lt;#&gt; \n",
            "After removing special chars: its still not working and this time i also tried adding zeros that was the savings the checking is  ltgt \n",
            "After tokenization: ['its', 'still', 'not', 'working', 'and', 'this', 'time', 'i', 'also', 'tried', 'adding', 'zeros', 'that', 'was', 'the', 'savings', 'the', 'checking', 'is', 'ltgt']\n",
            "After stop word removal: ['still', 'working', 'time', 'also', 'tried', 'adding', 'zeros', 'savings', 'checking', 'ltgt']\n",
            "After lemmatization: ['still', 'working', 'time', 'also', 'tried', 'adding', 'zero', 'saving', 'checking', 'ltgt']\n",
            "After lowercasing: hmm... dunno leh, mayb a bag 4 goigng out dat is not too small. or jus anything except perfume, smth dat i can keep.\n",
            "After removing special chars: hmm dunno leh mayb a bag  goigng out dat is not too small or jus anything except perfume smth dat i can keep\n",
            "After tokenization: ['hmm', 'dunno', 'leh', 'mayb', 'a', 'bag', 'goigng', 'out', 'dat', 'is', 'not', 'too', 'small', 'or', 'jus', 'anything', 'except', 'perfume', 'smth', 'dat', 'i', 'can', 'keep']\n",
            "After stop word removal: ['hmm', 'dunno', 'leh', 'mayb', 'bag', 'goigng', 'dat', 'small', 'jus', 'anything', 'except', 'perfume', 'smth', 'dat', 'keep']\n",
            "After lemmatization: ['hmm', 'dunno', 'leh', 'mayb', 'bag', 'goigng', 'dat', 'small', 'jus', 'anything', 'except', 'perfume', 'smth', 'dat', 'keep']\n",
            "After lowercasing: sday only joined.so training we started today:)\n",
            "After removing special chars: sday only joinedso training we started today\n",
            "After tokenization: ['sday', 'only', 'joinedso', 'training', 'we', 'started', 'today']\n",
            "After stop word removal: ['sday', 'joinedso', 'training', 'started', 'today']\n",
            "After lemmatization: ['sday', 'joinedso', 'training', 'started', 'today']\n",
            "After lowercasing: sorry * was at the grocers.\n",
            "After removing special chars: sorry  was at the grocers\n",
            "After tokenization: ['sorry', 'was', 'at', 'the', 'grocers']\n",
            "After stop word removal: ['sorry', 'grocers']\n",
            "After lemmatization: ['sorry', 'grocer']\n",
            "After lowercasing: there are some nice pubs near here or there is frankie n bennys near the warner cinema?\n",
            "After removing special chars: there are some nice pubs near here or there is frankie n bennys near the warner cinema\n",
            "After tokenization: ['there', 'are', 'some', 'nice', 'pubs', 'near', 'here', 'or', 'there', 'is', 'frankie', 'n', 'bennys', 'near', 'the', 'warner', 'cinema']\n",
            "After stop word removal: ['nice', 'pubs', 'near', 'frankie', 'n', 'bennys', 'near', 'warner', 'cinema']\n",
            "After lemmatization: ['nice', 'pub', 'near', 'frankie', 'n', 'benny', 'near', 'warner', 'cinema']\n",
            "After lowercasing: you ve won! your 4* costa del sol holiday or å£5000 await collection. call 09050090044 now toclaim. sae, tc s, pobox334, stockport, sk38xh, costå£1.50/pm, max10mins\n",
            "After removing special chars: you ve won your  costa del sol holiday or  await collection call  now toclaim sae tc s pobox stockport skxh costpm maxmins\n",
            "After tokenization: ['you', 've', 'won', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After stop word removal: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After lemmatization: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After lowercasing: yup... i havent been there before... you want to go for the yoga? i can call up to book \n",
            "After removing special chars: yup i havent been there before you want to go for the yoga i can call up to book \n",
            "After tokenization: ['yup', 'i', 'havent', 'been', 'there', 'before', 'you', 'want', 'to', 'go', 'for', 'the', 'yoga', 'i', 'can', 'call', 'up', 'to', 'book']\n",
            "After stop word removal: ['yup', 'havent', 'want', 'go', 'yoga', 'call', 'book']\n",
            "After lemmatization: ['yup', 'havent', 'want', 'go', 'yoga', 'call', 'book']\n",
            "After lowercasing: oh shut it. omg yesterday i had a dream that i had 2 kids both boys. i was so pissed. not only about the kids but them being boys. i even told mark in my dream that he was changing diapers cause i'm not getting owed in the face.\n",
            "After removing special chars: oh shut it omg yesterday i had a dream that i had  kids both boys i was so pissed not only about the kids but them being boys i even told mark in my dream that he was changing diapers cause im not getting owed in the face\n",
            "After tokenization: ['oh', 'shut', 'it', 'omg', 'yesterday', 'i', 'had', 'a', 'dream', 'that', 'i', 'had', 'kids', 'both', 'boys', 'i', 'was', 'so', 'pissed', 'not', 'only', 'about', 'the', 'kids', 'but', 'them', 'being', 'boys', 'i', 'even', 'told', 'mark', 'in', 'my', 'dream', 'that', 'he', 'was', 'changing', 'diapers', 'cause', 'im', 'not', 'getting', 'owed', 'in', 'the', 'face']\n",
            "After stop word removal: ['oh', 'shut', 'omg', 'yesterday', 'dream', 'kids', 'boys', 'pissed', 'kids', 'boys', 'even', 'told', 'mark', 'dream', 'changing', 'diapers', 'cause', 'im', 'getting', 'owed', 'face']\n",
            "After lemmatization: ['oh', 'shut', 'omg', 'yesterday', 'dream', 'kid', 'boy', 'pissed', 'kid', 'boy', 'even', 'told', 'mark', 'dream', 'changing', 'diaper', 'cause', 'im', 'getting', 'owed', 'face']\n",
            "After lowercasing: yeah i imagine he would be really gentle. unlike the other docs who treat their patients like turkeys.\n",
            "After removing special chars: yeah i imagine he would be really gentle unlike the other docs who treat their patients like turkeys\n",
            "After tokenization: ['yeah', 'i', 'imagine', 'he', 'would', 'be', 'really', 'gentle', 'unlike', 'the', 'other', 'docs', 'who', 'treat', 'their', 'patients', 'like', 'turkeys']\n",
            "After stop word removal: ['yeah', 'imagine', 'would', 'really', 'gentle', 'unlike', 'docs', 'treat', 'patients', 'like', 'turkeys']\n",
            "After lemmatization: ['yeah', 'imagine', 'would', 'really', 'gentle', 'unlike', 'doc', 'treat', 'patient', 'like', 'turkey']\n",
            "After lowercasing: free for 1st week! no1 nokia tone 4 ur mobile every week just txt nokia to 8077 get txting and tell ur mates. www.getzed.co.uk pobox 36504 w45wq 16+ norm150p/tone\n",
            "After removing special chars: free for st week no nokia tone  ur mobile every week just txt nokia to  get txting and tell ur mates wwwgetzedcouk pobox  wwq  normptone\n",
            "After tokenization: ['free', 'for', 'st', 'week', 'no', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'just', 'txt', 'nokia', 'to', 'get', 'txting', 'and', 'tell', 'ur', 'mates', 'wwwgetzedcouk', 'pobox', 'wwq', 'normptone']\n",
            "After stop word removal: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mates', 'wwwgetzedcouk', 'pobox', 'wwq', 'normptone']\n",
            "After lemmatization: ['free', 'st', 'week', 'nokia', 'tone', 'ur', 'mobile', 'every', 'week', 'txt', 'nokia', 'get', 'txting', 'tell', 'ur', 'mate', 'wwwgetzedcouk', 'pobox', 'wwq', 'normptone']\n",
            "After lowercasing: now that you have started dont stop. just pray for more good ideas and anything i see that can help you guys i.ll forward you a link.\n",
            "After removing special chars: now that you have started dont stop just pray for more good ideas and anything i see that can help you guys ill forward you a link\n",
            "After tokenization: ['now', 'that', 'you', 'have', 'started', 'dont', 'stop', 'just', 'pray', 'for', 'more', 'good', 'ideas', 'and', 'anything', 'i', 'see', 'that', 'can', 'help', 'you', 'guys', 'ill', 'forward', 'you', 'a', 'link']\n",
            "After stop word removal: ['started', 'dont', 'stop', 'pray', 'good', 'ideas', 'anything', 'see', 'help', 'guys', 'ill', 'forward', 'link']\n",
            "After lemmatization: ['started', 'dont', 'stop', 'pray', 'good', 'idea', 'anything', 'see', 'help', 'guy', 'ill', 'forward', 'link']\n",
            "After lowercasing: hi darlin im on helens fone im gonna b up the princes 2 nite please come up tb love kate\n",
            "After removing special chars: hi darlin im on helens fone im gonna b up the princes  nite please come up tb love kate\n",
            "After tokenization: ['hi', 'darlin', 'im', 'on', 'helens', 'fone', 'im', 'gon', 'na', 'b', 'up', 'the', 'princes', 'nite', 'please', 'come', 'up', 'tb', 'love', 'kate']\n",
            "After stop word removal: ['hi', 'darlin', 'im', 'helens', 'fone', 'im', 'gon', 'na', 'b', 'princes', 'nite', 'please', 'come', 'tb', 'love', 'kate']\n",
            "After lemmatization: ['hi', 'darlin', 'im', 'helen', 'fone', 'im', 'gon', 'na', 'b', 'prince', 'nite', 'please', 'come', 'tb', 'love', 'kate']\n",
            "After lowercasing: i'm in office now da:)where are you?\n",
            "After removing special chars: im in office now dawhere are you\n",
            "After tokenization: ['im', 'in', 'office', 'now', 'dawhere', 'are', 'you']\n",
            "After stop word removal: ['im', 'office', 'dawhere']\n",
            "After lemmatization: ['im', 'office', 'dawhere']\n",
            "After lowercasing: aiyar u so poor thing... i give u my support k... jia you! i'll think of u...\n",
            "After removing special chars: aiyar u so poor thing i give u my support k jia you ill think of u\n",
            "After tokenization: ['aiyar', 'u', 'so', 'poor', 'thing', 'i', 'give', 'u', 'my', 'support', 'k', 'jia', 'you', 'ill', 'think', 'of', 'u']\n",
            "After stop word removal: ['aiyar', 'u', 'poor', 'thing', 'give', 'u', 'support', 'k', 'jia', 'ill', 'think', 'u']\n",
            "After lemmatization: ['aiyar', 'u', 'poor', 'thing', 'give', 'u', 'support', 'k', 'jia', 'ill', 'think', 'u']\n",
            "After lowercasing: oh unintentionally not bad timing. great. fingers  the trains play along! will give fifteen min warning.\n",
            "After removing special chars: oh unintentionally not bad timing great fingers  the trains play along will give fifteen min warning\n",
            "After tokenization: ['oh', 'unintentionally', 'not', 'bad', 'timing', 'great', 'fingers', 'the', 'trains', 'play', 'along', 'will', 'give', 'fifteen', 'min', 'warning']\n",
            "After stop word removal: ['oh', 'unintentionally', 'bad', 'timing', 'great', 'fingers', 'trains', 'play', 'along', 'give', 'fifteen', 'min', 'warning']\n",
            "After lemmatization: ['oh', 'unintentionally', 'bad', 'timing', 'great', 'finger', 'train', 'play', 'along', 'give', 'fifteen', 'min', 'warning']\n",
            "After lowercasing: get your garden ready for summer with a free selection of summer bulbs and seeds worth å£33:50 only with the scotsman this saturday. to stop go2 notxt.co.uk\n",
            "After removing special chars: get your garden ready for summer with a free selection of summer bulbs and seeds worth  only with the scotsman this saturday to stop go notxtcouk\n",
            "After tokenization: ['get', 'your', 'garden', 'ready', 'for', 'summer', 'with', 'a', 'free', 'selection', 'of', 'summer', 'bulbs', 'and', 'seeds', 'worth', 'only', 'with', 'the', 'scotsman', 'this', 'saturday', 'to', 'stop', 'go', 'notxtcouk']\n",
            "After stop word removal: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulbs', 'seeds', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxtcouk']\n",
            "After lemmatization: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulb', 'seed', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxtcouk']\n",
            "After lowercasing: k..then come wenever u lik to come and also tel vikky to come by getting free time..:-)\n",
            "After removing special chars: kthen come wenever u lik to come and also tel vikky to come by getting free time\n",
            "After tokenization: ['kthen', 'come', 'wenever', 'u', 'lik', 'to', 'come', 'and', 'also', 'tel', 'vikky', 'to', 'come', 'by', 'getting', 'free', 'time']\n",
            "After stop word removal: ['kthen', 'come', 'wenever', 'u', 'lik', 'come', 'also', 'tel', 'vikky', 'come', 'getting', 'free', 'time']\n",
            "After lemmatization: ['kthen', 'come', 'wenever', 'u', 'lik', 'come', 'also', 'tel', 'vikky', 'come', 'getting', 'free', 'time']\n",
            "After lowercasing: pls call me da. what happen.\n",
            "After removing special chars: pls call me da what happen\n",
            "After tokenization: ['pls', 'call', 'me', 'da', 'what', 'happen']\n",
            "After stop word removal: ['pls', 'call', 'da', 'happen']\n",
            "After lemmatization: ['pls', 'call', 'da', 'happen']\n",
            "After lowercasing: happy new year to u and ur family...may this new year bring happiness , stability and tranquility to ur vibrant colourful life:):)\n",
            "After removing special chars: happy new year to u and ur familymay this new year bring happiness  stability and tranquility to ur vibrant colourful life\n",
            "After tokenization: ['happy', 'new', 'year', 'to', 'u', 'and', 'ur', 'familymay', 'this', 'new', 'year', 'bring', 'happiness', 'stability', 'and', 'tranquility', 'to', 'ur', 'vibrant', 'colourful', 'life']\n",
            "After stop word removal: ['happy', 'new', 'year', 'u', 'ur', 'familymay', 'new', 'year', 'bring', 'happiness', 'stability', 'tranquility', 'ur', 'vibrant', 'colourful', 'life']\n",
            "After lemmatization: ['happy', 'new', 'year', 'u', 'ur', 'familymay', 'new', 'year', 'bring', 'happiness', 'stability', 'tranquility', 'ur', 'vibrant', 'colourful', 'life']\n",
            "After lowercasing: no problem with the renewal. i.ll do it right away but i dont know his details.\n",
            "After removing special chars: no problem with the renewal ill do it right away but i dont know his details\n",
            "After tokenization: ['no', 'problem', 'with', 'the', 'renewal', 'ill', 'do', 'it', 'right', 'away', 'but', 'i', 'dont', 'know', 'his', 'details']\n",
            "After stop word removal: ['problem', 'renewal', 'ill', 'right', 'away', 'dont', 'know', 'details']\n",
            "After lemmatization: ['problem', 'renewal', 'ill', 'right', 'away', 'dont', 'know', 'detail']\n",
            "After lowercasing: idk. i'm sitting here in a stop and shop parking lot right now bawling my eyes out because i feel like i'm a failure in everything. nobody wants me and now i feel like i'm failing you.\n",
            "After removing special chars: idk im sitting here in a stop and shop parking lot right now bawling my eyes out because i feel like im a failure in everything nobody wants me and now i feel like im failing you\n",
            "After tokenization: ['idk', 'im', 'sitting', 'here', 'in', 'a', 'stop', 'and', 'shop', 'parking', 'lot', 'right', 'now', 'bawling', 'my', 'eyes', 'out', 'because', 'i', 'feel', 'like', 'im', 'a', 'failure', 'in', 'everything', 'nobody', 'wants', 'me', 'and', 'now', 'i', 'feel', 'like', 'im', 'failing', 'you']\n",
            "After stop word removal: ['idk', 'im', 'sitting', 'stop', 'shop', 'parking', 'lot', 'right', 'bawling', 'eyes', 'feel', 'like', 'im', 'failure', 'everything', 'nobody', 'wants', 'feel', 'like', 'im', 'failing']\n",
            "After lemmatization: ['idk', 'im', 'sitting', 'stop', 'shop', 'parking', 'lot', 'right', 'bawling', 'eye', 'feel', 'like', 'im', 'failure', 'everything', 'nobody', 'want', 'feel', 'like', 'im', 'failing']\n",
            "After lowercasing: haven't left yet so probably gonna be here til dinner\n",
            "After removing special chars: havent left yet so probably gonna be here til dinner\n",
            "After tokenization: ['havent', 'left', 'yet', 'so', 'probably', 'gon', 'na', 'be', 'here', 'til', 'dinner']\n",
            "After stop word removal: ['havent', 'left', 'yet', 'probably', 'gon', 'na', 'til', 'dinner']\n",
            "After lemmatization: ['havent', 'left', 'yet', 'probably', 'gon', 'na', 'til', 'dinner']\n",
            "After lowercasing: like  &lt;#&gt; , same question\n",
            "After removing special chars: like  ltgt  same question\n",
            "After tokenization: ['like', 'ltgt', 'same', 'question']\n",
            "After stop word removal: ['like', 'ltgt', 'question']\n",
            "After lemmatization: ['like', 'ltgt', 'question']\n",
            "After lowercasing: my new years eve was ok. i went to a party with my boyfriend. who is this si then hey\n",
            "After removing special chars: my new years eve was ok i went to a party with my boyfriend who is this si then hey\n",
            "After tokenization: ['my', 'new', 'years', 'eve', 'was', 'ok', 'i', 'went', 'to', 'a', 'party', 'with', 'my', 'boyfriend', 'who', 'is', 'this', 'si', 'then', 'hey']\n",
            "After stop word removal: ['new', 'years', 'eve', 'ok', 'went', 'party', 'boyfriend', 'si', 'hey']\n",
            "After lemmatization: ['new', 'year', 'eve', 'ok', 'went', 'party', 'boyfriend', 'si', 'hey']\n",
            "After lowercasing: sir, i need velusamy sir's date of birth and company bank facilities details.\n",
            "After removing special chars: sir i need velusamy sirs date of birth and company bank facilities details\n",
            "After tokenization: ['sir', 'i', 'need', 'velusamy', 'sirs', 'date', 'of', 'birth', 'and', 'company', 'bank', 'facilities', 'details']\n",
            "After stop word removal: ['sir', 'need', 'velusamy', 'sirs', 'date', 'birth', 'company', 'bank', 'facilities', 'details']\n",
            "After lemmatization: ['sir', 'need', 'velusamy', 'sir', 'date', 'birth', 'company', 'bank', 'facility', 'detail']\n",
            "After lowercasing: k k:) sms chat with me.\n",
            "After removing special chars: k k sms chat with me\n",
            "After tokenization: ['k', 'k', 'sms', 'chat', 'with', 'me']\n",
            "After stop word removal: ['k', 'k', 'sms', 'chat']\n",
            "After lemmatization: ['k', 'k', 'sm', 'chat']\n",
            "After lowercasing: i will come with karnan car. please wait till 6pm will directly goto doctor.\n",
            "After removing special chars: i will come with karnan car please wait till pm will directly goto doctor\n",
            "After tokenization: ['i', 'will', 'come', 'with', 'karnan', 'car', 'please', 'wait', 'till', 'pm', 'will', 'directly', 'goto', 'doctor']\n",
            "After stop word removal: ['come', 'karnan', 'car', 'please', 'wait', 'till', 'pm', 'directly', 'goto', 'doctor']\n",
            "After lemmatization: ['come', 'karnan', 'car', 'please', 'wait', 'till', 'pm', 'directly', 'goto', 'doctor']\n",
            "After lowercasing: no but the bluray player can\n",
            "After removing special chars: no but the bluray player can\n",
            "After tokenization: ['no', 'but', 'the', 'bluray', 'player', 'can']\n",
            "After stop word removal: ['bluray', 'player']\n",
            "After lemmatization: ['bluray', 'player']\n",
            "After lowercasing: ok... then r we meeting later?\n",
            "After removing special chars: ok then r we meeting later\n",
            "After tokenization: ['ok', 'then', 'r', 'we', 'meeting', 'later']\n",
            "After stop word removal: ['ok', 'r', 'meeting', 'later']\n",
            "After lemmatization: ['ok', 'r', 'meeting', 'later']\n",
            "After lowercasing: lol no. i just need to cash in my nitros. hurry come on before i crash out!\n",
            "After removing special chars: lol no i just need to cash in my nitros hurry come on before i crash out\n",
            "After tokenization: ['lol', 'no', 'i', 'just', 'need', 'to', 'cash', 'in', 'my', 'nitros', 'hurry', 'come', 'on', 'before', 'i', 'crash', 'out']\n",
            "After stop word removal: ['lol', 'need', 'cash', 'nitros', 'hurry', 'come', 'crash']\n",
            "After lemmatization: ['lol', 'need', 'cash', 'nitros', 'hurry', 'come', 'crash']\n",
            "After lowercasing: just send a text. we'll skype later.\n",
            "After removing special chars: just send a text well skype later\n",
            "After tokenization: ['just', 'send', 'a', 'text', 'well', 'skype', 'later']\n",
            "After stop word removal: ['send', 'text', 'well', 'skype', 'later']\n",
            "After lemmatization: ['send', 'text', 'well', 'skype', 'later']\n",
            "After lowercasing: ok leave no need to ask\n",
            "After removing special chars: ok leave no need to ask\n",
            "After tokenization: ['ok', 'leave', 'no', 'need', 'to', 'ask']\n",
            "After stop word removal: ['ok', 'leave', 'need', 'ask']\n",
            "After lemmatization: ['ok', 'leave', 'need', 'ask']\n",
            "After lowercasing: congrats 2 mobile 3g videophones r yours. call 09063458130 now! videochat wid ur mates, play java games, dload polyph music, noline rentl. bx420. ip4. 5we. 150p\n",
            "After removing special chars: congrats  mobile g videophones r yours call  now videochat wid ur mates play java games dload polyph music noline rentl bx ip we p\n",
            "After tokenization: ['congrats', 'mobile', 'g', 'videophones', 'r', 'yours', 'call', 'now', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'we', 'p']\n",
            "After stop word removal: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mates', 'play', 'java', 'games', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'p']\n",
            "After lemmatization: ['congrats', 'mobile', 'g', 'videophones', 'r', 'call', 'videochat', 'wid', 'ur', 'mate', 'play', 'java', 'game', 'dload', 'polyph', 'music', 'noline', 'rentl', 'bx', 'ip', 'p']\n",
            "After lowercasing: ìï still got lessons?  ìï in sch?\n",
            "After removing special chars:  still got lessons   in sch\n",
            "After tokenization: ['still', 'got', 'lessons', 'in', 'sch']\n",
            "After stop word removal: ['still', 'got', 'lessons', 'sch']\n",
            "After lemmatization: ['still', 'got', 'lesson', 'sch']\n",
            "After lowercasing: y she dun believe leh? i tot i told her it's true already. i thk she muz c us tog then she believe.\n",
            "After removing special chars: y she dun believe leh i tot i told her its true already i thk she muz c us tog then she believe\n",
            "After tokenization: ['y', 'she', 'dun', 'believe', 'leh', 'i', 'tot', 'i', 'told', 'her', 'its', 'true', 'already', 'i', 'thk', 'she', 'muz', 'c', 'us', 'tog', 'then', 'she', 'believe']\n",
            "After stop word removal: ['dun', 'believe', 'leh', 'tot', 'told', 'true', 'already', 'thk', 'muz', 'c', 'us', 'tog', 'believe']\n",
            "After lemmatization: ['dun', 'believe', 'leh', 'tot', 'told', 'true', 'already', 'thk', 'muz', 'c', 'u', 'tog', 'believe']\n",
            "After lowercasing: oh did you charge camera\n",
            "After removing special chars: oh did you charge camera\n",
            "After tokenization: ['oh', 'did', 'you', 'charge', 'camera']\n",
            "After stop word removal: ['oh', 'charge', 'camera']\n",
            "After lemmatization: ['oh', 'charge', 'camera']\n",
            "After lowercasing: iû÷ve got some salt, you can rub it in my open wounds if you like!\n",
            "After removing special chars: ive got some salt you can rub it in my open wounds if you like\n",
            "After tokenization: ['ive', 'got', 'some', 'salt', 'you', 'can', 'rub', 'it', 'in', 'my', 'open', 'wounds', 'if', 'you', 'like']\n",
            "After stop word removal: ['ive', 'got', 'salt', 'rub', 'open', 'wounds', 'like']\n",
            "After lemmatization: ['ive', 'got', 'salt', 'rub', 'open', 'wound', 'like']\n",
            "After lowercasing: now i'm going for lunch.\n",
            "After removing special chars: now im going for lunch\n",
            "After tokenization: ['now', 'im', 'going', 'for', 'lunch']\n",
            "After stop word removal: ['im', 'going', 'lunch']\n",
            "After lemmatization: ['im', 'going', 'lunch']\n",
            "After lowercasing: i'm in school now n i'll be in da lab doing some stuff give me a call when ì_ r done.\n",
            "After removing special chars: im in school now n ill be in da lab doing some stuff give me a call when  r done\n",
            "After tokenization: ['im', 'in', 'school', 'now', 'n', 'ill', 'be', 'in', 'da', 'lab', 'doing', 'some', 'stuff', 'give', 'me', 'a', 'call', 'when', 'r', 'done']\n",
            "After stop word removal: ['im', 'school', 'n', 'ill', 'da', 'lab', 'stuff', 'give', 'call', 'r', 'done']\n",
            "After lemmatization: ['im', 'school', 'n', 'ill', 'da', 'lab', 'stuff', 'give', 'call', 'r', 'done']\n",
            "After lowercasing: oh k. . i will come tomorrow\n",
            "After removing special chars: oh k  i will come tomorrow\n",
            "After tokenization: ['oh', 'k', 'i', 'will', 'come', 'tomorrow']\n",
            "After stop word removal: ['oh', 'k', 'come', 'tomorrow']\n",
            "After lemmatization: ['oh', 'k', 'come', 'tomorrow']\n",
            "After lowercasing: aight, text me tonight and we'll see what's up\n",
            "After removing special chars: aight text me tonight and well see whats up\n",
            "After tokenization: ['aight', 'text', 'me', 'tonight', 'and', 'well', 'see', 'whats', 'up']\n",
            "After stop word removal: ['aight', 'text', 'tonight', 'well', 'see', 'whats']\n",
            "After lemmatization: ['aight', 'text', 'tonight', 'well', 'see', 'whats']\n",
            "After lowercasing: u 2.\n",
            "After removing special chars: u \n",
            "After tokenization: ['u']\n",
            "After stop word removal: ['u']\n",
            "After lemmatization: ['u']\n",
            "After lowercasing: water logging in desert. geoenvironmental implications.\n",
            "After removing special chars: water logging in desert geoenvironmental implications\n",
            "After tokenization: ['water', 'logging', 'in', 'desert', 'geoenvironmental', 'implications']\n",
            "After stop word removal: ['water', 'logging', 'desert', 'geoenvironmental', 'implications']\n",
            "After lemmatization: ['water', 'logging', 'desert', 'geoenvironmental', 'implication']\n",
            "After lowercasing: raji..pls do me a favour. pls convey my birthday wishes to nimya. pls. today is her birthday.\n",
            "After removing special chars: rajipls do me a favour pls convey my birthday wishes to nimya pls today is her birthday\n",
            "After tokenization: ['rajipls', 'do', 'me', 'a', 'favour', 'pls', 'convey', 'my', 'birthday', 'wishes', 'to', 'nimya', 'pls', 'today', 'is', 'her', 'birthday']\n",
            "After stop word removal: ['rajipls', 'favour', 'pls', 'convey', 'birthday', 'wishes', 'nimya', 'pls', 'today', 'birthday']\n",
            "After lemmatization: ['rajipls', 'favour', 'pls', 'convey', 'birthday', 'wish', 'nimya', 'pls', 'today', 'birthday']\n",
            "After lowercasing: company is very good.environment is terrific and food is really nice:)\n",
            "After removing special chars: company is very goodenvironment is terrific and food is really nice\n",
            "After tokenization: ['company', 'is', 'very', 'goodenvironment', 'is', 'terrific', 'and', 'food', 'is', 'really', 'nice']\n",
            "After stop word removal: ['company', 'goodenvironment', 'terrific', 'food', 'really', 'nice']\n",
            "After lemmatization: ['company', 'goodenvironment', 'terrific', 'food', 'really', 'nice']\n",
            "After lowercasing: very strange.  and  are watching the 2nd one now but i'm in bed. sweet dreams, miss u \n",
            "After removing special chars: very strange  and  are watching the nd one now but im in bed sweet dreams miss u \n",
            "After tokenization: ['very', 'strange', 'and', 'are', 'watching', 'the', 'nd', 'one', 'now', 'but', 'im', 'in', 'bed', 'sweet', 'dreams', 'miss', 'u']\n",
            "After stop word removal: ['strange', 'watching', 'nd', 'one', 'im', 'bed', 'sweet', 'dreams', 'miss', 'u']\n",
            "After lemmatization: ['strange', 'watching', 'nd', 'one', 'im', 'bed', 'sweet', 'dream', 'miss', 'u']\n",
            "After lowercasing: sms auction - a brand new nokia 7250 is up 4 auction today! auction is free 2 join & take part! txt nokia to 86021 now!\n",
            "After removing special chars: sms auction  a brand new nokia  is up  auction today auction is free  join  take part txt nokia to  now\n",
            "After tokenization: ['sms', 'auction', 'a', 'brand', 'new', 'nokia', 'is', 'up', 'auction', 'today', 'auction', 'is', 'free', 'join', 'take', 'part', 'txt', 'nokia', 'to', 'now']\n",
            "After stop word removal: ['sms', 'auction', 'brand', 'new', 'nokia', 'auction', 'today', 'auction', 'free', 'join', 'take', 'part', 'txt', 'nokia']\n",
            "After lemmatization: ['sm', 'auction', 'brand', 'new', 'nokia', 'auction', 'today', 'auction', 'free', 'join', 'take', 'part', 'txt', 'nokia']\n",
            "After lowercasing: hi hope u r both ok, he said he would text and he hasn't, have u seen him, let me down gently please \n",
            "After removing special chars: hi hope u r both ok he said he would text and he hasnt have u seen him let me down gently please \n",
            "After tokenization: ['hi', 'hope', 'u', 'r', 'both', 'ok', 'he', 'said', 'he', 'would', 'text', 'and', 'he', 'hasnt', 'have', 'u', 'seen', 'him', 'let', 'me', 'down', 'gently', 'please']\n",
            "After stop word removal: ['hi', 'hope', 'u', 'r', 'ok', 'said', 'would', 'text', 'hasnt', 'u', 'seen', 'let', 'gently', 'please']\n",
            "After lemmatization: ['hi', 'hope', 'u', 'r', 'ok', 'said', 'would', 'text', 'hasnt', 'u', 'seen', 'let', 'gently', 'please']\n",
            "After lowercasing: babe! i fucking love you too !! you know? fuck it was so good to hear your voice. i so need that. i crave it. i can't get enough. i adore you, ahmad *kisses*\n",
            "After removing special chars: babe i fucking love you too  you know fuck it was so good to hear your voice i so need that i crave it i cant get enough i adore you ahmad kisses\n",
            "After tokenization: ['babe', 'i', 'fucking', 'love', 'you', 'too', 'you', 'know', 'fuck', 'it', 'was', 'so', 'good', 'to', 'hear', 'your', 'voice', 'i', 'so', 'need', 'that', 'i', 'crave', 'it', 'i', 'cant', 'get', 'enough', 'i', 'adore', 'you', 'ahmad', 'kisses']\n",
            "After stop word removal: ['babe', 'fucking', 'love', 'know', 'fuck', 'good', 'hear', 'voice', 'need', 'crave', 'cant', 'get', 'enough', 'adore', 'ahmad', 'kisses']\n",
            "After lemmatization: ['babe', 'fucking', 'love', 'know', 'fuck', 'good', 'hear', 'voice', 'need', 'crave', 'cant', 'get', 'enough', 'adore', 'ahmad', 'kiss']\n",
            "After lowercasing: k sure am in my relatives home. sms me de. pls:-)\n",
            "After removing special chars: k sure am in my relatives home sms me de pls\n",
            "After tokenization: ['k', 'sure', 'am', 'in', 'my', 'relatives', 'home', 'sms', 'me', 'de', 'pls']\n",
            "After stop word removal: ['k', 'sure', 'relatives', 'home', 'sms', 'de', 'pls']\n",
            "After lemmatization: ['k', 'sure', 'relative', 'home', 'sm', 'de', 'pls']\n",
            "After lowercasing: i sent them. do you like?\n",
            "After removing special chars: i sent them do you like\n",
            "After tokenization: ['i', 'sent', 'them', 'do', 'you', 'like']\n",
            "After stop word removal: ['sent', 'like']\n",
            "After lemmatization: ['sent', 'like']\n",
            "After lowercasing: fuuuuck i need to stop sleepin, sup\n",
            "After removing special chars: fuuuuck i need to stop sleepin sup\n",
            "After tokenization: ['fuuuuck', 'i', 'need', 'to', 'stop', 'sleepin', 'sup']\n",
            "After stop word removal: ['fuuuuck', 'need', 'stop', 'sleepin', 'sup']\n",
            "After lemmatization: ['fuuuuck', 'need', 'stop', 'sleepin', 'sup']\n",
            "After lowercasing: i'm in town now so i'll jus take mrt down later.\n",
            "After removing special chars: im in town now so ill jus take mrt down later\n",
            "After tokenization: ['im', 'in', 'town', 'now', 'so', 'ill', 'jus', 'take', 'mrt', 'down', 'later']\n",
            "After stop word removal: ['im', 'town', 'ill', 'jus', 'take', 'mrt', 'later']\n",
            "After lemmatization: ['im', 'town', 'ill', 'jus', 'take', 'mrt', 'later']\n",
            "After lowercasing: i just cooked a rather nice salmon a la you\n",
            "After removing special chars: i just cooked a rather nice salmon a la you\n",
            "After tokenization: ['i', 'just', 'cooked', 'a', 'rather', 'nice', 'salmon', 'a', 'la', 'you']\n",
            "After stop word removal: ['cooked', 'rather', 'nice', 'salmon', 'la']\n",
            "After lemmatization: ['cooked', 'rather', 'nice', 'salmon', 'la']\n",
            "After lowercasing: i uploaded mine to facebook\n",
            "After removing special chars: i uploaded mine to facebook\n",
            "After tokenization: ['i', 'uploaded', 'mine', 'to', 'facebook']\n",
            "After stop word removal: ['uploaded', 'mine', 'facebook']\n",
            "After lemmatization: ['uploaded', 'mine', 'facebook']\n",
            "After lowercasing: what time u wrkin?\n",
            "After removing special chars: what time u wrkin\n",
            "After tokenization: ['what', 'time', 'u', 'wrkin']\n",
            "After stop word removal: ['time', 'u', 'wrkin']\n",
            "After lemmatization: ['time', 'u', 'wrkin']\n",
            "After lowercasing: okie\n",
            "After removing special chars: okie\n",
            "After tokenization: ['okie']\n",
            "After stop word removal: ['okie']\n",
            "After lemmatization: ['okie']\n",
            "After lowercasing: ree entry in 2 a weekly comp for a chance to win an ipod. txt pod to 80182 to get entry (std txt rate) t&c's apply 08452810073 for details 18+\n",
            "After removing special chars: ree entry in  a weekly comp for a chance to win an ipod txt pod to  to get entry std txt rate tcs apply  for details \n",
            "After tokenization: ['ree', 'entry', 'in', 'a', 'weekly', 'comp', 'for', 'a', 'chance', 'to', 'win', 'an', 'ipod', 'txt', 'pod', 'to', 'to', 'get', 'entry', 'std', 'txt', 'rate', 'tcs', 'apply', 'for', 'details']\n",
            "After stop word removal: ['ree', 'entry', 'weekly', 'comp', 'chance', 'win', 'ipod', 'txt', 'pod', 'get', 'entry', 'std', 'txt', 'rate', 'tcs', 'apply', 'details']\n",
            "After lemmatization: ['ree', 'entry', 'weekly', 'comp', 'chance', 'win', 'ipod', 'txt', 'pod', 'get', 'entry', 'std', 'txt', 'rate', 'tc', 'apply', 'detail']\n",
            "After lowercasing: our records indicate u maybe entitled to 5000 pounds in compensation for the accident you had. to claim 4 free reply with claim to this msg. 2 stop txt stop\n",
            "After removing special chars: our records indicate u maybe entitled to  pounds in compensation for the accident you had to claim  free reply with claim to this msg  stop txt stop\n",
            "After tokenization: ['our', 'records', 'indicate', 'u', 'maybe', 'entitled', 'to', 'pounds', 'in', 'compensation', 'for', 'the', 'accident', 'you', 'had', 'to', 'claim', 'free', 'reply', 'with', 'claim', 'to', 'this', 'msg', 'stop', 'txt', 'stop']\n",
            "After stop word removal: ['records', 'indicate', 'u', 'maybe', 'entitled', 'pounds', 'compensation', 'accident', 'claim', 'free', 'reply', 'claim', 'msg', 'stop', 'txt', 'stop']\n",
            "After lemmatization: ['record', 'indicate', 'u', 'maybe', 'entitled', 'pound', 'compensation', 'accident', 'claim', 'free', 'reply', 'claim', 'msg', 'stop', 'txt', 'stop']\n",
            "After lowercasing: sorry, i'll call later\n",
            "After removing special chars: sorry ill call later\n",
            "After tokenization: ['sorry', 'ill', 'call', 'later']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later']\n",
            "After lowercasing: oh oh... den muz change plan liao... go back have to yan jiu again...\n",
            "After removing special chars: oh oh den muz change plan liao go back have to yan jiu again\n",
            "After tokenization: ['oh', 'oh', 'den', 'muz', 'change', 'plan', 'liao', 'go', 'back', 'have', 'to', 'yan', 'jiu', 'again']\n",
            "After stop word removal: ['oh', 'oh', 'den', 'muz', 'change', 'plan', 'liao', 'go', 'back', 'yan', 'jiu']\n",
            "After lemmatization: ['oh', 'oh', 'den', 'muz', 'change', 'plan', 'liao', 'go', 'back', 'yan', 'jiu']\n",
            "After lowercasing: it's wylie, you in tampa or sarasota?\n",
            "After removing special chars: its wylie you in tampa or sarasota\n",
            "After tokenization: ['its', 'wylie', 'you', 'in', 'tampa', 'or', 'sarasota']\n",
            "After stop word removal: ['wylie', 'tampa', 'sarasota']\n",
            "After lemmatization: ['wylie', 'tampa', 'sarasota']\n",
            "After lowercasing: ok... take ur time n enjoy ur dinner...\n",
            "After removing special chars: ok take ur time n enjoy ur dinner\n",
            "After tokenization: ['ok', 'take', 'ur', 'time', 'n', 'enjoy', 'ur', 'dinner']\n",
            "After stop word removal: ['ok', 'take', 'ur', 'time', 'n', 'enjoy', 'ur', 'dinner']\n",
            "After lemmatization: ['ok', 'take', 'ur', 'time', 'n', 'enjoy', 'ur', 'dinner']\n",
            "After lowercasing: darren was saying dat if u meeting da ge den we dun meet 4 dinner. cos later u leave xy will feel awkward. den u meet him 4 lunch lor.\n",
            "After removing special chars: darren was saying dat if u meeting da ge den we dun meet  dinner cos later u leave xy will feel awkward den u meet him  lunch lor\n",
            "After tokenization: ['darren', 'was', 'saying', 'dat', 'if', 'u', 'meeting', 'da', 'ge', 'den', 'we', 'dun', 'meet', 'dinner', 'cos', 'later', 'u', 'leave', 'xy', 'will', 'feel', 'awkward', 'den', 'u', 'meet', 'him', 'lunch', 'lor']\n",
            "After stop word removal: ['darren', 'saying', 'dat', 'u', 'meeting', 'da', 'ge', 'den', 'dun', 'meet', 'dinner', 'cos', 'later', 'u', 'leave', 'xy', 'feel', 'awkward', 'den', 'u', 'meet', 'lunch', 'lor']\n",
            "After lemmatization: ['darren', 'saying', 'dat', 'u', 'meeting', 'da', 'ge', 'den', 'dun', 'meet', 'dinner', 'co', 'later', 'u', 'leave', 'xy', 'feel', 'awkward', 'den', 'u', 'meet', 'lunch', 'lor']\n",
            "After lowercasing: spook up your mob with a halloween collection of a logo & pic message plus a free eerie tone, txt card spook to 8007 zed 08701417012150p per logo/pic \n",
            "After removing special chars: spook up your mob with a halloween collection of a logo  pic message plus a free eerie tone txt card spook to  zed p per logopic \n",
            "After tokenization: ['spook', 'up', 'your', 'mob', 'with', 'a', 'halloween', 'collection', 'of', 'a', 'logo', 'pic', 'message', 'plus', 'a', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'to', 'zed', 'p', 'per', 'logopic']\n",
            "After stop word removal: ['spook', 'mob', 'halloween', 'collection', 'logo', 'pic', 'message', 'plus', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'zed', 'p', 'per', 'logopic']\n",
            "After lemmatization: ['spook', 'mob', 'halloween', 'collection', 'logo', 'pic', 'message', 'plus', 'free', 'eerie', 'tone', 'txt', 'card', 'spook', 'zed', 'p', 'per', 'logopic']\n",
            "After lowercasing: i like cheap! but iû÷m happy to splash out on the wine if it makes you feel better..\n",
            "After removing special chars: i like cheap but im happy to splash out on the wine if it makes you feel better\n",
            "After tokenization: ['i', 'like', 'cheap', 'but', 'im', 'happy', 'to', 'splash', 'out', 'on', 'the', 'wine', 'if', 'it', 'makes', 'you', 'feel', 'better']\n",
            "After stop word removal: ['like', 'cheap', 'im', 'happy', 'splash', 'wine', 'makes', 'feel', 'better']\n",
            "After lemmatization: ['like', 'cheap', 'im', 'happy', 'splash', 'wine', 'make', 'feel', 'better']\n",
            "After lowercasing: she.s fine. i have had difficulties with her phone. it works with mine. can you pls send her another friend request.\n",
            "After removing special chars: shes fine i have had difficulties with her phone it works with mine can you pls send her another friend request\n",
            "After tokenization: ['shes', 'fine', 'i', 'have', 'had', 'difficulties', 'with', 'her', 'phone', 'it', 'works', 'with', 'mine', 'can', 'you', 'pls', 'send', 'her', 'another', 'friend', 'request']\n",
            "After stop word removal: ['shes', 'fine', 'difficulties', 'phone', 'works', 'mine', 'pls', 'send', 'another', 'friend', 'request']\n",
            "After lemmatization: ['shes', 'fine', 'difficulty', 'phone', 'work', 'mine', 'pls', 'send', 'another', 'friend', 'request']\n",
            "After lowercasing: ugh my leg hurts. musta overdid it on mon.\n",
            "After removing special chars: ugh my leg hurts musta overdid it on mon\n",
            "After tokenization: ['ugh', 'my', 'leg', 'hurts', 'musta', 'overdid', 'it', 'on', 'mon']\n",
            "After stop word removal: ['ugh', 'leg', 'hurts', 'musta', 'overdid', 'mon']\n",
            "After lemmatization: ['ugh', 'leg', 'hurt', 'musta', 'overdid', 'mon']\n",
            "After lowercasing: call germany for only 1 pence per minute! call from a fixed line via access number 0844 861 85 85. no prepayment. direct access! www.telediscount.co.uk\n",
            "After removing special chars: call germany for only  pence per minute call from a fixed line via access number     no prepayment direct access wwwtelediscountcouk\n",
            "After tokenization: ['call', 'germany', 'for', 'only', 'pence', 'per', 'minute', 'call', 'from', 'a', 'fixed', 'line', 'via', 'access', 'number', 'no', 'prepayment', 'direct', 'access', 'wwwtelediscountcouk']\n",
            "After stop word removal: ['call', 'germany', 'pence', 'per', 'minute', 'call', 'fixed', 'line', 'via', 'access', 'number', 'prepayment', 'direct', 'access', 'wwwtelediscountcouk']\n",
            "After lemmatization: ['call', 'germany', 'penny', 'per', 'minute', 'call', 'fixed', 'line', 'via', 'access', 'number', 'prepayment', 'direct', 'access', 'wwwtelediscountcouk']\n",
            "After lowercasing: you ve won! your 4* costa del sol holiday or å£5000 await collection. call 09050090044 now toclaim. sae, tc s, pobox334, stockport, sk38xh, costå£1.50/pm, max10mins\n",
            "After removing special chars: you ve won your  costa del sol holiday or  await collection call  now toclaim sae tc s pobox stockport skxh costpm maxmins\n",
            "After tokenization: ['you', 've', 'won', 'your', 'costa', 'del', 'sol', 'holiday', 'or', 'await', 'collection', 'call', 'now', 'toclaim', 'sae', 'tc', 's', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After stop word removal: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After lemmatization: ['costa', 'del', 'sol', 'holiday', 'await', 'collection', 'call', 'toclaim', 'sae', 'tc', 'pobox', 'stockport', 'skxh', 'costpm', 'maxmins']\n",
            "After lowercasing: wot student discount can u get on books?\n",
            "After removing special chars: wot student discount can u get on books\n",
            "After tokenization: ['wot', 'student', 'discount', 'can', 'u', 'get', 'on', 'books']\n",
            "After stop word removal: ['wot', 'student', 'discount', 'u', 'get', 'books']\n",
            "After lemmatization: ['wot', 'student', 'discount', 'u', 'get', 'book']\n",
            "After lowercasing: me fine..absolutly fine\n",
            "After removing special chars: me fineabsolutly fine\n",
            "After tokenization: ['me', 'fineabsolutly', 'fine']\n",
            "After stop word removal: ['fineabsolutly', 'fine']\n",
            "After lemmatization: ['fineabsolutly', 'fine']\n",
            "After lowercasing: how come she can get it? should b quite diff to guess rite...\n",
            "After removing special chars: how come she can get it should b quite diff to guess rite\n",
            "After tokenization: ['how', 'come', 'she', 'can', 'get', 'it', 'should', 'b', 'quite', 'diff', 'to', 'guess', 'rite']\n",
            "After stop word removal: ['come', 'get', 'b', 'quite', 'diff', 'guess', 'rite']\n",
            "After lemmatization: ['come', 'get', 'b', 'quite', 'diff', 'guess', 'rite']\n",
            "After lowercasing: had your mobile 11mths ? update for free to oranges latest colour camera mobiles & unlimited weekend calls. call mobile upd8 on freefone 08000839402 or 2stoptxt\n",
            "After removing special chars: had your mobile mths  update for free to oranges latest colour camera mobiles  unlimited weekend calls call mobile upd on freefone  or stoptxt\n",
            "After tokenization: ['had', 'your', 'mobile', 'mths', 'update', 'for', 'free', 'to', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'on', 'freefone', 'or', 'stoptxt']\n",
            "After stop word removal: ['mobile', 'mths', 'update', 'free', 'oranges', 'latest', 'colour', 'camera', 'mobiles', 'unlimited', 'weekend', 'calls', 'call', 'mobile', 'upd', 'freefone', 'stoptxt']\n",
            "After lemmatization: ['mobile', 'mths', 'update', 'free', 'orange', 'latest', 'colour', 'camera', 'mobile', 'unlimited', 'weekend', 'call', 'call', 'mobile', 'upd', 'freefone', 'stoptxt']\n",
            "After lowercasing: i will reach ur home in  &lt;#&gt;  minutes\n",
            "After removing special chars: i will reach ur home in  ltgt  minutes\n",
            "After tokenization: ['i', 'will', 'reach', 'ur', 'home', 'in', 'ltgt', 'minutes']\n",
            "After stop word removal: ['reach', 'ur', 'home', 'ltgt', 'minutes']\n",
            "After lemmatization: ['reach', 'ur', 'home', 'ltgt', 'minute']\n",
            "After lowercasing: babe, i'm answering you, can't you see me ? maybe you'd better reboot ym ... i got the photo ... it's great !\n",
            "After removing special chars: babe im answering you cant you see me  maybe youd better reboot ym  i got the photo  its great \n",
            "After tokenization: ['babe', 'im', 'answering', 'you', 'cant', 'you', 'see', 'me', 'maybe', 'youd', 'better', 'reboot', 'ym', 'i', 'got', 'the', 'photo', 'its', 'great']\n",
            "After stop word removal: ['babe', 'im', 'answering', 'cant', 'see', 'maybe', 'youd', 'better', 'reboot', 'ym', 'got', 'photo', 'great']\n",
            "After lemmatization: ['babe', 'im', 'answering', 'cant', 'see', 'maybe', 'youd', 'better', 'reboot', 'ym', 'got', 'photo', 'great']\n",
            "After lowercasing: hi.what you think about match?\n",
            "After removing special chars: hiwhat you think about match\n",
            "After tokenization: ['hiwhat', 'you', 'think', 'about', 'match']\n",
            "After stop word removal: ['hiwhat', 'think', 'match']\n",
            "After lemmatization: ['hiwhat', 'think', 'match']\n",
            "After lowercasing: i know you are thinkin malaria. but relax, children cant handle malaria. she would have been worse and its gastroenteritis. if she takes enough to replace her loss her temp will reduce. and if you give her malaria meds now she will just vomit. its a self limiting illness she has which means in a few days it will completely stop\n",
            "After removing special chars: i know you are thinkin malaria but relax children cant handle malaria she would have been worse and its gastroenteritis if she takes enough to replace her loss her temp will reduce and if you give her malaria meds now she will just vomit its a self limiting illness she has which means in a few days it will completely stop\n",
            "After tokenization: ['i', 'know', 'you', 'are', 'thinkin', 'malaria', 'but', 'relax', 'children', 'cant', 'handle', 'malaria', 'she', 'would', 'have', 'been', 'worse', 'and', 'its', 'gastroenteritis', 'if', 'she', 'takes', 'enough', 'to', 'replace', 'her', 'loss', 'her', 'temp', 'will', 'reduce', 'and', 'if', 'you', 'give', 'her', 'malaria', 'meds', 'now', 'she', 'will', 'just', 'vomit', 'its', 'a', 'self', 'limiting', 'illness', 'she', 'has', 'which', 'means', 'in', 'a', 'few', 'days', 'it', 'will', 'completely', 'stop']\n",
            "After stop word removal: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'takes', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'meds', 'vomit', 'self', 'limiting', 'illness', 'means', 'days', 'completely', 'stop']\n",
            "After lemmatization: ['know', 'thinkin', 'malaria', 'relax', 'child', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'take', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'med', 'vomit', 'self', 'limiting', 'illness', 'mean', 'day', 'completely', 'stop']\n",
            "After lowercasing: dai i downloaded but there is only exe file which i can only run that exe after installing.\n",
            "After removing special chars: dai i downloaded but there is only exe file which i can only run that exe after installing\n",
            "After tokenization: ['dai', 'i', 'downloaded', 'but', 'there', 'is', 'only', 'exe', 'file', 'which', 'i', 'can', 'only', 'run', 'that', 'exe', 'after', 'installing']\n",
            "After stop word removal: ['dai', 'downloaded', 'exe', 'file', 'run', 'exe', 'installing']\n",
            "After lemmatization: ['dai', 'downloaded', 'exe', 'file', 'run', 'exe', 'installing']\n",
            "After lowercasing: it is only yesterday true true.\n",
            "After removing special chars: it is only yesterday true true\n",
            "After tokenization: ['it', 'is', 'only', 'yesterday', 'true', 'true']\n",
            "After stop word removal: ['yesterday', 'true', 'true']\n",
            "After lemmatization: ['yesterday', 'true', 'true']\n",
            "After lowercasing: k.k.how is your business now?\n",
            "After removing special chars: kkhow is your business now\n",
            "After tokenization: ['kkhow', 'is', 'your', 'business', 'now']\n",
            "After stop word removal: ['kkhow', 'business']\n",
            "After lemmatization: ['kkhow', 'business']\n",
            "After lowercasing: 3 pa but not selected.\n",
            "After removing special chars:  pa but not selected\n",
            "After tokenization: ['pa', 'but', 'not', 'selected']\n",
            "After stop word removal: ['pa', 'selected']\n",
            "After lemmatization: ['pa', 'selected']\n",
            "After lowercasing: natalja (25/f) is inviting you to be her friend. reply yes-440 or no-440 see her: www.sms.ac/u/nat27081980 stop? send stop frnd to 62468\n",
            "After removing special chars: natalja f is inviting you to be her friend reply yes or no see her wwwsmsacunat stop send stop frnd to \n",
            "After tokenization: ['natalja', 'f', 'is', 'inviting', 'you', 'to', 'be', 'her', 'friend', 'reply', 'yes', 'or', 'no', 'see', 'her', 'wwwsmsacunat', 'stop', 'send', 'stop', 'frnd', 'to']\n",
            "After stop word removal: ['natalja', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacunat', 'stop', 'send', 'stop', 'frnd']\n",
            "After lemmatization: ['natalja', 'f', 'inviting', 'friend', 'reply', 'yes', 'see', 'wwwsmsacunat', 'stop', 'send', 'stop', 'frnd']\n",
            "After lowercasing: i keep ten rs in my shelf:) buy two egg.\n",
            "After removing special chars: i keep ten rs in my shelf buy two egg\n",
            "After tokenization: ['i', 'keep', 'ten', 'rs', 'in', 'my', 'shelf', 'buy', 'two', 'egg']\n",
            "After stop word removal: ['keep', 'ten', 'rs', 'shelf', 'buy', 'two', 'egg']\n",
            "After lemmatization: ['keep', 'ten', 'r', 'shelf', 'buy', 'two', 'egg']\n",
            "After lowercasing: i am late. i will be there at\n",
            "After removing special chars: i am late i will be there at\n",
            "After tokenization: ['i', 'am', 'late', 'i', 'will', 'be', 'there', 'at']\n",
            "After stop word removal: ['late']\n",
            "After lemmatization: ['late']\n",
            "After lowercasing: well thats nice. too bad i cant eat it\n",
            "After removing special chars: well thats nice too bad i cant eat it\n",
            "After tokenization: ['well', 'thats', 'nice', 'too', 'bad', 'i', 'cant', 'eat', 'it']\n",
            "After stop word removal: ['well', 'thats', 'nice', 'bad', 'cant', 'eat']\n",
            "After lemmatization: ['well', 'thats', 'nice', 'bad', 'cant', 'eat']\n",
            "After lowercasing: i accidentally brought em home in the box\n",
            "After removing special chars: i accidentally brought em home in the box\n",
            "After tokenization: ['i', 'accidentally', 'brought', 'em', 'home', 'in', 'the', 'box']\n",
            "After stop word removal: ['accidentally', 'brought', 'em', 'home', 'box']\n",
            "After lemmatization: ['accidentally', 'brought', 'em', 'home', 'box']\n",
            "After lowercasing: pls she needs to dat slowly or she will vomit more.\n",
            "After removing special chars: pls she needs to dat slowly or she will vomit more\n",
            "After tokenization: ['pls', 'she', 'needs', 'to', 'dat', 'slowly', 'or', 'she', 'will', 'vomit', 'more']\n",
            "After stop word removal: ['pls', 'needs', 'dat', 'slowly', 'vomit']\n",
            "After lemmatization: ['pls', 'need', 'dat', 'slowly', 'vomit']\n",
            "After lowercasing: i have to take exam with in march 3\n",
            "After removing special chars: i have to take exam with in march \n",
            "After tokenization: ['i', 'have', 'to', 'take', 'exam', 'with', 'in', 'march']\n",
            "After stop word removal: ['take', 'exam', 'march']\n",
            "After lemmatization: ['take', 'exam', 'march']\n",
            "After lowercasing: jane babes not goin 2 wrk, feel ill after lst nite. foned in already cover 4 me chuck.:-)\n",
            "After removing special chars: jane babes not goin  wrk feel ill after lst nite foned in already cover  me chuck\n",
            "After tokenization: ['jane', 'babes', 'not', 'goin', 'wrk', 'feel', 'ill', 'after', 'lst', 'nite', 'foned', 'in', 'already', 'cover', 'me', 'chuck']\n",
            "After stop word removal: ['jane', 'babes', 'goin', 'wrk', 'feel', 'ill', 'lst', 'nite', 'foned', 'already', 'cover', 'chuck']\n",
            "After lemmatization: ['jane', 'babe', 'goin', 'wrk', 'feel', 'ill', 'lst', 'nite', 'foned', 'already', 'cover', 'chuck']\n",
            "After lowercasing: 5 nights...we nt staying at port step liao...too ex\n",
            "After removing special chars:  nightswe nt staying at port step liaotoo ex\n",
            "After tokenization: ['nightswe', 'nt', 'staying', 'at', 'port', 'step', 'liaotoo', 'ex']\n",
            "After stop word removal: ['nightswe', 'nt', 'staying', 'port', 'step', 'liaotoo', 'ex']\n",
            "After lemmatization: ['nightswe', 'nt', 'staying', 'port', 'step', 'liaotoo', 'ex']\n",
            "After lowercasing: if i die i want u to have all my stuffs.\n",
            "After removing special chars: if i die i want u to have all my stuffs\n",
            "After tokenization: ['if', 'i', 'die', 'i', 'want', 'u', 'to', 'have', 'all', 'my', 'stuffs']\n",
            "After stop word removal: ['die', 'want', 'u', 'stuffs']\n",
            "After lemmatization: ['die', 'want', 'u', 'stuff']\n",
            "After lowercasing: \\oh fuck. juswoke up in a bed on a boatin the docks. slept wid 25 year old. spinout! giv u da gossip l8r. xxx\\\"\"\n",
            "After removing special chars: oh fuck juswoke up in a bed on a boatin the docks slept wid  year old spinout giv u da gossip lr xxx\n",
            "After tokenization: ['oh', 'fuck', 'juswoke', 'up', 'in', 'a', 'bed', 'on', 'a', 'boatin', 'the', 'docks', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'u', 'da', 'gossip', 'lr', 'xxx']\n",
            "After stop word removal: ['oh', 'fuck', 'juswoke', 'bed', 'boatin', 'docks', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'u', 'da', 'gossip', 'lr', 'xxx']\n",
            "After lemmatization: ['oh', 'fuck', 'juswoke', 'bed', 'boatin', 'dock', 'slept', 'wid', 'year', 'old', 'spinout', 'giv', 'u', 'da', 'gossip', 'lr', 'xxx']\n",
            "After lowercasing: smile in pleasure smile in pain smile when trouble pours like rain smile when sum1 hurts u smile becoz someone still loves to see u smiling!!\n",
            "After removing special chars: smile in pleasure smile in pain smile when trouble pours like rain smile when sum hurts u smile becoz someone still loves to see u smiling\n",
            "After tokenization: ['smile', 'in', 'pleasure', 'smile', 'in', 'pain', 'smile', 'when', 'trouble', 'pours', 'like', 'rain', 'smile', 'when', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'to', 'see', 'u', 'smiling']\n",
            "After stop word removal: ['smile', 'pleasure', 'smile', 'pain', 'smile', 'trouble', 'pours', 'like', 'rain', 'smile', 'sum', 'hurts', 'u', 'smile', 'becoz', 'someone', 'still', 'loves', 'see', 'u', 'smiling']\n",
            "After lemmatization: ['smile', 'pleasure', 'smile', 'pain', 'smile', 'trouble', 'pours', 'like', 'rain', 'smile', 'sum', 'hurt', 'u', 'smile', 'becoz', 'someone', 'still', 'love', 'see', 'u', 'smiling']\n",
            "After lowercasing: prabha..i'm soryda..realy..frm heart i'm sory\n",
            "After removing special chars: prabhaim sorydarealyfrm heart im sory\n",
            "After tokenization: ['prabhaim', 'sorydarealyfrm', 'heart', 'im', 'sory']\n",
            "After stop word removal: ['prabhaim', 'sorydarealyfrm', 'heart', 'im', 'sory']\n",
            "After lemmatization: ['prabhaim', 'sorydarealyfrm', 'heart', 'im', 'sory']\n",
            "After lowercasing: i re-met alex nichols from middle school and it turns out he's dealing!\n",
            "After removing special chars: i remet alex nichols from middle school and it turns out hes dealing\n",
            "After tokenization: ['i', 'remet', 'alex', 'nichols', 'from', 'middle', 'school', 'and', 'it', 'turns', 'out', 'hes', 'dealing']\n",
            "After stop word removal: ['remet', 'alex', 'nichols', 'middle', 'school', 'turns', 'hes', 'dealing']\n",
            "After lemmatization: ['remet', 'alex', 'nichols', 'middle', 'school', 'turn', 'he', 'dealing']\n",
            "After lowercasing: private! your 2003 account statement for <fone no> shows 800 un-redeemed s. i. m. points. call 08715203656 identifier code: 42049 expires 26/10/04\n",
            "After removing special chars: private your  account statement for fone no shows  unredeemed s i m points call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'fone', 'no', 'shows', 'unredeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'fone', 'shows', 'unredeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'fone', 'show', 'unredeemed', 'point', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: it means u could not keep ur words.\n",
            "After removing special chars: it means u could not keep ur words\n",
            "After tokenization: ['it', 'means', 'u', 'could', 'not', 'keep', 'ur', 'words']\n",
            "After stop word removal: ['means', 'u', 'could', 'keep', 'ur', 'words']\n",
            "After lemmatization: ['mean', 'u', 'could', 'keep', 'ur', 'word']\n",
            "After lowercasing: nope, i'm still in the market\n",
            "After removing special chars: nope im still in the market\n",
            "After tokenization: ['nope', 'im', 'still', 'in', 'the', 'market']\n",
            "After stop word removal: ['nope', 'im', 'still', 'market']\n",
            "After lemmatization: ['nope', 'im', 'still', 'market']\n",
            "After lowercasing: i realise you are a busy guy and i'm trying not to be a bother. i have to get some exams outta the way and then try the cars. do have a gr8 day\n",
            "After removing special chars: i realise you are a busy guy and im trying not to be a bother i have to get some exams outta the way and then try the cars do have a gr day\n",
            "After tokenization: ['i', 'realise', 'you', 'are', 'a', 'busy', 'guy', 'and', 'im', 'trying', 'not', 'to', 'be', 'a', 'bother', 'i', 'have', 'to', 'get', 'some', 'exams', 'outta', 'the', 'way', 'and', 'then', 'try', 'the', 'cars', 'do', 'have', 'a', 'gr', 'day']\n",
            "After stop word removal: ['realise', 'busy', 'guy', 'im', 'trying', 'bother', 'get', 'exams', 'outta', 'way', 'try', 'cars', 'gr', 'day']\n",
            "After lemmatization: ['realise', 'busy', 'guy', 'im', 'trying', 'bother', 'get', 'exam', 'outta', 'way', 'try', 'car', 'gr', 'day']\n",
            "After lowercasing: you are chosen to receive a å£350 award! pls call claim number 09066364311 to collect your award which you are selected to receive as a valued mobile customer.\n",
            "After removing special chars: you are chosen to receive a  award pls call claim number  to collect your award which you are selected to receive as a valued mobile customer\n",
            "After tokenization: ['you', 'are', 'chosen', 'to', 'receive', 'a', 'award', 'pls', 'call', 'claim', 'number', 'to', 'collect', 'your', 'award', 'which', 'you', 'are', 'selected', 'to', 'receive', 'as', 'a', 'valued', 'mobile', 'customer']\n",
            "After stop word removal: ['chosen', 'receive', 'award', 'pls', 'call', 'claim', 'number', 'collect', 'award', 'selected', 'receive', 'valued', 'mobile', 'customer']\n",
            "After lemmatization: ['chosen', 'receive', 'award', 'pls', 'call', 'claim', 'number', 'collect', 'award', 'selected', 'receive', 'valued', 'mobile', 'customer']\n",
            "After lowercasing: hey what how about your project. started aha da.\n",
            "After removing special chars: hey what how about your project started aha da\n",
            "After tokenization: ['hey', 'what', 'how', 'about', 'your', 'project', 'started', 'aha', 'da']\n",
            "After stop word removal: ['hey', 'project', 'started', 'aha', 'da']\n",
            "After lemmatization: ['hey', 'project', 'started', 'aha', 'da']\n",
            "After lowercasing: ok cool. see ya then.\n",
            "After removing special chars: ok cool see ya then\n",
            "After tokenization: ['ok', 'cool', 'see', 'ya', 'then']\n",
            "After stop word removal: ['ok', 'cool', 'see', 'ya']\n",
            "After lemmatization: ['ok', 'cool', 'see', 'ya']\n",
            "After lowercasing: am on the uworld site. am i buying the qbank only or am i buying it with the self assessment also?\n",
            "After removing special chars: am on the uworld site am i buying the qbank only or am i buying it with the self assessment also\n",
            "After tokenization: ['am', 'on', 'the', 'uworld', 'site', 'am', 'i', 'buying', 'the', 'qbank', 'only', 'or', 'am', 'i', 'buying', 'it', 'with', 'the', 'self', 'assessment', 'also']\n",
            "After stop word removal: ['uworld', 'site', 'buying', 'qbank', 'buying', 'self', 'assessment', 'also']\n",
            "After lemmatization: ['uworld', 'site', 'buying', 'qbank', 'buying', 'self', 'assessment', 'also']\n",
            "After lowercasing: your opinion about me? 1. over 2. jada 3. kusruthi 4. lovable 5. silent 6. spl character 7. not matured 8. stylish 9. simple pls reply..\n",
            "After removing special chars: your opinion about me  over  jada  kusruthi  lovable  silent  spl character  not matured  stylish  simple pls reply\n",
            "After tokenization: ['your', 'opinion', 'about', 'me', 'over', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'not', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
            "After stop word removal: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
            "After lemmatization: ['opinion', 'jada', 'kusruthi', 'lovable', 'silent', 'spl', 'character', 'matured', 'stylish', 'simple', 'pls', 'reply']\n",
            "After lowercasing: someonone you know is trying to contact you via our dating service! to find out who it could be call from your mobile or landline 09064015307 box334sk38ch \n",
            "After removing special chars: someonone you know is trying to contact you via our dating service to find out who it could be call from your mobile or landline  boxskch \n",
            "After tokenization: ['someonone', 'you', 'know', 'is', 'trying', 'to', 'contact', 'you', 'via', 'our', 'dating', 'service', 'to', 'find', 'out', 'who', 'it', 'could', 'be', 'call', 'from', 'your', 'mobile', 'or', 'landline', 'boxskch']\n",
            "After stop word removal: ['someonone', 'know', 'trying', 'contact', 'via', 'dating', 'service', 'find', 'could', 'call', 'mobile', 'landline', 'boxskch']\n",
            "After lemmatization: ['someonone', 'know', 'trying', 'contact', 'via', 'dating', 'service', 'find', 'could', 'call', 'mobile', 'landline', 'boxskch']\n",
            "After lowercasing: yeah i can still give you a ride\n",
            "After removing special chars: yeah i can still give you a ride\n",
            "After tokenization: ['yeah', 'i', 'can', 'still', 'give', 'you', 'a', 'ride']\n",
            "After stop word removal: ['yeah', 'still', 'give', 'ride']\n",
            "After lemmatization: ['yeah', 'still', 'give', 'ride']\n",
            "After lowercasing: jay wants to work out first, how's 4 sound?\n",
            "After removing special chars: jay wants to work out first hows  sound\n",
            "After tokenization: ['jay', 'wants', 'to', 'work', 'out', 'first', 'hows', 'sound']\n",
            "After stop word removal: ['jay', 'wants', 'work', 'first', 'hows', 'sound']\n",
            "After lemmatization: ['jay', 'want', 'work', 'first', 'hows', 'sound']\n",
            "After lowercasing: gud gud..k, chikku tke care.. sleep well gud nyt\n",
            "After removing special chars: gud gudk chikku tke care sleep well gud nyt\n",
            "After tokenization: ['gud', 'gudk', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
            "After stop word removal: ['gud', 'gudk', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
            "After lemmatization: ['gud', 'gudk', 'chikku', 'tke', 'care', 'sleep', 'well', 'gud', 'nyt']\n",
            "After lowercasing: its a part of checking iq\n",
            "After removing special chars: its a part of checking iq\n",
            "After tokenization: ['its', 'a', 'part', 'of', 'checking', 'iq']\n",
            "After stop word removal: ['part', 'checking', 'iq']\n",
            "After lemmatization: ['part', 'checking', 'iq']\n",
            "After lowercasing: hmm thinking lor...\n",
            "After removing special chars: hmm thinking lor\n",
            "After tokenization: ['hmm', 'thinking', 'lor']\n",
            "After stop word removal: ['hmm', 'thinking', 'lor']\n",
            "After lemmatization: ['hmm', 'thinking', 'lor']\n",
            "After lowercasing: of course ! don't tease me ... you know i simply must see ! *grins* ... do keep me posted my prey ... *loving smile* *devouring kiss*\n",
            "After removing special chars: of course  dont tease me  you know i simply must see  grins  do keep me posted my prey  loving smile devouring kiss\n",
            "After tokenization: ['of', 'course', 'dont', 'tease', 'me', 'you', 'know', 'i', 'simply', 'must', 'see', 'grins', 'do', 'keep', 'me', 'posted', 'my', 'prey', 'loving', 'smile', 'devouring', 'kiss']\n",
            "After stop word removal: ['course', 'dont', 'tease', 'know', 'simply', 'must', 'see', 'grins', 'keep', 'posted', 'prey', 'loving', 'smile', 'devouring', 'kiss']\n",
            "After lemmatization: ['course', 'dont', 'tease', 'know', 'simply', 'must', 'see', 'grin', 'keep', 'posted', 'prey', 'loving', 'smile', 'devouring', 'kiss']\n",
            "After lowercasing: thanks for the temales it was wonderful. thank. have a great week.\n",
            "After removing special chars: thanks for the temales it was wonderful thank have a great week\n",
            "After tokenization: ['thanks', 'for', 'the', 'temales', 'it', 'was', 'wonderful', 'thank', 'have', 'a', 'great', 'week']\n",
            "After stop word removal: ['thanks', 'temales', 'wonderful', 'thank', 'great', 'week']\n",
            "After lemmatization: ['thanks', 'temales', 'wonderful', 'thank', 'great', 'week']\n",
            "After lowercasing: thank you princess! i want to see your nice juicy booty...\n",
            "After removing special chars: thank you princess i want to see your nice juicy booty\n",
            "After tokenization: ['thank', 'you', 'princess', 'i', 'want', 'to', 'see', 'your', 'nice', 'juicy', 'booty']\n",
            "After stop word removal: ['thank', 'princess', 'want', 'see', 'nice', 'juicy', 'booty']\n",
            "After lemmatization: ['thank', 'princess', 'want', 'see', 'nice', 'juicy', 'booty']\n",
            "After lowercasing: haven't eaten all day. i'm sitting here staring at this juicy pizza and i can't eat it. these meds are ruining my life.\n",
            "After removing special chars: havent eaten all day im sitting here staring at this juicy pizza and i cant eat it these meds are ruining my life\n",
            "After tokenization: ['havent', 'eaten', 'all', 'day', 'im', 'sitting', 'here', 'staring', 'at', 'this', 'juicy', 'pizza', 'and', 'i', 'cant', 'eat', 'it', 'these', 'meds', 'are', 'ruining', 'my', 'life']\n",
            "After stop word removal: ['havent', 'eaten', 'day', 'im', 'sitting', 'staring', 'juicy', 'pizza', 'cant', 'eat', 'meds', 'ruining', 'life']\n",
            "After lemmatization: ['havent', 'eaten', 'day', 'im', 'sitting', 'staring', 'juicy', 'pizza', 'cant', 'eat', 'med', 'ruining', 'life']\n",
            "After lowercasing: gud ni8 dear..slp well..take care..swt dreams..muah..\n",
            "After removing special chars: gud ni dearslp welltake careswt dreamsmuah\n",
            "After tokenization: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After stop word removal: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After lemmatization: ['gud', 'ni', 'dearslp', 'welltake', 'careswt', 'dreamsmuah']\n",
            "After lowercasing: u come n search tat vid..not finishd..\n",
            "After removing special chars: u come n search tat vidnot finishd\n",
            "After tokenization: ['u', 'come', 'n', 'search', 'tat', 'vidnot', 'finishd']\n",
            "After stop word removal: ['u', 'come', 'n', 'search', 'tat', 'vidnot', 'finishd']\n",
            "After lemmatization: ['u', 'come', 'n', 'search', 'tat', 'vidnot', 'finishd']\n",
            "After lowercasing: k i'm leaving soon, be there a little after 9\n",
            "After removing special chars: k im leaving soon be there a little after \n",
            "After tokenization: ['k', 'im', 'leaving', 'soon', 'be', 'there', 'a', 'little', 'after']\n",
            "After stop word removal: ['k', 'im', 'leaving', 'soon', 'little']\n",
            "After lemmatization: ['k', 'im', 'leaving', 'soon', 'little']\n",
            "After lowercasing: urgent! please call 09061213237 from a landline. å£5000 cash or a 4* holiday await collection. t &cs sae po box 177 m227xy. 16+\n",
            "After removing special chars: urgent please call  from a landline  cash or a  holiday await collection t cs sae po box  mxy \n",
            "After tokenization: ['urgent', 'please', 'call', 'from', 'a', 'landline', 'cash', 'or', 'a', 'holiday', 'await', 'collection', 't', 'cs', 'sae', 'po', 'box', 'mxy']\n",
            "After stop word removal: ['urgent', 'please', 'call', 'landline', 'cash', 'holiday', 'await', 'collection', 'cs', 'sae', 'po', 'box', 'mxy']\n",
            "After lemmatization: ['urgent', 'please', 'call', 'landline', 'cash', 'holiday', 'await', 'collection', 'c', 'sae', 'po', 'box', 'mxy']\n",
            "After lowercasing: yeah work is fine, started last week, all the same stuff as before, dull but easy and guys are fun!\n",
            "After removing special chars: yeah work is fine started last week all the same stuff as before dull but easy and guys are fun\n",
            "After tokenization: ['yeah', 'work', 'is', 'fine', 'started', 'last', 'week', 'all', 'the', 'same', 'stuff', 'as', 'before', 'dull', 'but', 'easy', 'and', 'guys', 'are', 'fun']\n",
            "After stop word removal: ['yeah', 'work', 'fine', 'started', 'last', 'week', 'stuff', 'dull', 'easy', 'guys', 'fun']\n",
            "After lemmatization: ['yeah', 'work', 'fine', 'started', 'last', 'week', 'stuff', 'dull', 'easy', 'guy', 'fun']\n",
            "After lowercasing: you do your studies alone without anyones help. if you cant no need to study.\n",
            "After removing special chars: you do your studies alone without anyones help if you cant no need to study\n",
            "After tokenization: ['you', 'do', 'your', 'studies', 'alone', 'without', 'anyones', 'help', 'if', 'you', 'cant', 'no', 'need', 'to', 'study']\n",
            "After stop word removal: ['studies', 'alone', 'without', 'anyones', 'help', 'cant', 'need', 'study']\n",
            "After lemmatization: ['study', 'alone', 'without', 'anyones', 'help', 'cant', 'need', 'study']\n",
            "After lowercasing: please tell me not all of my car keys are in your purse\n",
            "After removing special chars: please tell me not all of my car keys are in your purse\n",
            "After tokenization: ['please', 'tell', 'me', 'not', 'all', 'of', 'my', 'car', 'keys', 'are', 'in', 'your', 'purse']\n",
            "After stop word removal: ['please', 'tell', 'car', 'keys', 'purse']\n",
            "After lemmatization: ['please', 'tell', 'car', 'key', 'purse']\n",
            "After lowercasing: i didnt get anything da\n",
            "After removing special chars: i didnt get anything da\n",
            "After tokenization: ['i', 'didnt', 'get', 'anything', 'da']\n",
            "After stop word removal: ['didnt', 'get', 'anything', 'da']\n",
            "After lemmatization: ['didnt', 'get', 'anything', 'da']\n",
            "After lowercasing: ok... sweet dreams...\n",
            "After removing special chars: ok sweet dreams\n",
            "After tokenization: ['ok', 'sweet', 'dreams']\n",
            "After stop word removal: ['ok', 'sweet', 'dreams']\n",
            "After lemmatization: ['ok', 'sweet', 'dream']\n",
            "After lowercasing: well she's in for a big surprise!\n",
            "After removing special chars: well shes in for a big surprise\n",
            "After tokenization: ['well', 'shes', 'in', 'for', 'a', 'big', 'surprise']\n",
            "After stop word removal: ['well', 'shes', 'big', 'surprise']\n",
            "After lemmatization: ['well', 'shes', 'big', 'surprise']\n",
            "After lowercasing: as usual..iam fine, happy &amp; doing well..:)\n",
            "After removing special chars: as usualiam fine happy amp doing well\n",
            "After tokenization: ['as', 'usualiam', 'fine', 'happy', 'amp', 'doing', 'well']\n",
            "After stop word removal: ['usualiam', 'fine', 'happy', 'amp', 'well']\n",
            "After lemmatization: ['usualiam', 'fine', 'happy', 'amp', 'well']\n",
            "After lowercasing: 1 in cbe. 2 in chennai.\n",
            "After removing special chars:  in cbe  in chennai\n",
            "After tokenization: ['in', 'cbe', 'in', 'chennai']\n",
            "After stop word removal: ['cbe', 'chennai']\n",
            "After lemmatization: ['cbe', 'chennai']\n",
            "After lowercasing: can help u swoop by picking u up from wherever ur other birds r meeting if u want.\n",
            "After removing special chars: can help u swoop by picking u up from wherever ur other birds r meeting if u want\n",
            "After tokenization: ['can', 'help', 'u', 'swoop', 'by', 'picking', 'u', 'up', 'from', 'wherever', 'ur', 'other', 'birds', 'r', 'meeting', 'if', 'u', 'want']\n",
            "After stop word removal: ['help', 'u', 'swoop', 'picking', 'u', 'wherever', 'ur', 'birds', 'r', 'meeting', 'u', 'want']\n",
            "After lemmatization: ['help', 'u', 'swoop', 'picking', 'u', 'wherever', 'ur', 'bird', 'r', 'meeting', 'u', 'want']\n",
            "After lowercasing: if anyone calls for a treadmill say you'll buy it. make sure its working. i found an ad on craigslist selling for $ &lt;#&gt; .\n",
            "After removing special chars: if anyone calls for a treadmill say youll buy it make sure its working i found an ad on craigslist selling for  ltgt \n",
            "After tokenization: ['if', 'anyone', 'calls', 'for', 'a', 'treadmill', 'say', 'youll', 'buy', 'it', 'make', 'sure', 'its', 'working', 'i', 'found', 'an', 'ad', 'on', 'craigslist', 'selling', 'for', 'ltgt']\n",
            "After stop word removal: ['anyone', 'calls', 'treadmill', 'say', 'youll', 'buy', 'make', 'sure', 'working', 'found', 'ad', 'craigslist', 'selling', 'ltgt']\n",
            "After lemmatization: ['anyone', 'call', 'treadmill', 'say', 'youll', 'buy', 'make', 'sure', 'working', 'found', 'ad', 'craigslist', 'selling', 'ltgt']\n",
            "After lowercasing: i absolutely love south park! i only recently started watching the office.\n",
            "After removing special chars: i absolutely love south park i only recently started watching the office\n",
            "After tokenization: ['i', 'absolutely', 'love', 'south', 'park', 'i', 'only', 'recently', 'started', 'watching', 'the', 'office']\n",
            "After stop word removal: ['absolutely', 'love', 'south', 'park', 'recently', 'started', 'watching', 'office']\n",
            "After lemmatization: ['absolutely', 'love', 'south', 'park', 'recently', 'started', 'watching', 'office']\n",
            "After lowercasing: did you see that film:)\n",
            "After removing special chars: did you see that film\n",
            "After tokenization: ['did', 'you', 'see', 'that', 'film']\n",
            "After stop word removal: ['see', 'film']\n",
            "After lemmatization: ['see', 'film']\n",
            "After lowercasing: pls speak with me. i wont ask anything other then you friendship.\n",
            "After removing special chars: pls speak with me i wont ask anything other then you friendship\n",
            "After tokenization: ['pls', 'speak', 'with', 'me', 'i', 'wont', 'ask', 'anything', 'other', 'then', 'you', 'friendship']\n",
            "After stop word removal: ['pls', 'speak', 'wont', 'ask', 'anything', 'friendship']\n",
            "After lemmatization: ['pls', 'speak', 'wont', 'ask', 'anything', 'friendship']\n",
            "After lowercasing: storming msg: wen u lift d phne, u say \\hello\\\" do u knw wt is d real meaning of hello?? . . . it's d name of a girl..! . . . yes.. and u knw who is dat girl?? \\\"margaret hello\\\" she is d girlfrnd f grahmbell who invnted telphone... . . . . moral:one can 4get d name of a person\n",
            "After removing special chars: storming msg wen u lift d phne u say hello do u knw wt is d real meaning of hello    its d name of a girl    yes and u knw who is dat girl margaret hello she is d girlfrnd f grahmbell who invnted telphone     moralone can get d name of a person\n",
            "After tokenization: ['storming', 'msg', 'wen', 'u', 'lift', 'd', 'phne', 'u', 'say', 'hello', 'do', 'u', 'knw', 'wt', 'is', 'd', 'real', 'meaning', 'of', 'hello', 'its', 'd', 'name', 'of', 'a', 'girl', 'yes', 'and', 'u', 'knw', 'who', 'is', 'dat', 'girl', 'margaret', 'hello', 'she', 'is', 'd', 'girlfrnd', 'f', 'grahmbell', 'who', 'invnted', 'telphone', 'moralone', 'can', 'get', 'd', 'name', 'of', 'a', 'person']\n",
            "After stop word removal: ['storming', 'msg', 'wen', 'u', 'lift', 'phne', 'u', 'say', 'hello', 'u', 'knw', 'wt', 'real', 'meaning', 'hello', 'name', 'girl', 'yes', 'u', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'f', 'grahmbell', 'invnted', 'telphone', 'moralone', 'get', 'name', 'person']\n",
            "After lemmatization: ['storming', 'msg', 'wen', 'u', 'lift', 'phne', 'u', 'say', 'hello', 'u', 'knw', 'wt', 'real', 'meaning', 'hello', 'name', 'girl', 'yes', 'u', 'knw', 'dat', 'girl', 'margaret', 'hello', 'girlfrnd', 'f', 'grahmbell', 'invnted', 'telphone', 'moralone', 'get', 'name', 'person']\n",
            "After lowercasing: gud ni8.swt drms.take care\n",
            "After removing special chars: gud niswt drmstake care\n",
            "After tokenization: ['gud', 'niswt', 'drmstake', 'care']\n",
            "After stop word removal: ['gud', 'niswt', 'drmstake', 'care']\n",
            "After lemmatization: ['gud', 'niswt', 'drmstake', 'care']\n",
            "After lowercasing: hi darlin its kate are u up for doin somethin tonight? im going to a pub called the swan or something with my parents for one drink so phone me if u can\n",
            "After removing special chars: hi darlin its kate are u up for doin somethin tonight im going to a pub called the swan or something with my parents for one drink so phone me if u can\n",
            "After tokenization: ['hi', 'darlin', 'its', 'kate', 'are', 'u', 'up', 'for', 'doin', 'somethin', 'tonight', 'im', 'going', 'to', 'a', 'pub', 'called', 'the', 'swan', 'or', 'something', 'with', 'my', 'parents', 'for', 'one', 'drink', 'so', 'phone', 'me', 'if', 'u', 'can']\n",
            "After stop word removal: ['hi', 'darlin', 'kate', 'u', 'doin', 'somethin', 'tonight', 'im', 'going', 'pub', 'called', 'swan', 'something', 'parents', 'one', 'drink', 'phone', 'u']\n",
            "After lemmatization: ['hi', 'darlin', 'kate', 'u', 'doin', 'somethin', 'tonight', 'im', 'going', 'pub', 'called', 'swan', 'something', 'parent', 'one', 'drink', 'phone', 'u']\n",
            "After lowercasing: anything lar then ì_ not going home 4 dinner?\n",
            "After removing special chars: anything lar then  not going home  dinner\n",
            "After tokenization: ['anything', 'lar', 'then', 'not', 'going', 'home', 'dinner']\n",
            "After stop word removal: ['anything', 'lar', 'going', 'home', 'dinner']\n",
            "After lemmatization: ['anything', 'lar', 'going', 'home', 'dinner']\n",
            "After lowercasing: \\er\n",
            "After removing special chars: er\n",
            "After tokenization: ['er']\n",
            "After stop word removal: ['er']\n",
            "After lemmatization: ['er']\n",
            "After lowercasing: if you don't, your prize will go to another customer. t&c at www.t-c.biz 18+ 150p/min polo ltd suite 373 london w1j 6hl please call back if busy \n",
            "After removing special chars: if you dont your prize will go to another customer tc at wwwtcbiz  pmin polo ltd suite  london wj hl please call back if busy \n",
            "After tokenization: ['if', 'you', 'dont', 'your', 'prize', 'will', 'go', 'to', 'another', 'customer', 'tc', 'at', 'wwwtcbiz', 'pmin', 'polo', 'ltd', 'suite', 'london', 'wj', 'hl', 'please', 'call', 'back', 'if', 'busy']\n",
            "After stop word removal: ['dont', 'prize', 'go', 'another', 'customer', 'tc', 'wwwtcbiz', 'pmin', 'polo', 'ltd', 'suite', 'london', 'wj', 'hl', 'please', 'call', 'back', 'busy']\n",
            "After lemmatization: ['dont', 'prize', 'go', 'another', 'customer', 'tc', 'wwwtcbiz', 'pmin', 'polo', 'ltd', 'suite', 'london', 'wj', 'hl', 'please', 'call', 'back', 'busy']\n",
            "After lowercasing: did u fix the teeth?if not do it asap.ok take care.\n",
            "After removing special chars: did u fix the teethif not do it asapok take care\n",
            "After tokenization: ['did', 'u', 'fix', 'the', 'teethif', 'not', 'do', 'it', 'asapok', 'take', 'care']\n",
            "After stop word removal: ['u', 'fix', 'teethif', 'asapok', 'take', 'care']\n",
            "After lemmatization: ['u', 'fix', 'teethif', 'asapok', 'take', 'care']\n",
            "After lowercasing: so u wan 2 come for our dinner tonight a not?\n",
            "After removing special chars: so u wan  come for our dinner tonight a not\n",
            "After tokenization: ['so', 'u', 'wan', 'come', 'for', 'our', 'dinner', 'tonight', 'a', 'not']\n",
            "After stop word removal: ['u', 'wan', 'come', 'dinner', 'tonight']\n",
            "After lemmatization: ['u', 'wan', 'come', 'dinner', 'tonight']\n",
            "After lowercasing: hello.how u doing?what u been up 2?when will u b moving out of the flat, cos i will need to arrange to pick up the lamp, etc. take care. hello caroline!\n",
            "After removing special chars: hellohow u doingwhat u been up when will u b moving out of the flat cos i will need to arrange to pick up the lamp etc take care hello caroline\n",
            "After tokenization: ['hellohow', 'u', 'doingwhat', 'u', 'been', 'up', 'when', 'will', 'u', 'b', 'moving', 'out', 'of', 'the', 'flat', 'cos', 'i', 'will', 'need', 'to', 'arrange', 'to', 'pick', 'up', 'the', 'lamp', 'etc', 'take', 'care', 'hello', 'caroline']\n",
            "After stop word removal: ['hellohow', 'u', 'doingwhat', 'u', 'u', 'b', 'moving', 'flat', 'cos', 'need', 'arrange', 'pick', 'lamp', 'etc', 'take', 'care', 'hello', 'caroline']\n",
            "After lemmatization: ['hellohow', 'u', 'doingwhat', 'u', 'u', 'b', 'moving', 'flat', 'co', 'need', 'arrange', 'pick', 'lamp', 'etc', 'take', 'care', 'hello', 'caroline']\n",
            "After lowercasing: its too late:)but its k.wish you the same.\n",
            "After removing special chars: its too latebut its kwish you the same\n",
            "After tokenization: ['its', 'too', 'latebut', 'its', 'kwish', 'you', 'the', 'same']\n",
            "After stop word removal: ['latebut', 'kwish']\n",
            "After lemmatization: ['latebut', 'kwish']\n",
            "After lowercasing: hi. hope ur day * good! back from walk, table booked for half eight. let me know when ur coming over.\n",
            "After removing special chars: hi hope ur day  good back from walk table booked for half eight let me know when ur coming over\n",
            "After tokenization: ['hi', 'hope', 'ur', 'day', 'good', 'back', 'from', 'walk', 'table', 'booked', 'for', 'half', 'eight', 'let', 'me', 'know', 'when', 'ur', 'coming', 'over']\n",
            "After stop word removal: ['hi', 'hope', 'ur', 'day', 'good', 'back', 'walk', 'table', 'booked', 'half', 'eight', 'let', 'know', 'ur', 'coming']\n",
            "After lemmatization: ['hi', 'hope', 'ur', 'day', 'good', 'back', 'walk', 'table', 'booked', 'half', 'eight', 'let', 'know', 'ur', 'coming']\n",
            "After lowercasing: oh yeah clearly it's my fault\n",
            "After removing special chars: oh yeah clearly its my fault\n",
            "After tokenization: ['oh', 'yeah', 'clearly', 'its', 'my', 'fault']\n",
            "After stop word removal: ['oh', 'yeah', 'clearly', 'fault']\n",
            "After lemmatization: ['oh', 'yeah', 'clearly', 'fault']\n",
            "After lowercasing: dunno leh cant remember mayb lor. so wat time r we meeting tmr?\n",
            "After removing special chars: dunno leh cant remember mayb lor so wat time r we meeting tmr\n",
            "After tokenization: ['dunno', 'leh', 'cant', 'remember', 'mayb', 'lor', 'so', 'wat', 'time', 'r', 'we', 'meeting', 'tmr']\n",
            "After stop word removal: ['dunno', 'leh', 'cant', 'remember', 'mayb', 'lor', 'wat', 'time', 'r', 'meeting', 'tmr']\n",
            "After lemmatization: ['dunno', 'leh', 'cant', 'remember', 'mayb', 'lor', 'wat', 'time', 'r', 'meeting', 'tmr']\n",
            "After lowercasing: best msg: it's hard to be with a person, when u know that one more step foward will make u fall in love.. &amp; one step back can ruin ur friendship.. good night:-) ...\n",
            "After removing special chars: best msg its hard to be with a person when u know that one more step foward will make u fall in love amp one step back can ruin ur friendship good night \n",
            "After tokenization: ['best', 'msg', 'its', 'hard', 'to', 'be', 'with', 'a', 'person', 'when', 'u', 'know', 'that', 'one', 'more', 'step', 'foward', 'will', 'make', 'u', 'fall', 'in', 'love', 'amp', 'one', 'step', 'back', 'can', 'ruin', 'ur', 'friendship', 'good', 'night']\n",
            "After stop word removal: ['best', 'msg', 'hard', 'person', 'u', 'know', 'one', 'step', 'foward', 'make', 'u', 'fall', 'love', 'amp', 'one', 'step', 'back', 'ruin', 'ur', 'friendship', 'good', 'night']\n",
            "After lemmatization: ['best', 'msg', 'hard', 'person', 'u', 'know', 'one', 'step', 'foward', 'make', 'u', 'fall', 'love', 'amp', 'one', 'step', 'back', 'ruin', 'ur', 'friendship', 'good', 'night']\n",
            "After lowercasing: urgent! your mobile number has been awarded with a å£2000 prize guaranteed. call 09061790126 from land line. claim 3030. valid 12hrs only 150ppm\n",
            "After removing special chars: urgent your mobile number has been awarded with a  prize guaranteed call  from land line claim  valid hrs only ppm\n",
            "After tokenization: ['urgent', 'your', 'mobile', 'number', 'has', 'been', 'awarded', 'with', 'a', 'prize', 'guaranteed', 'call', 'from', 'land', 'line', 'claim', 'valid', 'hrs', 'only', 'ppm']\n",
            "After stop word removal: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hrs', 'ppm']\n",
            "After lemmatization: ['urgent', 'mobile', 'number', 'awarded', 'prize', 'guaranteed', 'call', 'land', 'line', 'claim', 'valid', 'hr', 'ppm']\n",
            "After lowercasing: helloooo... wake up..! \\sweet\\\" \\\"morning\\\" \\\"welcomes\\\" \\\"you\\\" \\\"enjoy\\\" \\\"this day\\\" \\\"with full of joy\\\".. \\\"gud mrng\\\".\"\n",
            "After removing special chars: helloooo wake up sweet morning welcomes you enjoy this day with full of joy gud mrng\n",
            "After tokenization: ['helloooo', 'wake', 'up', 'sweet', 'morning', 'welcomes', 'you', 'enjoy', 'this', 'day', 'with', 'full', 'of', 'joy', 'gud', 'mrng']\n",
            "After stop word removal: ['helloooo', 'wake', 'sweet', 'morning', 'welcomes', 'enjoy', 'day', 'full', 'joy', 'gud', 'mrng']\n",
            "After lemmatization: ['helloooo', 'wake', 'sweet', 'morning', 'welcome', 'enjoy', 'day', 'full', 'joy', 'gud', 'mrng']\n",
            "After lowercasing: vikky, come around  &lt;time&gt; ..\n",
            "After removing special chars: vikky come around  lttimegt \n",
            "After tokenization: ['vikky', 'come', 'around', 'lttimegt']\n",
            "After stop word removal: ['vikky', 'come', 'around', 'lttimegt']\n",
            "After lemmatization: ['vikky', 'come', 'around', 'lttimegt']\n",
            "After lowercasing: and how you will do that, princess? :)\n",
            "After removing special chars: and how you will do that princess \n",
            "After tokenization: ['and', 'how', 'you', 'will', 'do', 'that', 'princess']\n",
            "After stop word removal: ['princess']\n",
            "After lemmatization: ['princess']\n",
            "After lowercasing: i have gone into get info bt dont know what to do\n",
            "After removing special chars: i have gone into get info bt dont know what to do\n",
            "After tokenization: ['i', 'have', 'gone', 'into', 'get', 'info', 'bt', 'dont', 'know', 'what', 'to', 'do']\n",
            "After stop word removal: ['gone', 'get', 'info', 'bt', 'dont', 'know']\n",
            "After lemmatization: ['gone', 'get', 'info', 'bt', 'dont', 'know']\n",
            "After lowercasing: yeah, probably here for a while\n",
            "After removing special chars: yeah probably here for a while\n",
            "After tokenization: ['yeah', 'probably', 'here', 'for', 'a', 'while']\n",
            "After stop word removal: ['yeah', 'probably']\n",
            "After lemmatization: ['yeah', 'probably']\n",
            "After lowercasing: sent me ur email id soon\n",
            "After removing special chars: sent me ur email id soon\n",
            "After tokenization: ['sent', 'me', 'ur', 'email', 'id', 'soon']\n",
            "After stop word removal: ['sent', 'ur', 'email', 'id', 'soon']\n",
            "After lemmatization: ['sent', 'ur', 'email', 'id', 'soon']\n",
            "After lowercasing: urgent! you have won a 1 week free membership in our å£100,000 prize jackpot! txt the word: claim to no: 81010 t&c www.dbuk.net lccltd pobox 4403ldnw1a7rw18\n",
            "After removing special chars: urgent you have won a  week free membership in our  prize jackpot txt the word claim to no  tc wwwdbuknet lccltd pobox ldnwarw\n",
            "After tokenization: ['urgent', 'you', 'have', 'won', 'a', 'week', 'free', 'membership', 'in', 'our', 'prize', 'jackpot', 'txt', 'the', 'word', 'claim', 'to', 'no', 'tc', 'wwwdbuknet', 'lccltd', 'pobox', 'ldnwarw']\n",
            "After stop word removal: ['urgent', 'week', 'free', 'membership', 'prize', 'jackpot', 'txt', 'word', 'claim', 'tc', 'wwwdbuknet', 'lccltd', 'pobox', 'ldnwarw']\n",
            "After lemmatization: ['urgent', 'week', 'free', 'membership', 'prize', 'jackpot', 'txt', 'word', 'claim', 'tc', 'wwwdbuknet', 'lccltd', 'pobox', 'ldnwarw']\n",
            "After lowercasing: i'm still pretty weak today .. bad day ?\n",
            "After removing special chars: im still pretty weak today  bad day \n",
            "After tokenization: ['im', 'still', 'pretty', 'weak', 'today', 'bad', 'day']\n",
            "After stop word removal: ['im', 'still', 'pretty', 'weak', 'today', 'bad', 'day']\n",
            "After lemmatization: ['im', 'still', 'pretty', 'weak', 'today', 'bad', 'day']\n",
            "After lowercasing: hey ! don't forget ... you are mine ... for me ... my possession ... my property ... mmm ... *childish smile* ...\n",
            "After removing special chars: hey  dont forget  you are mine  for me  my possession  my property  mmm  childish smile \n",
            "After tokenization: ['hey', 'dont', 'forget', 'you', 'are', 'mine', 'for', 'me', 'my', 'possession', 'my', 'property', 'mmm', 'childish', 'smile']\n",
            "After stop word removal: ['hey', 'dont', 'forget', 'mine', 'possession', 'property', 'mmm', 'childish', 'smile']\n",
            "After lemmatization: ['hey', 'dont', 'forget', 'mine', 'possession', 'property', 'mmm', 'childish', 'smile']\n",
            "After lowercasing: an excellent thought by a misundrstud frnd: i knw u hate me bt the day wen u'll knw the truth u'll hate urself:-( gn:-)\n",
            "After removing special chars: an excellent thought by a misundrstud frnd i knw u hate me bt the day wen ull knw the truth ull hate urself gn\n",
            "After tokenization: ['an', 'excellent', 'thought', 'by', 'a', 'misundrstud', 'frnd', 'i', 'knw', 'u', 'hate', 'me', 'bt', 'the', 'day', 'wen', 'ull', 'knw', 'the', 'truth', 'ull', 'hate', 'urself', 'gn']\n",
            "After stop word removal: ['excellent', 'thought', 'misundrstud', 'frnd', 'knw', 'u', 'hate', 'bt', 'day', 'wen', 'ull', 'knw', 'truth', 'ull', 'hate', 'urself', 'gn']\n",
            "After lemmatization: ['excellent', 'thought', 'misundrstud', 'frnd', 'knw', 'u', 'hate', 'bt', 'day', 'wen', 'ull', 'knw', 'truth', 'ull', 'hate', 'urself', 'gn']\n",
            "After lowercasing: hey! congrats 2u2. id luv 2 but ive had 2 go home!\n",
            "After removing special chars: hey congrats u id luv  but ive had  go home\n",
            "After tokenization: ['hey', 'congrats', 'u', 'id', 'luv', 'but', 'ive', 'had', 'go', 'home']\n",
            "After stop word removal: ['hey', 'congrats', 'u', 'id', 'luv', 'ive', 'go', 'home']\n",
            "After lemmatization: ['hey', 'congrats', 'u', 'id', 'luv', 'ive', 'go', 'home']\n",
            "After lowercasing: dear where you. call me\n",
            "After removing special chars: dear where you call me\n",
            "After tokenization: ['dear', 'where', 'you', 'call', 'me']\n",
            "After stop word removal: ['dear', 'call']\n",
            "After lemmatization: ['dear', 'call']\n",
            "After lowercasing: xy trying smth now. u eat already? we havent...\n",
            "After removing special chars: xy trying smth now u eat already we havent\n",
            "After tokenization: ['xy', 'trying', 'smth', 'now', 'u', 'eat', 'already', 'we', 'havent']\n",
            "After stop word removal: ['xy', 'trying', 'smth', 'u', 'eat', 'already', 'havent']\n",
            "After lemmatization: ['xy', 'trying', 'smth', 'u', 'eat', 'already', 'havent']\n",
            "After lowercasing: urgent! please call 09061213237 from landline. å£5000 cash or a luxury 4* canary islands holiday await collection. t&cs sae po box 177. m227xy. 150ppm. 16+\n",
            "After removing special chars: urgent please call  from landline  cash or a luxury  canary islands holiday await collection tcs sae po box  mxy ppm \n",
            "After tokenization: ['urgent', 'please', 'call', 'from', 'landline', 'cash', 'or', 'a', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 'tcs', 'sae', 'po', 'box', 'mxy', 'ppm']\n",
            "After stop word removal: ['urgent', 'please', 'call', 'landline', 'cash', 'luxury', 'canary', 'islands', 'holiday', 'await', 'collection', 'tcs', 'sae', 'po', 'box', 'mxy', 'ppm']\n",
            "After lemmatization: ['urgent', 'please', 'call', 'landline', 'cash', 'luxury', 'canary', 'island', 'holiday', 'await', 'collection', 'tc', 'sae', 'po', 'box', 'mxy', 'ppm']\n",
            "After lowercasing: i donno its in your genes or something\n",
            "After removing special chars: i donno its in your genes or something\n",
            "After tokenization: ['i', 'donno', 'its', 'in', 'your', 'genes', 'or', 'something']\n",
            "After stop word removal: ['donno', 'genes', 'something']\n",
            "After lemmatization: ['donno', 'gene', 'something']\n",
            "After lowercasing: xmas iscoming & ur awarded either å£500 cd gift vouchers & free entry 2 r å£100 weekly draw txt music to 87066 tnc www.ldew.com1win150ppmx3age16subscription \n",
            "After removing special chars: xmas iscoming  ur awarded either  cd gift vouchers  free entry  r  weekly draw txt music to  tnc wwwldewcomwinppmxagesubscription \n",
            "After tokenization: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'to', 'tnc', 'wwwldewcomwinppmxagesubscription']\n",
            "After stop word removal: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'vouchers', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'tnc', 'wwwldewcomwinppmxagesubscription']\n",
            "After lemmatization: ['xmas', 'iscoming', 'ur', 'awarded', 'either', 'cd', 'gift', 'voucher', 'free', 'entry', 'r', 'weekly', 'draw', 'txt', 'music', 'tnc', 'wwwldewcomwinppmxagesubscription']\n",
            "After lowercasing: alex says he's not ok with you not being ok with it\n",
            "After removing special chars: alex says hes not ok with you not being ok with it\n",
            "After tokenization: ['alex', 'says', 'hes', 'not', 'ok', 'with', 'you', 'not', 'being', 'ok', 'with', 'it']\n",
            "After stop word removal: ['alex', 'says', 'hes', 'ok', 'ok']\n",
            "After lemmatization: ['alex', 'say', 'he', 'ok', 'ok']\n",
            "After lowercasing: are u coming to the funeral home\n",
            "After removing special chars: are u coming to the funeral home\n",
            "After tokenization: ['are', 'u', 'coming', 'to', 'the', 'funeral', 'home']\n",
            "After stop word removal: ['u', 'coming', 'funeral', 'home']\n",
            "After lemmatization: ['u', 'coming', 'funeral', 'home']\n",
            "After lowercasing: my darling sister. how are you doing. when's school resuming. is there a minimum wait period before you reapply? do take care\n",
            "After removing special chars: my darling sister how are you doing whens school resuming is there a minimum wait period before you reapply do take care\n",
            "After tokenization: ['my', 'darling', 'sister', 'how', 'are', 'you', 'doing', 'whens', 'school', 'resuming', 'is', 'there', 'a', 'minimum', 'wait', 'period', 'before', 'you', 'reapply', 'do', 'take', 'care']\n",
            "After stop word removal: ['darling', 'sister', 'whens', 'school', 'resuming', 'minimum', 'wait', 'period', 'reapply', 'take', 'care']\n",
            "After lemmatization: ['darling', 'sister', 'whens', 'school', 'resuming', 'minimum', 'wait', 'period', 'reapply', 'take', 'care']\n",
            "After lowercasing: i.ll hand her my phone to chat wit u\n",
            "After removing special chars: ill hand her my phone to chat wit u\n",
            "After tokenization: ['ill', 'hand', 'her', 'my', 'phone', 'to', 'chat', 'wit', 'u']\n",
            "After stop word removal: ['ill', 'hand', 'phone', 'chat', 'wit', 'u']\n",
            "After lemmatization: ['ill', 'hand', 'phone', 'chat', 'wit', 'u']\n",
            "After lowercasing: well good morning mr . hows london treatin' ya treacle?\n",
            "After removing special chars: well good morning mr  hows london treatin ya treacle\n",
            "After tokenization: ['well', 'good', 'morning', 'mr', 'hows', 'london', 'treatin', 'ya', 'treacle']\n",
            "After stop word removal: ['well', 'good', 'morning', 'mr', 'hows', 'london', 'treatin', 'ya', 'treacle']\n",
            "After lemmatization: ['well', 'good', 'morning', 'mr', 'hows', 'london', 'treatin', 'ya', 'treacle']\n",
            "After lowercasing: i can't make it tonight\n",
            "After removing special chars: i cant make it tonight\n",
            "After tokenization: ['i', 'cant', 'make', 'it', 'tonight']\n",
            "After stop word removal: ['cant', 'make', 'tonight']\n",
            "After lemmatization: ['cant', 'make', 'tonight']\n",
            "After lowercasing: at what time should i come tomorrow\n",
            "After removing special chars: at what time should i come tomorrow\n",
            "After tokenization: ['at', 'what', 'time', 'should', 'i', 'come', 'tomorrow']\n",
            "After stop word removal: ['time', 'come', 'tomorrow']\n",
            "After lemmatization: ['time', 'come', 'tomorrow']\n",
            "After lowercasing: about  &lt;#&gt; bucks. the banks fees are fixed. better to call the bank and find out.\n",
            "After removing special chars: about  ltgt bucks the banks fees are fixed better to call the bank and find out\n",
            "After tokenization: ['about', 'ltgt', 'bucks', 'the', 'banks', 'fees', 'are', 'fixed', 'better', 'to', 'call', 'the', 'bank', 'and', 'find', 'out']\n",
            "After stop word removal: ['ltgt', 'bucks', 'banks', 'fees', 'fixed', 'better', 'call', 'bank', 'find']\n",
            "After lemmatization: ['ltgt', 'buck', 'bank', 'fee', 'fixed', 'better', 'call', 'bank', 'find']\n",
            "After lowercasing: i can. but it will tell quite long, cos i haven't finish my film yet...\n",
            "After removing special chars: i can but it will tell quite long cos i havent finish my film yet\n",
            "After tokenization: ['i', 'can', 'but', 'it', 'will', 'tell', 'quite', 'long', 'cos', 'i', 'havent', 'finish', 'my', 'film', 'yet']\n",
            "After stop word removal: ['tell', 'quite', 'long', 'cos', 'havent', 'finish', 'film', 'yet']\n",
            "After lemmatization: ['tell', 'quite', 'long', 'co', 'havent', 'finish', 'film', 'yet']\n",
            "After lowercasing: pls ask macho how much is budget for bb bold 2 is cos i saw a new one for  &lt;#&gt;  dollars.\n",
            "After removing special chars: pls ask macho how much is budget for bb bold  is cos i saw a new one for  ltgt  dollars\n",
            "After tokenization: ['pls', 'ask', 'macho', 'how', 'much', 'is', 'budget', 'for', 'bb', 'bold', 'is', 'cos', 'i', 'saw', 'a', 'new', 'one', 'for', 'ltgt', 'dollars']\n",
            "After stop word removal: ['pls', 'ask', 'macho', 'much', 'budget', 'bb', 'bold', 'cos', 'saw', 'new', 'one', 'ltgt', 'dollars']\n",
            "After lemmatization: ['pls', 'ask', 'macho', 'much', 'budget', 'bb', 'bold', 'co', 'saw', 'new', 'one', 'ltgt', 'dollar']\n",
            "After lowercasing: \\hi missed your call and my mumhas beendropping red wine all over theplace! what is your adress?\\\"\"\n",
            "After removing special chars: hi missed your call and my mumhas beendropping red wine all over theplace what is your adress\n",
            "After tokenization: ['hi', 'missed', 'your', 'call', 'and', 'my', 'mumhas', 'beendropping', 'red', 'wine', 'all', 'over', 'theplace', 'what', 'is', 'your', 'adress']\n",
            "After stop word removal: ['hi', 'missed', 'call', 'mumhas', 'beendropping', 'red', 'wine', 'theplace', 'adress']\n",
            "After lemmatization: ['hi', 'missed', 'call', 'mumhas', 'beendropping', 'red', 'wine', 'theplace', 'adress']\n",
            "After lowercasing: ill be at yours in about 3 mins but look out for me\n",
            "After removing special chars: ill be at yours in about  mins but look out for me\n",
            "After tokenization: ['ill', 'be', 'at', 'yours', 'in', 'about', 'mins', 'but', 'look', 'out', 'for', 'me']\n",
            "After stop word removal: ['ill', 'mins', 'look']\n",
            "After lemmatization: ['ill', 'min', 'look']\n",
            "After lowercasing: what you did in  leave.\n",
            "After removing special chars: what you did in  leave\n",
            "After tokenization: ['what', 'you', 'did', 'in', 'leave']\n",
            "After stop word removal: ['leave']\n",
            "After lemmatization: ['leave']\n",
            "After lowercasing: i'm coming back on thursday. yay. is it gonna be ok to get the money. cheers. oh yeah and how are you. everything alright. hows school. or do you call it work now\n",
            "After removing special chars: im coming back on thursday yay is it gonna be ok to get the money cheers oh yeah and how are you everything alright hows school or do you call it work now\n",
            "After tokenization: ['im', 'coming', 'back', 'on', 'thursday', 'yay', 'is', 'it', 'gon', 'na', 'be', 'ok', 'to', 'get', 'the', 'money', 'cheers', 'oh', 'yeah', 'and', 'how', 'are', 'you', 'everything', 'alright', 'hows', 'school', 'or', 'do', 'you', 'call', 'it', 'work', 'now']\n",
            "After stop word removal: ['im', 'coming', 'back', 'thursday', 'yay', 'gon', 'na', 'ok', 'get', 'money', 'cheers', 'oh', 'yeah', 'everything', 'alright', 'hows', 'school', 'call', 'work']\n",
            "After lemmatization: ['im', 'coming', 'back', 'thursday', 'yay', 'gon', 'na', 'ok', 'get', 'money', 'cheer', 'oh', 'yeah', 'everything', 'alright', 'hows', 'school', 'call', 'work']\n",
            "After lowercasing: jolly good! by the way,  will give u tickets for sat eve 7.30. speak before then x\n",
            "After removing special chars: jolly good by the way  will give u tickets for sat eve  speak before then x\n",
            "After tokenization: ['jolly', 'good', 'by', 'the', 'way', 'will', 'give', 'u', 'tickets', 'for', 'sat', 'eve', 'speak', 'before', 'then', 'x']\n",
            "After stop word removal: ['jolly', 'good', 'way', 'give', 'u', 'tickets', 'sat', 'eve', 'speak', 'x']\n",
            "After lemmatization: ['jolly', 'good', 'way', 'give', 'u', 'ticket', 'sat', 'eve', 'speak', 'x']\n",
            "After lowercasing: yeah, that's what i was thinking\n",
            "After removing special chars: yeah thats what i was thinking\n",
            "After tokenization: ['yeah', 'thats', 'what', 'i', 'was', 'thinking']\n",
            "After stop word removal: ['yeah', 'thats', 'thinking']\n",
            "After lemmatization: ['yeah', 'thats', 'thinking']\n",
            "After lowercasing: k.k:)i'm going to tirunelvali this week to see my uncle ..i already spend the amount by taking dress .so only i want money.i will give it on feb 1\n",
            "After removing special chars: kkim going to tirunelvali this week to see my uncle i already spend the amount by taking dress so only i want moneyi will give it on feb \n",
            "After tokenization: ['kkim', 'going', 'to', 'tirunelvali', 'this', 'week', 'to', 'see', 'my', 'uncle', 'i', 'already', 'spend', 'the', 'amount', 'by', 'taking', 'dress', 'so', 'only', 'i', 'want', 'moneyi', 'will', 'give', 'it', 'on', 'feb']\n",
            "After stop word removal: ['kkim', 'going', 'tirunelvali', 'week', 'see', 'uncle', 'already', 'spend', 'amount', 'taking', 'dress', 'want', 'moneyi', 'give', 'feb']\n",
            "After lemmatization: ['kkim', 'going', 'tirunelvali', 'week', 'see', 'uncle', 'already', 'spend', 'amount', 'taking', 'dress', 'want', 'moneyi', 'give', 'feb']\n",
            "After lowercasing: here got ur favorite oyster... n got my favorite sashimi... ok lar i dun say already... wait ur stomach start rumbling...\n",
            "After removing special chars: here got ur favorite oyster n got my favorite sashimi ok lar i dun say already wait ur stomach start rumbling\n",
            "After tokenization: ['here', 'got', 'ur', 'favorite', 'oyster', 'n', 'got', 'my', 'favorite', 'sashimi', 'ok', 'lar', 'i', 'dun', 'say', 'already', 'wait', 'ur', 'stomach', 'start', 'rumbling']\n",
            "After stop word removal: ['got', 'ur', 'favorite', 'oyster', 'n', 'got', 'favorite', 'sashimi', 'ok', 'lar', 'dun', 'say', 'already', 'wait', 'ur', 'stomach', 'start', 'rumbling']\n",
            "After lemmatization: ['got', 'ur', 'favorite', 'oyster', 'n', 'got', 'favorite', 'sashimi', 'ok', 'lar', 'dun', 'say', 'already', 'wait', 'ur', 'stomach', 'start', 'rumbling']\n",
            "After lowercasing: my sister going to earn more than me da.\n",
            "After removing special chars: my sister going to earn more than me da\n",
            "After tokenization: ['my', 'sister', 'going', 'to', 'earn', 'more', 'than', 'me', 'da']\n",
            "After stop word removal: ['sister', 'going', 'earn', 'da']\n",
            "After lemmatization: ['sister', 'going', 'earn', 'da']\n",
            "After lowercasing: get the official england poly ringtone or colour flag on yer mobile for tonights game! text tone or flag to 84199. optout txt eng stop box39822 w111wx å£1.50\n",
            "After removing special chars: get the official england poly ringtone or colour flag on yer mobile for tonights game text tone or flag to  optout txt eng stop box wwx \n",
            "After tokenization: ['get', 'the', 'official', 'england', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', 'for', 'tonights', 'game', 'text', 'tone', 'or', 'flag', 'to', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After stop word removal: ['get', 'official', 'england', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'tonights', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After lemmatization: ['get', 'official', 'england', 'poly', 'ringtone', 'colour', 'flag', 'yer', 'mobile', 'tonight', 'game', 'text', 'tone', 'flag', 'optout', 'txt', 'eng', 'stop', 'box', 'wwx']\n",
            "After lowercasing: hahaha..use your brain dear\n",
            "After removing special chars: hahahause your brain dear\n",
            "After tokenization: ['hahahause', 'your', 'brain', 'dear']\n",
            "After stop word removal: ['hahahause', 'brain', 'dear']\n",
            "After lemmatization: ['hahahause', 'brain', 'dear']\n",
            "After lowercasing: jus finish watching tv... u?\n",
            "After removing special chars: jus finish watching tv u\n",
            "After tokenization: ['jus', 'finish', 'watching', 'tv', 'u']\n",
            "After stop word removal: ['jus', 'finish', 'watching', 'tv', 'u']\n",
            "After lemmatization: ['jus', 'finish', 'watching', 'tv', 'u']\n",
            "After lowercasing: k, fyi i'm back in my parents' place in south tampa so i might need to do the deal somewhere else\n",
            "After removing special chars: k fyi im back in my parents place in south tampa so i might need to do the deal somewhere else\n",
            "After tokenization: ['k', 'fyi', 'im', 'back', 'in', 'my', 'parents', 'place', 'in', 'south', 'tampa', 'so', 'i', 'might', 'need', 'to', 'do', 'the', 'deal', 'somewhere', 'else']\n",
            "After stop word removal: ['k', 'fyi', 'im', 'back', 'parents', 'place', 'south', 'tampa', 'might', 'need', 'deal', 'somewhere', 'else']\n",
            "After lemmatization: ['k', 'fyi', 'im', 'back', 'parent', 'place', 'south', 'tampa', 'might', 'need', 'deal', 'somewhere', 'else']\n",
            "After lowercasing: good morning, my love ... i go to sleep now and wish you a great day full of feeling better and opportunity ... you are my last thought babe, i love you *kiss*\n",
            "After removing special chars: good morning my love  i go to sleep now and wish you a great day full of feeling better and opportunity  you are my last thought babe i love you kiss\n",
            "After tokenization: ['good', 'morning', 'my', 'love', 'i', 'go', 'to', 'sleep', 'now', 'and', 'wish', 'you', 'a', 'great', 'day', 'full', 'of', 'feeling', 'better', 'and', 'opportunity', 'you', 'are', 'my', 'last', 'thought', 'babe', 'i', 'love', 'you', 'kiss']\n",
            "After stop word removal: ['good', 'morning', 'love', 'go', 'sleep', 'wish', 'great', 'day', 'full', 'feeling', 'better', 'opportunity', 'last', 'thought', 'babe', 'love', 'kiss']\n",
            "After lemmatization: ['good', 'morning', 'love', 'go', 'sleep', 'wish', 'great', 'day', 'full', 'feeling', 'better', 'opportunity', 'last', 'thought', 'babe', 'love', 'kiss']\n",
            "After lowercasing: kothi print out marandratha.\n",
            "After removing special chars: kothi print out marandratha\n",
            "After tokenization: ['kothi', 'print', 'out', 'marandratha']\n",
            "After stop word removal: ['kothi', 'print', 'marandratha']\n",
            "After lemmatization: ['kothi', 'print', 'marandratha']\n",
            "After lowercasing: but we havent got da topic yet rite?\n",
            "After removing special chars: but we havent got da topic yet rite\n",
            "After tokenization: ['but', 'we', 'havent', 'got', 'da', 'topic', 'yet', 'rite']\n",
            "After stop word removal: ['havent', 'got', 'da', 'topic', 'yet', 'rite']\n",
            "After lemmatization: ['havent', 'got', 'da', 'topic', 'yet', 'rite']\n",
            "After lowercasing: ok no problem... yup i'm going to sch at 4 if i rem correctly...\n",
            "After removing special chars: ok no problem yup im going to sch at  if i rem correctly\n",
            "After tokenization: ['ok', 'no', 'problem', 'yup', 'im', 'going', 'to', 'sch', 'at', 'if', 'i', 'rem', 'correctly']\n",
            "After stop word removal: ['ok', 'problem', 'yup', 'im', 'going', 'sch', 'rem', 'correctly']\n",
            "After lemmatization: ['ok', 'problem', 'yup', 'im', 'going', 'sch', 'rem', 'correctly']\n",
            "After lowercasing: thanks, i'll keep that in mind\n",
            "After removing special chars: thanks ill keep that in mind\n",
            "After tokenization: ['thanks', 'ill', 'keep', 'that', 'in', 'mind']\n",
            "After stop word removal: ['thanks', 'ill', 'keep', 'mind']\n",
            "After lemmatization: ['thanks', 'ill', 'keep', 'mind']\n",
            "After lowercasing: aah bless! how's your arm?\n",
            "After removing special chars: aah bless hows your arm\n",
            "After tokenization: ['aah', 'bless', 'hows', 'your', 'arm']\n",
            "After stop word removal: ['aah', 'bless', 'hows', 'arm']\n",
            "After lemmatization: ['aah', 'bless', 'hows', 'arm']\n",
            "After lowercasing: dear sir,salam alaikkum.pride and pleasure meeting you today at the tea shop.we are pleased to send you our contact number at qatar.rakhesh an indian.pls save our number.respectful regards.\n",
            "After removing special chars: dear sirsalam alaikkumpride and pleasure meeting you today at the tea shopwe are pleased to send you our contact number at qatarrakhesh an indianpls save our numberrespectful regards\n",
            "After tokenization: ['dear', 'sirsalam', 'alaikkumpride', 'and', 'pleasure', 'meeting', 'you', 'today', 'at', 'the', 'tea', 'shopwe', 'are', 'pleased', 'to', 'send', 'you', 'our', 'contact', 'number', 'at', 'qatarrakhesh', 'an', 'indianpls', 'save', 'our', 'numberrespectful', 'regards']\n",
            "After stop word removal: ['dear', 'sirsalam', 'alaikkumpride', 'pleasure', 'meeting', 'today', 'tea', 'shopwe', 'pleased', 'send', 'contact', 'number', 'qatarrakhesh', 'indianpls', 'save', 'numberrespectful', 'regards']\n",
            "After lemmatization: ['dear', 'sirsalam', 'alaikkumpride', 'pleasure', 'meeting', 'today', 'tea', 'shopwe', 'pleased', 'send', 'contact', 'number', 'qatarrakhesh', 'indianpls', 'save', 'numberrespectful', 'regard']\n",
            "After lowercasing: gal n boy walking in d park. gal-can i hold ur hand? boy-y? do u think i would run away? gal-no, jst wana c how it feels walking in heaven with an prince..gn:-)\n",
            "After removing special chars: gal n boy walking in d park galcan i hold ur hand boyy do u think i would run away galno jst wana c how it feels walking in heaven with an princegn\n",
            "After tokenization: ['gal', 'n', 'boy', 'walking', 'in', 'd', 'park', 'galcan', 'i', 'hold', 'ur', 'hand', 'boyy', 'do', 'u', 'think', 'i', 'would', 'run', 'away', 'galno', 'jst', 'wana', 'c', 'how', 'it', 'feels', 'walking', 'in', 'heaven', 'with', 'an', 'princegn']\n",
            "After stop word removal: ['gal', 'n', 'boy', 'walking', 'park', 'galcan', 'hold', 'ur', 'hand', 'boyy', 'u', 'think', 'would', 'run', 'away', 'galno', 'jst', 'wana', 'c', 'feels', 'walking', 'heaven', 'princegn']\n",
            "After lemmatization: ['gal', 'n', 'boy', 'walking', 'park', 'galcan', 'hold', 'ur', 'hand', 'boyy', 'u', 'think', 'would', 'run', 'away', 'galno', 'jst', 'wana', 'c', 'feel', 'walking', 'heaven', 'princegn']\n",
            "After lowercasing: what makes you most happy?\n",
            "After removing special chars: what makes you most happy\n",
            "After tokenization: ['what', 'makes', 'you', 'most', 'happy']\n",
            "After stop word removal: ['makes', 'happy']\n",
            "After lemmatization: ['make', 'happy']\n",
            "After lowercasing: wishing you a wonderful week.\n",
            "After removing special chars: wishing you a wonderful week\n",
            "After tokenization: ['wishing', 'you', 'a', 'wonderful', 'week']\n",
            "After stop word removal: ['wishing', 'wonderful', 'week']\n",
            "After lemmatization: ['wishing', 'wonderful', 'week']\n",
            "After lowercasing: sweet heart how are you?\n",
            "After removing special chars: sweet heart how are you\n",
            "After tokenization: ['sweet', 'heart', 'how', 'are', 'you']\n",
            "After stop word removal: ['sweet', 'heart']\n",
            "After lemmatization: ['sweet', 'heart']\n",
            "After lowercasing: sir, waiting for your letter.\n",
            "After removing special chars: sir waiting for your letter\n",
            "After tokenization: ['sir', 'waiting', 'for', 'your', 'letter']\n",
            "After stop word removal: ['sir', 'waiting', 'letter']\n",
            "After lemmatization: ['sir', 'waiting', 'letter']\n",
            "After lowercasing: dude im no longer a pisces. im an aquarius now.\n",
            "After removing special chars: dude im no longer a pisces im an aquarius now\n",
            "After tokenization: ['dude', 'im', 'no', 'longer', 'a', 'pisces', 'im', 'an', 'aquarius', 'now']\n",
            "After stop word removal: ['dude', 'im', 'longer', 'pisces', 'im', 'aquarius']\n",
            "After lemmatization: ['dude', 'im', 'longer', 'pisces', 'im', 'aquarius']\n",
            "After lowercasing: x course it 2yrs. just so her messages on messenger lik you r sending me\n",
            "After removing special chars: x course it yrs just so her messages on messenger lik you r sending me\n",
            "After tokenization: ['x', 'course', 'it', 'yrs', 'just', 'so', 'her', 'messages', 'on', 'messenger', 'lik', 'you', 'r', 'sending', 'me']\n",
            "After stop word removal: ['x', 'course', 'yrs', 'messages', 'messenger', 'lik', 'r', 'sending']\n",
            "After lemmatization: ['x', 'course', 'yr', 'message', 'messenger', 'lik', 'r', 'sending']\n",
            "After lowercasing: i think steyn surely get one wicket:)\n",
            "After removing special chars: i think steyn surely get one wicket\n",
            "After tokenization: ['i', 'think', 'steyn', 'surely', 'get', 'one', 'wicket']\n",
            "After stop word removal: ['think', 'steyn', 'surely', 'get', 'one', 'wicket']\n",
            "After lemmatization: ['think', 'steyn', 'surely', 'get', 'one', 'wicket']\n",
            "After lowercasing: neither [in sterm voice] - i'm studying. all fine with me! not sure the  thing will be resolved, tho. anyway. have a fab hols\n",
            "After removing special chars: neither in sterm voice  im studying all fine with me not sure the  thing will be resolved tho anyway have a fab hols\n",
            "After tokenization: ['neither', 'in', 'sterm', 'voice', 'im', 'studying', 'all', 'fine', 'with', 'me', 'not', 'sure', 'the', 'thing', 'will', 'be', 'resolved', 'tho', 'anyway', 'have', 'a', 'fab', 'hols']\n",
            "After stop word removal: ['neither', 'sterm', 'voice', 'im', 'studying', 'fine', 'sure', 'thing', 'resolved', 'tho', 'anyway', 'fab', 'hols']\n",
            "After lemmatization: ['neither', 'sterm', 'voice', 'im', 'studying', 'fine', 'sure', 'thing', 'resolved', 'tho', 'anyway', 'fab', 'hols']\n",
            "After lowercasing: garbage bags, eggs, jam, bread, hannaford wheat chex\n",
            "After removing special chars: garbage bags eggs jam bread hannaford wheat chex\n",
            "After tokenization: ['garbage', 'bags', 'eggs', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
            "After stop word removal: ['garbage', 'bags', 'eggs', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
            "After lemmatization: ['garbage', 'bag', 'egg', 'jam', 'bread', 'hannaford', 'wheat', 'chex']\n",
            "After lowercasing: no. it's not pride. i'm almost  &lt;#&gt;  years old and shouldn't be takin money from my kid. you're not supposed to have to deal with this stuff. this is grownup stuff--why i don't tell you.\n",
            "After removing special chars: no its not pride im almost  ltgt  years old and shouldnt be takin money from my kid youre not supposed to have to deal with this stuff this is grownup stuffwhy i dont tell you\n",
            "After tokenization: ['no', 'its', 'not', 'pride', 'im', 'almost', 'ltgt', 'years', 'old', 'and', 'shouldnt', 'be', 'takin', 'money', 'from', 'my', 'kid', 'youre', 'not', 'supposed', 'to', 'have', 'to', 'deal', 'with', 'this', 'stuff', 'this', 'is', 'grownup', 'stuffwhy', 'i', 'dont', 'tell', 'you']\n",
            "After stop word removal: ['pride', 'im', 'almost', 'ltgt', 'years', 'old', 'shouldnt', 'takin', 'money', 'kid', 'youre', 'supposed', 'deal', 'stuff', 'grownup', 'stuffwhy', 'dont', 'tell']\n",
            "After lemmatization: ['pride', 'im', 'almost', 'ltgt', 'year', 'old', 'shouldnt', 'takin', 'money', 'kid', 'youre', 'supposed', 'deal', 'stuff', 'grownup', 'stuffwhy', 'dont', 'tell']\n",
            "After lowercasing: sounds better than my evening im just doing my costume. im not sure what time i finish tomorrow but i will txt you at the end.\n",
            "After removing special chars: sounds better than my evening im just doing my costume im not sure what time i finish tomorrow but i will txt you at the end\n",
            "After tokenization: ['sounds', 'better', 'than', 'my', 'evening', 'im', 'just', 'doing', 'my', 'costume', 'im', 'not', 'sure', 'what', 'time', 'i', 'finish', 'tomorrow', 'but', 'i', 'will', 'txt', 'you', 'at', 'the', 'end']\n",
            "After stop word removal: ['sounds', 'better', 'evening', 'im', 'costume', 'im', 'sure', 'time', 'finish', 'tomorrow', 'txt', 'end']\n",
            "After lemmatization: ['sound', 'better', 'evening', 'im', 'costume', 'im', 'sure', 'time', 'finish', 'tomorrow', 'txt', 'end']\n",
            "After lowercasing: my birthday is on feb  &lt;#&gt;  da. .\n",
            "After removing special chars: my birthday is on feb  ltgt  da \n",
            "After tokenization: ['my', 'birthday', 'is', 'on', 'feb', 'ltgt', 'da']\n",
            "After stop word removal: ['birthday', 'feb', 'ltgt', 'da']\n",
            "After lemmatization: ['birthday', 'feb', 'ltgt', 'da']\n",
            "After lowercasing: so when do you wanna gym?\n",
            "After removing special chars: so when do you wanna gym\n",
            "After tokenization: ['so', 'when', 'do', 'you', 'wan', 'na', 'gym']\n",
            "After stop word removal: ['wan', 'na', 'gym']\n",
            "After lemmatization: ['wan', 'na', 'gym']\n",
            "After lowercasing: you'd like that wouldn't you? jerk!\n",
            "After removing special chars: youd like that wouldnt you jerk\n",
            "After tokenization: ['youd', 'like', 'that', 'wouldnt', 'you', 'jerk']\n",
            "After stop word removal: ['youd', 'like', 'wouldnt', 'jerk']\n",
            "After lemmatization: ['youd', 'like', 'wouldnt', 'jerk']\n",
            "After lowercasing: are u awake? is there snow there?\n",
            "After removing special chars: are u awake is there snow there\n",
            "After tokenization: ['are', 'u', 'awake', 'is', 'there', 'snow', 'there']\n",
            "After stop word removal: ['u', 'awake', 'snow']\n",
            "After lemmatization: ['u', 'awake', 'snow']\n",
            "After lowercasing: and of course you should make a stink!\n",
            "After removing special chars: and of course you should make a stink\n",
            "After tokenization: ['and', 'of', 'course', 'you', 'should', 'make', 'a', 'stink']\n",
            "After stop word removal: ['course', 'make', 'stink']\n",
            "After lemmatization: ['course', 'make', 'stink']\n",
            "After lowercasing: u r subscribed 2 textcomp 250 wkly comp. 1st wk?s free question follows, subsequent wks charged@150p/msg.2 unsubscribe txt stop 2 84128,custcare 08712405020\n",
            "After removing special chars: u r subscribed  textcomp  wkly comp st wks free question follows subsequent wks chargedpmsg unsubscribe txt stop  custcare \n",
            "After tokenization: ['u', 'r', 'subscribed', 'textcomp', 'wkly', 'comp', 'st', 'wks', 'free', 'question', 'follows', 'subsequent', 'wks', 'chargedpmsg', 'unsubscribe', 'txt', 'stop', 'custcare']\n",
            "After stop word removal: ['u', 'r', 'subscribed', 'textcomp', 'wkly', 'comp', 'st', 'wks', 'free', 'question', 'follows', 'subsequent', 'wks', 'chargedpmsg', 'unsubscribe', 'txt', 'stop', 'custcare']\n",
            "After lemmatization: ['u', 'r', 'subscribed', 'textcomp', 'wkly', 'comp', 'st', 'wks', 'free', 'question', 'follows', 'subsequent', 'wks', 'chargedpmsg', 'unsubscribe', 'txt', 'stop', 'custcare']\n",
            "After lowercasing: no go. no openings for that room 'til after thanksgiving without an upcharge.\n",
            "After removing special chars: no go no openings for that room til after thanksgiving without an upcharge\n",
            "After tokenization: ['no', 'go', 'no', 'openings', 'for', 'that', 'room', 'til', 'after', 'thanksgiving', 'without', 'an', 'upcharge']\n",
            "After stop word removal: ['go', 'openings', 'room', 'til', 'thanksgiving', 'without', 'upcharge']\n",
            "After lemmatization: ['go', 'opening', 'room', 'til', 'thanksgiving', 'without', 'upcharge']\n",
            "After lowercasing: when you guys planning on coming over?\n",
            "After removing special chars: when you guys planning on coming over\n",
            "After tokenization: ['when', 'you', 'guys', 'planning', 'on', 'coming', 'over']\n",
            "After stop word removal: ['guys', 'planning', 'coming']\n",
            "After lemmatization: ['guy', 'planning', 'coming']\n",
            "After lowercasing: wat ì_ doing now?\n",
            "After removing special chars: wat  doing now\n",
            "After tokenization: ['wat', 'doing', 'now']\n",
            "After stop word removal: ['wat']\n",
            "After lemmatization: ['wat']\n",
            "After lowercasing: my parents, my kidz, my friends n my colleagues. all screaming.. surprise !! and i was waiting on the sofa.. ... ..... ' naked...!\n",
            "After removing special chars: my parents my kidz my friends n my colleagues all screaming surprise  and i was waiting on the sofa    naked\n",
            "After tokenization: ['my', 'parents', 'my', 'kidz', 'my', 'friends', 'n', 'my', 'colleagues', 'all', 'screaming', 'surprise', 'and', 'i', 'was', 'waiting', 'on', 'the', 'sofa', 'naked']\n",
            "After stop word removal: ['parents', 'kidz', 'friends', 'n', 'colleagues', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
            "After lemmatization: ['parent', 'kidz', 'friend', 'n', 'colleague', 'screaming', 'surprise', 'waiting', 'sofa', 'naked']\n",
            "After lowercasing: no sir. that's why i had an 8-hr trip on the bus last week. have another audition next wednesday but i think i might drive this time.\n",
            "After removing special chars: no sir thats why i had an hr trip on the bus last week have another audition next wednesday but i think i might drive this time\n",
            "After tokenization: ['no', 'sir', 'thats', 'why', 'i', 'had', 'an', 'hr', 'trip', 'on', 'the', 'bus', 'last', 'week', 'have', 'another', 'audition', 'next', 'wednesday', 'but', 'i', 'think', 'i', 'might', 'drive', 'this', 'time']\n",
            "After stop word removal: ['sir', 'thats', 'hr', 'trip', 'bus', 'last', 'week', 'another', 'audition', 'next', 'wednesday', 'think', 'might', 'drive', 'time']\n",
            "After lemmatization: ['sir', 'thats', 'hr', 'trip', 'bus', 'last', 'week', 'another', 'audition', 'next', 'wednesday', 'think', 'might', 'drive', 'time']\n",
            "After lowercasing: do i? i thought i put it back in the box\n",
            "After removing special chars: do i i thought i put it back in the box\n",
            "After tokenization: ['do', 'i', 'i', 'thought', 'i', 'put', 'it', 'back', 'in', 'the', 'box']\n",
            "After stop word removal: ['thought', 'put', 'back', 'box']\n",
            "After lemmatization: ['thought', 'put', 'back', 'box']\n",
            "After lowercasing: i'm home...\n",
            "After removing special chars: im home\n",
            "After tokenization: ['im', 'home']\n",
            "After stop word removal: ['im', 'home']\n",
            "After lemmatization: ['im', 'home']\n",
            "After lowercasing: no one interested. may be some business plan.\n",
            "After removing special chars: no one interested may be some business plan\n",
            "After tokenization: ['no', 'one', 'interested', 'may', 'be', 'some', 'business', 'plan']\n",
            "After stop word removal: ['one', 'interested', 'may', 'business', 'plan']\n",
            "After lemmatization: ['one', 'interested', 'may', 'business', 'plan']\n",
            "After lowercasing: yup it's at paragon... i havent decided whether 2 cut yet... hee...\n",
            "After removing special chars: yup its at paragon i havent decided whether  cut yet hee\n",
            "After tokenization: ['yup', 'its', 'at', 'paragon', 'i', 'havent', 'decided', 'whether', 'cut', 'yet', 'hee']\n",
            "After stop word removal: ['yup', 'paragon', 'havent', 'decided', 'whether', 'cut', 'yet', 'hee']\n",
            "After lemmatization: ['yup', 'paragon', 'havent', 'decided', 'whether', 'cut', 'yet', 'hee']\n",
            "After lowercasing: good morning princess! have a great day!\n",
            "After removing special chars: good morning princess have a great day\n",
            "After tokenization: ['good', 'morning', 'princess', 'have', 'a', 'great', 'day']\n",
            "After stop word removal: ['good', 'morning', 'princess', 'great', 'day']\n",
            "After lemmatization: ['good', 'morning', 'princess', 'great', 'day']\n",
            "After lowercasing: guai... ìï shd haf seen him when he's naughty... ìï so free today? can go jogging...\n",
            "After removing special chars: guai  shd haf seen him when hes naughty  so free today can go jogging\n",
            "After tokenization: ['guai', 'shd', 'haf', 'seen', 'him', 'when', 'hes', 'naughty', 'so', 'free', 'today', 'can', 'go', 'jogging']\n",
            "After stop word removal: ['guai', 'shd', 'haf', 'seen', 'hes', 'naughty', 'free', 'today', 'go', 'jogging']\n",
            "After lemmatization: ['guai', 'shd', 'haf', 'seen', 'he', 'naughty', 'free', 'today', 'go', 'jogging']\n",
            "After lowercasing: aiyo cos i sms ì_ then ì_ neva reply so i wait 4 ì_ to reply lar. i tot ì_ havent finish ur lab wat.\n",
            "After removing special chars: aiyo cos i sms  then  neva reply so i wait   to reply lar i tot  havent finish ur lab wat\n",
            "After tokenization: ['aiyo', 'cos', 'i', 'sms', 'then', 'neva', 'reply', 'so', 'i', 'wait', 'to', 'reply', 'lar', 'i', 'tot', 'havent', 'finish', 'ur', 'lab', 'wat']\n",
            "After stop word removal: ['aiyo', 'cos', 'sms', 'neva', 'reply', 'wait', 'reply', 'lar', 'tot', 'havent', 'finish', 'ur', 'lab', 'wat']\n",
            "After lemmatization: ['aiyo', 'co', 'sm', 'neva', 'reply', 'wait', 'reply', 'lar', 'tot', 'havent', 'finish', 'ur', 'lab', 'wat']\n",
            "After lowercasing: living is very simple.. loving is also simple.. laughing is too simple.. winning is tooo simple.. but, being 'simple' is very difficult...;-) :-)\n",
            "After removing special chars: living is very simple loving is also simple laughing is too simple winning is tooo simple but being simple is very difficult \n",
            "After tokenization: ['living', 'is', 'very', 'simple', 'loving', 'is', 'also', 'simple', 'laughing', 'is', 'too', 'simple', 'winning', 'is', 'tooo', 'simple', 'but', 'being', 'simple', 'is', 'very', 'difficult']\n",
            "After stop word removal: ['living', 'simple', 'loving', 'also', 'simple', 'laughing', 'simple', 'winning', 'tooo', 'simple', 'simple', 'difficult']\n",
            "After lemmatization: ['living', 'simple', 'loving', 'also', 'simple', 'laughing', 'simple', 'winning', 'tooo', 'simple', 'simple', 'difficult']\n",
            "After lowercasing: tell me something. thats okay.\n",
            "After removing special chars: tell me something thats okay\n",
            "After tokenization: ['tell', 'me', 'something', 'thats', 'okay']\n",
            "After stop word removal: ['tell', 'something', 'thats', 'okay']\n",
            "After lemmatization: ['tell', 'something', 'thats', 'okay']\n",
            "After lowercasing: ok\n",
            "After removing special chars: ok\n",
            "After tokenization: ['ok']\n",
            "After stop word removal: ['ok']\n",
            "After lemmatization: ['ok']\n",
            "After lowercasing: hmm. shall i bring a bottle of wine to keep us amused? just joking! i'll still bring a bottle. red or white? see you tomorrow\n",
            "After removing special chars: hmm shall i bring a bottle of wine to keep us amused just joking ill still bring a bottle red or white see you tomorrow\n",
            "After tokenization: ['hmm', 'shall', 'i', 'bring', 'a', 'bottle', 'of', 'wine', 'to', 'keep', 'us', 'amused', 'just', 'joking', 'ill', 'still', 'bring', 'a', 'bottle', 'red', 'or', 'white', 'see', 'you', 'tomorrow']\n",
            "After stop word removal: ['hmm', 'shall', 'bring', 'bottle', 'wine', 'keep', 'us', 'amused', 'joking', 'ill', 'still', 'bring', 'bottle', 'red', 'white', 'see', 'tomorrow']\n",
            "After lemmatization: ['hmm', 'shall', 'bring', 'bottle', 'wine', 'keep', 'u', 'amused', 'joking', 'ill', 'still', 'bring', 'bottle', 'red', 'white', 'see', 'tomorrow']\n",
            "After lowercasing: this is ur face test ( 1 2 3 4 5 6 7 8 9  &lt;#&gt;  ) select any number i will tell ur face astrology.... am waiting. quick reply...\n",
            "After removing special chars: this is ur face test            ltgt   select any number i will tell ur face astrology am waiting quick reply\n",
            "After tokenization: ['this', 'is', 'ur', 'face', 'test', 'ltgt', 'select', 'any', 'number', 'i', 'will', 'tell', 'ur', 'face', 'astrology', 'am', 'waiting', 'quick', 'reply']\n",
            "After stop word removal: ['ur', 'face', 'test', 'ltgt', 'select', 'number', 'tell', 'ur', 'face', 'astrology', 'waiting', 'quick', 'reply']\n",
            "After lemmatization: ['ur', 'face', 'test', 'ltgt', 'select', 'number', 'tell', 'ur', 'face', 'astrology', 'waiting', 'quick', 'reply']\n",
            "After lowercasing: hey, iouri gave me your number, i'm wylie, ryan's friend\n",
            "After removing special chars: hey iouri gave me your number im wylie ryans friend\n",
            "After tokenization: ['hey', 'iouri', 'gave', 'me', 'your', 'number', 'im', 'wylie', 'ryans', 'friend']\n",
            "After stop word removal: ['hey', 'iouri', 'gave', 'number', 'im', 'wylie', 'ryans', 'friend']\n",
            "After lemmatization: ['hey', 'iouri', 'gave', 'number', 'im', 'wylie', 'ryans', 'friend']\n",
            "After lowercasing: yep get with the program. you're slacking.\n",
            "After removing special chars: yep get with the program youre slacking\n",
            "After tokenization: ['yep', 'get', 'with', 'the', 'program', 'youre', 'slacking']\n",
            "After stop word removal: ['yep', 'get', 'program', 'youre', 'slacking']\n",
            "After lemmatization: ['yep', 'get', 'program', 'youre', 'slacking']\n",
            "After lowercasing: i'm in inside office..still filling forms.don know when they leave me.\n",
            "After removing special chars: im in inside officestill filling formsdon know when they leave me\n",
            "After tokenization: ['im', 'in', 'inside', 'officestill', 'filling', 'formsdon', 'know', 'when', 'they', 'leave', 'me']\n",
            "After stop word removal: ['im', 'inside', 'officestill', 'filling', 'formsdon', 'know', 'leave']\n",
            "After lemmatization: ['im', 'inside', 'officestill', 'filling', 'formsdon', 'know', 'leave']\n",
            "After lowercasing: i think your mentor is , but not 100 percent sure.\n",
            "After removing special chars: i think your mentor is  but not  percent sure\n",
            "After tokenization: ['i', 'think', 'your', 'mentor', 'is', 'but', 'not', 'percent', 'sure']\n",
            "After stop word removal: ['think', 'mentor', 'percent', 'sure']\n",
            "After lemmatization: ['think', 'mentor', 'percent', 'sure']\n",
            "After lowercasing: call 09095350301 and send our girls into erotic ecstacy. just 60p/min. to stop texts call 08712460324 (nat rate)\n",
            "After removing special chars: call  and send our girls into erotic ecstacy just pmin to stop texts call  nat rate\n",
            "After tokenization: ['call', 'and', 'send', 'our', 'girls', 'into', 'erotic', 'ecstacy', 'just', 'pmin', 'to', 'stop', 'texts', 'call', 'nat', 'rate']\n",
            "After stop word removal: ['call', 'send', 'girls', 'erotic', 'ecstacy', 'pmin', 'stop', 'texts', 'call', 'nat', 'rate']\n",
            "After lemmatization: ['call', 'send', 'girl', 'erotic', 'ecstacy', 'pmin', 'stop', 'text', 'call', 'nat', 'rate']\n",
            "After lowercasing: camera - you are awarded a sipix digital camera! call 09061221066 fromm landline. delivery within 28 days.\n",
            "After removing special chars: camera  you are awarded a sipix digital camera call  fromm landline delivery within  days\n",
            "After tokenization: ['camera', 'you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
            "After stop word removal: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'days']\n",
            "After lemmatization: ['camera', 'awarded', 'sipix', 'digital', 'camera', 'call', 'fromm', 'landline', 'delivery', 'within', 'day']\n",
            "After lowercasing: a å£400 xmas reward is waiting for you! our computer has randomly picked you from our loyal mobile customers to receive a å£400 reward. just call 09066380611\n",
            "After removing special chars: a  xmas reward is waiting for you our computer has randomly picked you from our loyal mobile customers to receive a  reward just call \n",
            "After tokenization: ['a', 'xmas', 'reward', 'is', 'waiting', 'for', 'you', 'our', 'computer', 'has', 'randomly', 'picked', 'you', 'from', 'our', 'loyal', 'mobile', 'customers', 'to', 'receive', 'a', 'reward', 'just', 'call']\n",
            "After stop word removal: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customers', 'receive', 'reward', 'call']\n",
            "After lemmatization: ['xmas', 'reward', 'waiting', 'computer', 'randomly', 'picked', 'loyal', 'mobile', 'customer', 'receive', 'reward', 'call']\n",
            "After lowercasing: just trying to figure out when i'm suppose to see a couple different people this week. we said we'd get together but i didn't set dates\n",
            "After removing special chars: just trying to figure out when im suppose to see a couple different people this week we said wed get together but i didnt set dates\n",
            "After tokenization: ['just', 'trying', 'to', 'figure', 'out', 'when', 'im', 'suppose', 'to', 'see', 'a', 'couple', 'different', 'people', 'this', 'week', 'we', 'said', 'wed', 'get', 'together', 'but', 'i', 'didnt', 'set', 'dates']\n",
            "After stop word removal: ['trying', 'figure', 'im', 'suppose', 'see', 'couple', 'different', 'people', 'week', 'said', 'wed', 'get', 'together', 'didnt', 'set', 'dates']\n",
            "After lemmatization: ['trying', 'figure', 'im', 'suppose', 'see', 'couple', 'different', 'people', 'week', 'said', 'wed', 'get', 'together', 'didnt', 'set', 'date']\n",
            "After lowercasing: important message. this is a final contact attempt. you have important messages waiting out our customer claims dept. expires 13/4/04. call 08717507382 now!\n",
            "After removing special chars: important message this is a final contact attempt you have important messages waiting out our customer claims dept expires  call  now\n",
            "After tokenization: ['important', 'message', 'this', 'is', 'a', 'final', 'contact', 'attempt', 'you', 'have', 'important', 'messages', 'waiting', 'out', 'our', 'customer', 'claims', 'dept', 'expires', 'call', 'now']\n",
            "After stop word removal: ['important', 'message', 'final', 'contact', 'attempt', 'important', 'messages', 'waiting', 'customer', 'claims', 'dept', 'expires', 'call']\n",
            "After lemmatization: ['important', 'message', 'final', 'contact', 'attempt', 'important', 'message', 'waiting', 'customer', 'claim', 'dept', 'expires', 'call']\n",
            "After lowercasing: hi mom we might be back later than  &lt;#&gt; \n",
            "After removing special chars: hi mom we might be back later than  ltgt \n",
            "After tokenization: ['hi', 'mom', 'we', 'might', 'be', 'back', 'later', 'than', 'ltgt']\n",
            "After stop word removal: ['hi', 'mom', 'might', 'back', 'later', 'ltgt']\n",
            "After lemmatization: ['hi', 'mom', 'might', 'back', 'later', 'ltgt']\n",
            "After lowercasing: dating:i have had two of these. only started after i sent a text to talk sport radio last week. any connection do you think or coincidence?\n",
            "After removing special chars: datingi have had two of these only started after i sent a text to talk sport radio last week any connection do you think or coincidence\n",
            "After tokenization: ['datingi', 'have', 'had', 'two', 'of', 'these', 'only', 'started', 'after', 'i', 'sent', 'a', 'text', 'to', 'talk', 'sport', 'radio', 'last', 'week', 'any', 'connection', 'do', 'you', 'think', 'or', 'coincidence']\n",
            "After stop word removal: ['datingi', 'two', 'started', 'sent', 'text', 'talk', 'sport', 'radio', 'last', 'week', 'connection', 'think', 'coincidence']\n",
            "After lemmatization: ['datingi', 'two', 'started', 'sent', 'text', 'talk', 'sport', 'radio', 'last', 'week', 'connection', 'think', 'coincidence']\n",
            "After lowercasing: lol, oh you got a friend for the dog ?\n",
            "After removing special chars: lol oh you got a friend for the dog \n",
            "After tokenization: ['lol', 'oh', 'you', 'got', 'a', 'friend', 'for', 'the', 'dog']\n",
            "After stop word removal: ['lol', 'oh', 'got', 'friend', 'dog']\n",
            "After lemmatization: ['lol', 'oh', 'got', 'friend', 'dog']\n",
            "After lowercasing: ok., is any problem to u frm him? wats matter?\n",
            "After removing special chars: ok is any problem to u frm him wats matter\n",
            "After tokenization: ['ok', 'is', 'any', 'problem', 'to', 'u', 'frm', 'him', 'wats', 'matter']\n",
            "After stop word removal: ['ok', 'problem', 'u', 'frm', 'wats', 'matter']\n",
            "After lemmatization: ['ok', 'problem', 'u', 'frm', 'wats', 'matter']\n",
            "After lowercasing: k i'll head out in a few mins, see you there\n",
            "After removing special chars: k ill head out in a few mins see you there\n",
            "After tokenization: ['k', 'ill', 'head', 'out', 'in', 'a', 'few', 'mins', 'see', 'you', 'there']\n",
            "After stop word removal: ['k', 'ill', 'head', 'mins', 'see']\n",
            "After lemmatization: ['k', 'ill', 'head', 'min', 'see']\n",
            "After lowercasing: do u konw waht is rael friendship im gving yuo an exmpel: jsut ese tihs msg.. evrey splleing of tihs msg is wrnog.. bt sitll yuo can raed it wihtuot ayn mitsake.. goodnight &amp; have a nice sleep..sweet dreams..\n",
            "After removing special chars: do u konw waht is rael friendship im gving yuo an exmpel jsut ese tihs msg evrey splleing of tihs msg is wrnog bt sitll yuo can raed it wihtuot ayn mitsake goodnight amp have a nice sleepsweet dreams\n",
            "After tokenization: ['do', 'u', 'konw', 'waht', 'is', 'rael', 'friendship', 'im', 'gving', 'yuo', 'an', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'of', 'tihs', 'msg', 'is', 'wrnog', 'bt', 'sitll', 'yuo', 'can', 'raed', 'it', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'have', 'a', 'nice', 'sleepsweet', 'dreams']\n",
            "After stop word removal: ['u', 'konw', 'waht', 'rael', 'friendship', 'im', 'gving', 'yuo', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'tihs', 'msg', 'wrnog', 'bt', 'sitll', 'yuo', 'raed', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'nice', 'sleepsweet', 'dreams']\n",
            "After lemmatization: ['u', 'konw', 'waht', 'rael', 'friendship', 'im', 'gving', 'yuo', 'exmpel', 'jsut', 'ese', 'tihs', 'msg', 'evrey', 'splleing', 'tihs', 'msg', 'wrnog', 'bt', 'sitll', 'yuo', 'raed', 'wihtuot', 'ayn', 'mitsake', 'goodnight', 'amp', 'nice', 'sleepsweet', 'dream']\n",
            "After lowercasing: i cant pick the phone right now. pls send a message\n",
            "After removing special chars: i cant pick the phone right now pls send a message\n",
            "After tokenization: ['i', 'cant', 'pick', 'the', 'phone', 'right', 'now', 'pls', 'send', 'a', 'message']\n",
            "After stop word removal: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lemmatization: ['cant', 'pick', 'phone', 'right', 'pls', 'send', 'message']\n",
            "After lowercasing: i don't want you to leave. but i'm barely doing what i can to stay sane. fighting with you constantly isn't helping.\n",
            "After removing special chars: i dont want you to leave but im barely doing what i can to stay sane fighting with you constantly isnt helping\n",
            "After tokenization: ['i', 'dont', 'want', 'you', 'to', 'leave', 'but', 'im', 'barely', 'doing', 'what', 'i', 'can', 'to', 'stay', 'sane', 'fighting', 'with', 'you', 'constantly', 'isnt', 'helping']\n",
            "After stop word removal: ['dont', 'want', 'leave', 'im', 'barely', 'stay', 'sane', 'fighting', 'constantly', 'isnt', 'helping']\n",
            "After lemmatization: ['dont', 'want', 'leave', 'im', 'barely', 'stay', 'sane', 'fighting', 'constantly', 'isnt', 'helping']\n",
            "After lowercasing: the current leading bid is 151. to pause this auction send out. customer care: 08718726270\n",
            "After removing special chars: the current leading bid is  to pause this auction send out customer care \n",
            "After tokenization: ['the', 'current', 'leading', 'bid', 'is', 'to', 'pause', 'this', 'auction', 'send', 'out', 'customer', 'care']\n",
            "After stop word removal: ['current', 'leading', 'bid', 'pause', 'auction', 'send', 'customer', 'care']\n",
            "After lemmatization: ['current', 'leading', 'bid', 'pause', 'auction', 'send', 'customer', 'care']\n",
            "After lowercasing: free entry to the gr8prizes wkly comp 4 a chance to win the latest nokia 8800, psp or å£250 cash every wk.txt great to 80878 http//www.gr8prizes.com 08715705022\n",
            "After removing special chars: free entry to the grprizes wkly comp  a chance to win the latest nokia  psp or  cash every wktxt great to  httpwwwgrprizescom \n",
            "After tokenization: ['free', 'entry', 'to', 'the', 'grprizes', 'wkly', 'comp', 'a', 'chance', 'to', 'win', 'the', 'latest', 'nokia', 'psp', 'or', 'cash', 'every', 'wktxt', 'great', 'to', 'httpwwwgrprizescom']\n",
            "After stop word removal: ['free', 'entry', 'grprizes', 'wkly', 'comp', 'chance', 'win', 'latest', 'nokia', 'psp', 'cash', 'every', 'wktxt', 'great', 'httpwwwgrprizescom']\n",
            "After lemmatization: ['free', 'entry', 'grprizes', 'wkly', 'comp', 'chance', 'win', 'latest', 'nokia', 'psp', 'cash', 'every', 'wktxt', 'great', 'httpwwwgrprizescom']\n",
            "After lowercasing: somebody set up a website where you can play hold em using eve online spacebucks\n",
            "After removing special chars: somebody set up a website where you can play hold em using eve online spacebucks\n",
            "After tokenization: ['somebody', 'set', 'up', 'a', 'website', 'where', 'you', 'can', 'play', 'hold', 'em', 'using', 'eve', 'online', 'spacebucks']\n",
            "After stop word removal: ['somebody', 'set', 'website', 'play', 'hold', 'em', 'using', 'eve', 'online', 'spacebucks']\n",
            "After lemmatization: ['somebody', 'set', 'website', 'play', 'hold', 'em', 'using', 'eve', 'online', 'spacebucks']\n",
            "After lowercasing: its sunny in california. the weather's just cool\n",
            "After removing special chars: its sunny in california the weathers just cool\n",
            "After tokenization: ['its', 'sunny', 'in', 'california', 'the', 'weathers', 'just', 'cool']\n",
            "After stop word removal: ['sunny', 'california', 'weathers', 'cool']\n",
            "After lemmatization: ['sunny', 'california', 'weather', 'cool']\n",
            "After lowercasing: you have 1 new message. call 0207-083-6089\n",
            "After removing special chars: you have  new message call \n",
            "After tokenization: ['you', 'have', 'new', 'message', 'call']\n",
            "After stop word removal: ['new', 'message', 'call']\n",
            "After lemmatization: ['new', 'message', 'call']\n",
            "After lowercasing: i can make it up there, squeezed  &lt;#&gt;  bucks out of my dad\n",
            "After removing special chars: i can make it up there squeezed  ltgt  bucks out of my dad\n",
            "After tokenization: ['i', 'can', 'make', 'it', 'up', 'there', 'squeezed', 'ltgt', 'bucks', 'out', 'of', 'my', 'dad']\n",
            "After stop word removal: ['make', 'squeezed', 'ltgt', 'bucks', 'dad']\n",
            "After lemmatization: ['make', 'squeezed', 'ltgt', 'buck', 'dad']\n",
            "After lowercasing: good day to you too.pray for me.remove the teeth as its painful maintaining other stuff.\n",
            "After removing special chars: good day to you toopray for meremove the teeth as its painful maintaining other stuff\n",
            "After tokenization: ['good', 'day', 'to', 'you', 'toopray', 'for', 'meremove', 'the', 'teeth', 'as', 'its', 'painful', 'maintaining', 'other', 'stuff']\n",
            "After stop word removal: ['good', 'day', 'toopray', 'meremove', 'teeth', 'painful', 'maintaining', 'stuff']\n",
            "After lemmatization: ['good', 'day', 'toopray', 'meremove', 'teeth', 'painful', 'maintaining', 'stuff']\n",
            "After lowercasing: how are you babes. hope your doing ok. i had a shit nights sleep. i fell asleep at 5.iåõm knackered and iåõm dreading work tonight. what are thou upto tonight. x\n",
            "After removing special chars: how are you babes hope your doing ok i had a shit nights sleep i fell asleep at im knackered and im dreading work tonight what are thou upto tonight x\n",
            "After tokenization: ['how', 'are', 'you', 'babes', 'hope', 'your', 'doing', 'ok', 'i', 'had', 'a', 'shit', 'nights', 'sleep', 'i', 'fell', 'asleep', 'at', 'im', 'knackered', 'and', 'im', 'dreading', 'work', 'tonight', 'what', 'are', 'thou', 'upto', 'tonight', 'x']\n",
            "After stop word removal: ['babes', 'hope', 'ok', 'shit', 'nights', 'sleep', 'fell', 'asleep', 'im', 'knackered', 'im', 'dreading', 'work', 'tonight', 'thou', 'upto', 'tonight', 'x']\n",
            "After lemmatization: ['babe', 'hope', 'ok', 'shit', 'night', 'sleep', 'fell', 'asleep', 'im', 'knackered', 'im', 'dreading', 'work', 'tonight', 'thou', 'upto', 'tonight', 'x']\n",
            "After lowercasing: how do friends help us in problems? they give the most stupid suggestion that lands us into another problem and helps us forgt the previous problem\n",
            "After removing special chars: how do friends help us in problems they give the most stupid suggestion that lands us into another problem and helps us forgt the previous problem\n",
            "After tokenization: ['how', 'do', 'friends', 'help', 'us', 'in', 'problems', 'they', 'give', 'the', 'most', 'stupid', 'suggestion', 'that', 'lands', 'us', 'into', 'another', 'problem', 'and', 'helps', 'us', 'forgt', 'the', 'previous', 'problem']\n",
            "After stop word removal: ['friends', 'help', 'us', 'problems', 'give', 'stupid', 'suggestion', 'lands', 'us', 'another', 'problem', 'helps', 'us', 'forgt', 'previous', 'problem']\n",
            "After lemmatization: ['friend', 'help', 'u', 'problem', 'give', 'stupid', 'suggestion', 'land', 'u', 'another', 'problem', 'help', 'u', 'forgt', 'previous', 'problem']\n",
            "After lowercasing: i'm at work. please call\n",
            "After removing special chars: im at work please call\n",
            "After tokenization: ['im', 'at', 'work', 'please', 'call']\n",
            "After stop word removal: ['im', 'work', 'please', 'call']\n",
            "After lemmatization: ['im', 'work', 'please', 'call']\n",
            "After lowercasing: i will be gentle baby! soon you will be taking all  &lt;#&gt;  inches deep inside your tight pussy...\n",
            "After removing special chars: i will be gentle baby soon you will be taking all  ltgt  inches deep inside your tight pussy\n",
            "After tokenization: ['i', 'will', 'be', 'gentle', 'baby', 'soon', 'you', 'will', 'be', 'taking', 'all', 'ltgt', 'inches', 'deep', 'inside', 'your', 'tight', 'pussy']\n",
            "After stop word removal: ['gentle', 'baby', 'soon', 'taking', 'ltgt', 'inches', 'deep', 'inside', 'tight', 'pussy']\n",
            "After lemmatization: ['gentle', 'baby', 'soon', 'taking', 'ltgt', 'inch', 'deep', 'inside', 'tight', 'pussy']\n",
            "After lowercasing: not much no fights. it was a good nite!!\n",
            "After removing special chars: not much no fights it was a good nite\n",
            "After tokenization: ['not', 'much', 'no', 'fights', 'it', 'was', 'a', 'good', 'nite']\n",
            "After stop word removal: ['much', 'fights', 'good', 'nite']\n",
            "After lemmatization: ['much', 'fight', 'good', 'nite']\n",
            "After lowercasing: ok.ok ok..then..whats ur todays plan\n",
            "After removing special chars: okok okthenwhats ur todays plan\n",
            "After tokenization: ['okok', 'okthenwhats', 'ur', 'todays', 'plan']\n",
            "After stop word removal: ['okok', 'okthenwhats', 'ur', 'todays', 'plan']\n",
            "After lemmatization: ['okok', 'okthenwhats', 'ur', 'today', 'plan']\n",
            "After lowercasing: nt joking seriously i told\n",
            "After removing special chars: nt joking seriously i told\n",
            "After tokenization: ['nt', 'joking', 'seriously', 'i', 'told']\n",
            "After stop word removal: ['nt', 'joking', 'seriously', 'told']\n",
            "After lemmatization: ['nt', 'joking', 'seriously', 'told']\n",
            "After lowercasing: watching ajith film ah?\n",
            "After removing special chars: watching ajith film ah\n",
            "After tokenization: ['watching', 'ajith', 'film', 'ah']\n",
            "After stop word removal: ['watching', 'ajith', 'film', 'ah']\n",
            "After lemmatization: ['watching', 'ajith', 'film', 'ah']\n",
            "After lowercasing: ooooooh i forgot to tell u i can get on yoville on my phone\n",
            "After removing special chars: ooooooh i forgot to tell u i can get on yoville on my phone\n",
            "After tokenization: ['ooooooh', 'i', 'forgot', 'to', 'tell', 'u', 'i', 'can', 'get', 'on', 'yoville', 'on', 'my', 'phone']\n",
            "After stop word removal: ['ooooooh', 'forgot', 'tell', 'u', 'get', 'yoville', 'phone']\n",
            "After lemmatization: ['ooooooh', 'forgot', 'tell', 'u', 'get', 'yoville', 'phone']\n",
            "After lowercasing: all done, all handed in. don't know if mega shop in asda counts as celebration but thats what i'm doing!\n",
            "After removing special chars: all done all handed in dont know if mega shop in asda counts as celebration but thats what im doing\n",
            "After tokenization: ['all', 'done', 'all', 'handed', 'in', 'dont', 'know', 'if', 'mega', 'shop', 'in', 'asda', 'counts', 'as', 'celebration', 'but', 'thats', 'what', 'im', 'doing']\n",
            "After stop word removal: ['done', 'handed', 'dont', 'know', 'mega', 'shop', 'asda', 'counts', 'celebration', 'thats', 'im']\n",
            "After lemmatization: ['done', 'handed', 'dont', 'know', 'mega', 'shop', 'asda', 'count', 'celebration', 'thats', 'im']\n",
            "After lowercasing: i dont know exactly could you ask chechi.\n",
            "After removing special chars: i dont know exactly could you ask chechi\n",
            "After tokenization: ['i', 'dont', 'know', 'exactly', 'could', 'you', 'ask', 'chechi']\n",
            "After stop word removal: ['dont', 'know', 'exactly', 'could', 'ask', 'chechi']\n",
            "After lemmatization: ['dont', 'know', 'exactly', 'could', 'ask', 'chechi']\n",
            "After lowercasing: dunno lei shd b driving lor cos i go sch 1 hr oni.\n",
            "After removing special chars: dunno lei shd b driving lor cos i go sch  hr oni\n",
            "After tokenization: ['dunno', 'lei', 'shd', 'b', 'driving', 'lor', 'cos', 'i', 'go', 'sch', 'hr', 'oni']\n",
            "After stop word removal: ['dunno', 'lei', 'shd', 'b', 'driving', 'lor', 'cos', 'go', 'sch', 'hr', 'oni']\n",
            "After lemmatization: ['dunno', 'lei', 'shd', 'b', 'driving', 'lor', 'co', 'go', 'sch', 'hr', 'oni']\n",
            "After lowercasing: as in i want custom officer discount oh.\n",
            "After removing special chars: as in i want custom officer discount oh\n",
            "After tokenization: ['as', 'in', 'i', 'want', 'custom', 'officer', 'discount', 'oh']\n",
            "After stop word removal: ['want', 'custom', 'officer', 'discount', 'oh']\n",
            "After lemmatization: ['want', 'custom', 'officer', 'discount', 'oh']\n",
            "After lowercasing: that's necessarily respectful\n",
            "After removing special chars: thats necessarily respectful\n",
            "After tokenization: ['thats', 'necessarily', 'respectful']\n",
            "After stop word removal: ['thats', 'necessarily', 'respectful']\n",
            "After lemmatization: ['thats', 'necessarily', 'respectful']\n",
            "After lowercasing: hi. hope you had a good day. have a better night.\n",
            "After removing special chars: hi hope you had a good day have a better night\n",
            "After tokenization: ['hi', 'hope', 'you', 'had', 'a', 'good', 'day', 'have', 'a', 'better', 'night']\n",
            "After stop word removal: ['hi', 'hope', 'good', 'day', 'better', 'night']\n",
            "After lemmatization: ['hi', 'hope', 'good', 'day', 'better', 'night']\n",
            "After lowercasing: and he's apparently bffs with carly quick now\n",
            "After removing special chars: and hes apparently bffs with carly quick now\n",
            "After tokenization: ['and', 'hes', 'apparently', 'bffs', 'with', 'carly', 'quick', 'now']\n",
            "After stop word removal: ['hes', 'apparently', 'bffs', 'carly', 'quick']\n",
            "After lemmatization: ['he', 'apparently', 'bffs', 'carly', 'quick']\n",
            "After lowercasing: hard but true: how much you show &amp;  express your love to someone....that much it will hurt when they leave you or you get seperated...!ö´ó_??û¬ud evening...\n",
            "After removing special chars: hard but true how much you show amp  express your love to someonethat much it will hurt when they leave you or you get seperatedud evening\n",
            "After tokenization: ['hard', 'but', 'true', 'how', 'much', 'you', 'show', 'amp', 'express', 'your', 'love', 'to', 'someonethat', 'much', 'it', 'will', 'hurt', 'when', 'they', 'leave', 'you', 'or', 'you', 'get', 'seperatedud', 'evening']\n",
            "After stop word removal: ['hard', 'true', 'much', 'show', 'amp', 'express', 'love', 'someonethat', 'much', 'hurt', 'leave', 'get', 'seperatedud', 'evening']\n",
            "After lemmatization: ['hard', 'true', 'much', 'show', 'amp', 'express', 'love', 'someonethat', 'much', 'hurt', 'leave', 'get', 'seperatedud', 'evening']\n",
            "After lowercasing: babes i think i got ur brolly i left it in english wil bring it in 2mrw 4 u luv franxx\n",
            "After removing special chars: babes i think i got ur brolly i left it in english wil bring it in mrw  u luv franxx\n",
            "After tokenization: ['babes', 'i', 'think', 'i', 'got', 'ur', 'brolly', 'i', 'left', 'it', 'in', 'english', 'wil', 'bring', 'it', 'in', 'mrw', 'u', 'luv', 'franxx']\n",
            "After stop word removal: ['babes', 'think', 'got', 'ur', 'brolly', 'left', 'english', 'wil', 'bring', 'mrw', 'u', 'luv', 'franxx']\n",
            "After lemmatization: ['babe', 'think', 'got', 'ur', 'brolly', 'left', 'english', 'wil', 'bring', 'mrw', 'u', 'luv', 'franxx']\n",
            "After lowercasing: hi babe its me thanks for coming even though it didnt go that well!i just wanted my bed! hope to see you soon love and kisses xxx\n",
            "After removing special chars: hi babe its me thanks for coming even though it didnt go that welli just wanted my bed hope to see you soon love and kisses xxx\n",
            "After tokenization: ['hi', 'babe', 'its', 'me', 'thanks', 'for', 'coming', 'even', 'though', 'it', 'didnt', 'go', 'that', 'welli', 'just', 'wanted', 'my', 'bed', 'hope', 'to', 'see', 'you', 'soon', 'love', 'and', 'kisses', 'xxx']\n",
            "After stop word removal: ['hi', 'babe', 'thanks', 'coming', 'even', 'though', 'didnt', 'go', 'welli', 'wanted', 'bed', 'hope', 'see', 'soon', 'love', 'kisses', 'xxx']\n",
            "After lemmatization: ['hi', 'babe', 'thanks', 'coming', 'even', 'though', 'didnt', 'go', 'welli', 'wanted', 'bed', 'hope', 'see', 'soon', 'love', 'kiss', 'xxx']\n",
            "After lowercasing: so gd got free ice cream... i oso wan...\n",
            "After removing special chars: so gd got free ice cream i oso wan\n",
            "After tokenization: ['so', 'gd', 'got', 'free', 'ice', 'cream', 'i', 'oso', 'wan']\n",
            "After stop word removal: ['gd', 'got', 'free', 'ice', 'cream', 'oso', 'wan']\n",
            "After lemmatization: ['gd', 'got', 'free', 'ice', 'cream', 'oso', 'wan']\n",
            "After lowercasing: pls give her prometazine syrup. 5mls then  &lt;#&gt; mins later feed.\n",
            "After removing special chars: pls give her prometazine syrup mls then  ltgt mins later feed\n",
            "After tokenization: ['pls', 'give', 'her', 'prometazine', 'syrup', 'mls', 'then', 'ltgt', 'mins', 'later', 'feed']\n",
            "After stop word removal: ['pls', 'give', 'prometazine', 'syrup', 'mls', 'ltgt', 'mins', 'later', 'feed']\n",
            "After lemmatization: ['pls', 'give', 'prometazine', 'syrup', 'ml', 'ltgt', 'min', 'later', 'feed']\n",
            "After lowercasing: so how many days since then?\n",
            "After removing special chars: so how many days since then\n",
            "After tokenization: ['so', 'how', 'many', 'days', 'since', 'then']\n",
            "After stop word removal: ['many', 'days', 'since']\n",
            "After lemmatization: ['many', 'day', 'since']\n",
            "After lowercasing: dear are you angry i was busy dear\n",
            "After removing special chars: dear are you angry i was busy dear\n",
            "After tokenization: ['dear', 'are', 'you', 'angry', 'i', 'was', 'busy', 'dear']\n",
            "After stop word removal: ['dear', 'angry', 'busy', 'dear']\n",
            "After lemmatization: ['dear', 'angry', 'busy', 'dear']\n",
            "After lowercasing: yup he msg me: is tat yijue? then i tot it's my group mate cos we meeting today mah... i'm askin if ì_ leaving earlier or wat mah cos mayb ì_ haf to walk v far...\n",
            "After removing special chars: yup he msg me is tat yijue then i tot its my group mate cos we meeting today mah im askin if  leaving earlier or wat mah cos mayb  haf to walk v far\n",
            "After tokenization: ['yup', 'he', 'msg', 'me', 'is', 'tat', 'yijue', 'then', 'i', 'tot', 'its', 'my', 'group', 'mate', 'cos', 'we', 'meeting', 'today', 'mah', 'im', 'askin', 'if', 'leaving', 'earlier', 'or', 'wat', 'mah', 'cos', 'mayb', 'haf', 'to', 'walk', 'v', 'far']\n",
            "After stop word removal: ['yup', 'msg', 'tat', 'yijue', 'tot', 'group', 'mate', 'cos', 'meeting', 'today', 'mah', 'im', 'askin', 'leaving', 'earlier', 'wat', 'mah', 'cos', 'mayb', 'haf', 'walk', 'v', 'far']\n",
            "After lemmatization: ['yup', 'msg', 'tat', 'yijue', 'tot', 'group', 'mate', 'co', 'meeting', 'today', 'mah', 'im', 'askin', 'leaving', 'earlier', 'wat', 'mah', 'co', 'mayb', 'haf', 'walk', 'v', 'far']\n",
            "After lowercasing: ... are you in the pub?\n",
            "After removing special chars:  are you in the pub\n",
            "After tokenization: ['are', 'you', 'in', 'the', 'pub']\n",
            "After stop word removal: ['pub']\n",
            "After lemmatization: ['pub']\n",
            "After lowercasing: there is a first time for everything :)\n",
            "After removing special chars: there is a first time for everything \n",
            "After tokenization: ['there', 'is', 'a', 'first', 'time', 'for', 'everything']\n",
            "After stop word removal: ['first', 'time', 'everything']\n",
            "After lemmatization: ['first', 'time', 'everything']\n",
            "After lowercasing: daddy, shu shu is looking 4 u... u wan me 2 tell him u're not in singapore or wat?\n",
            "After removing special chars: daddy shu shu is looking  u u wan me  tell him ure not in singapore or wat\n",
            "After tokenization: ['daddy', 'shu', 'shu', 'is', 'looking', 'u', 'u', 'wan', 'me', 'tell', 'him', 'ure', 'not', 'in', 'singapore', 'or', 'wat']\n",
            "After stop word removal: ['daddy', 'shu', 'shu', 'looking', 'u', 'u', 'wan', 'tell', 'ure', 'singapore', 'wat']\n",
            "After lemmatization: ['daddy', 'shu', 'shu', 'looking', 'u', 'u', 'wan', 'tell', 'ure', 'singapore', 'wat']\n",
            "After lowercasing: i ask if u meeting da ge tmr nite...\n",
            "After removing special chars: i ask if u meeting da ge tmr nite\n",
            "After tokenization: ['i', 'ask', 'if', 'u', 'meeting', 'da', 'ge', 'tmr', 'nite']\n",
            "After stop word removal: ['ask', 'u', 'meeting', 'da', 'ge', 'tmr', 'nite']\n",
            "After lemmatization: ['ask', 'u', 'meeting', 'da', 'ge', 'tmr', 'nite']\n",
            "After lowercasing: gr8. so how do you handle the victoria island traffic. plus when's the album due\n",
            "After removing special chars: gr so how do you handle the victoria island traffic plus whens the album due\n",
            "After tokenization: ['gr', 'so', 'how', 'do', 'you', 'handle', 'the', 'victoria', 'island', 'traffic', 'plus', 'whens', 'the', 'album', 'due']\n",
            "After stop word removal: ['gr', 'handle', 'victoria', 'island', 'traffic', 'plus', 'whens', 'album', 'due']\n",
            "After lemmatization: ['gr', 'handle', 'victoria', 'island', 'traffic', 'plus', 'whens', 'album', 'due']\n",
            "After lowercasing: nite nite pocay wocay luv u more than n e thing 4eva i promise ring u 2morrowxxxx\n",
            "After removing special chars: nite nite pocay wocay luv u more than n e thing eva i promise ring u morrowxxxx\n",
            "After tokenization: ['nite', 'nite', 'pocay', 'wocay', 'luv', 'u', 'more', 'than', 'n', 'e', 'thing', 'eva', 'i', 'promise', 'ring', 'u', 'morrowxxxx']\n",
            "After stop word removal: ['nite', 'nite', 'pocay', 'wocay', 'luv', 'u', 'n', 'e', 'thing', 'eva', 'promise', 'ring', 'u', 'morrowxxxx']\n",
            "After lemmatization: ['nite', 'nite', 'pocay', 'wocay', 'luv', 'u', 'n', 'e', 'thing', 'eva', 'promise', 'ring', 'u', 'morrowxxxx']\n",
            "After lowercasing: east coast\n",
            "After removing special chars: east coast\n",
            "After tokenization: ['east', 'coast']\n",
            "After stop word removal: ['east', 'coast']\n",
            "After lemmatization: ['east', 'coast']\n",
            "After lowercasing: you should get more chicken broth if you want ramen unless there's some i don't know about\n",
            "After removing special chars: you should get more chicken broth if you want ramen unless theres some i dont know about\n",
            "After tokenization: ['you', 'should', 'get', 'more', 'chicken', 'broth', 'if', 'you', 'want', 'ramen', 'unless', 'theres', 'some', 'i', 'dont', 'know', 'about']\n",
            "After stop word removal: ['get', 'chicken', 'broth', 'want', 'ramen', 'unless', 'theres', 'dont', 'know']\n",
            "After lemmatization: ['get', 'chicken', 'broth', 'want', 'ramen', 'unless', 'there', 'dont', 'know']\n",
            "After lowercasing: my slave! i want you to take 2 or 3 pictures of yourself today in bright light on your cell phone! bright light!\n",
            "After removing special chars: my slave i want you to take  or  pictures of yourself today in bright light on your cell phone bright light\n",
            "After tokenization: ['my', 'slave', 'i', 'want', 'you', 'to', 'take', 'or', 'pictures', 'of', 'yourself', 'today', 'in', 'bright', 'light', 'on', 'your', 'cell', 'phone', 'bright', 'light']\n",
            "After stop word removal: ['slave', 'want', 'take', 'pictures', 'today', 'bright', 'light', 'cell', 'phone', 'bright', 'light']\n",
            "After lemmatization: ['slave', 'want', 'take', 'picture', 'today', 'bright', 'light', 'cell', 'phone', 'bright', 'light']\n",
            "After lowercasing: nope. i just forgot. will show next week\n",
            "After removing special chars: nope i just forgot will show next week\n",
            "After tokenization: ['nope', 'i', 'just', 'forgot', 'will', 'show', 'next', 'week']\n",
            "After stop word removal: ['nope', 'forgot', 'show', 'next', 'week']\n",
            "After lemmatization: ['nope', 'forgot', 'show', 'next', 'week']\n",
            "After lowercasing: so how are you really. what are you up to. how's the masters. and so on.\n",
            "After removing special chars: so how are you really what are you up to hows the masters and so on\n",
            "After tokenization: ['so', 'how', 'are', 'you', 'really', 'what', 'are', 'you', 'up', 'to', 'hows', 'the', 'masters', 'and', 'so', 'on']\n",
            "After stop word removal: ['really', 'hows', 'masters']\n",
            "After lemmatization: ['really', 'hows', 'master']\n",
            "After lowercasing: i'm at bruce &amp; fowler now but i'm in my mom's car so i can't park (long story)\n",
            "After removing special chars: im at bruce amp fowler now but im in my moms car so i cant park long story\n",
            "After tokenization: ['im', 'at', 'bruce', 'amp', 'fowler', 'now', 'but', 'im', 'in', 'my', 'moms', 'car', 'so', 'i', 'cant', 'park', 'long', 'story']\n",
            "After stop word removal: ['im', 'bruce', 'amp', 'fowler', 'im', 'moms', 'car', 'cant', 'park', 'long', 'story']\n",
            "After lemmatization: ['im', 'bruce', 'amp', 'fowler', 'im', 'mom', 'car', 'cant', 'park', 'long', 'story']\n",
            "After lowercasing: i dont know oh. hopefully this month.\n",
            "After removing special chars: i dont know oh hopefully this month\n",
            "After tokenization: ['i', 'dont', 'know', 'oh', 'hopefully', 'this', 'month']\n",
            "After stop word removal: ['dont', 'know', 'oh', 'hopefully', 'month']\n",
            "After lemmatization: ['dont', 'know', 'oh', 'hopefully', 'month']\n",
            "After lowercasing: hi elaine, is today's meeting confirmed?\n",
            "After removing special chars: hi elaine is todays meeting confirmed\n",
            "After tokenization: ['hi', 'elaine', 'is', 'todays', 'meeting', 'confirmed']\n",
            "After stop word removal: ['hi', 'elaine', 'todays', 'meeting', 'confirmed']\n",
            "After lemmatization: ['hi', 'elaine', 'today', 'meeting', 'confirmed']\n",
            "After lowercasing: ok k..sry i knw 2 siva..tats y i askd..\n",
            "After removing special chars: ok ksry i knw  sivatats y i askd\n",
            "After tokenization: ['ok', 'ksry', 'i', 'knw', 'sivatats', 'y', 'i', 'askd']\n",
            "After stop word removal: ['ok', 'ksry', 'knw', 'sivatats', 'askd']\n",
            "After lemmatization: ['ok', 'ksry', 'knw', 'sivatats', 'askd']\n",
            "After lowercasing: sorry, i'll call later\n",
            "After removing special chars: sorry ill call later\n",
            "After tokenization: ['sorry', 'ill', 'call', 'later']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later']\n",
            "After lowercasing: u horrible gal... u knew dat i was going out wif him yest n u still come n ask me...\n",
            "After removing special chars: u horrible gal u knew dat i was going out wif him yest n u still come n ask me\n",
            "After tokenization: ['u', 'horrible', 'gal', 'u', 'knew', 'dat', 'i', 'was', 'going', 'out', 'wif', 'him', 'yest', 'n', 'u', 'still', 'come', 'n', 'ask', 'me']\n",
            "After stop word removal: ['u', 'horrible', 'gal', 'u', 'knew', 'dat', 'going', 'wif', 'yest', 'n', 'u', 'still', 'come', 'n', 'ask']\n",
            "After lemmatization: ['u', 'horrible', 'gal', 'u', 'knew', 'dat', 'going', 'wif', 'yest', 'n', 'u', 'still', 'come', 'n', 'ask']\n",
            "After lowercasing: otherwise had part time job na-tuition..\n",
            "After removing special chars: otherwise had part time job natuition\n",
            "After tokenization: ['otherwise', 'had', 'part', 'time', 'job', 'natuition']\n",
            "After stop word removal: ['otherwise', 'part', 'time', 'job', 'natuition']\n",
            "After lemmatization: ['otherwise', 'part', 'time', 'job', 'natuition']\n",
            "After lowercasing: oh yeah! and my diet just flew out the window\n",
            "After removing special chars: oh yeah and my diet just flew out the window\n",
            "After tokenization: ['oh', 'yeah', 'and', 'my', 'diet', 'just', 'flew', 'out', 'the', 'window']\n",
            "After stop word removal: ['oh', 'yeah', 'diet', 'flew', 'window']\n",
            "After lemmatization: ['oh', 'yeah', 'diet', 'flew', 'window']\n",
            "After lowercasing: santa calling! would your little ones like a call from santa xmas eve? call 09058094583 to book your time.\n",
            "After removing special chars: santa calling would your little ones like a call from santa xmas eve call  to book your time\n",
            "After tokenization: ['santa', 'calling', 'would', 'your', 'little', 'ones', 'like', 'a', 'call', 'from', 'santa', 'xmas', 'eve', 'call', 'to', 'book', 'your', 'time']\n",
            "After stop word removal: ['santa', 'calling', 'would', 'little', 'ones', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time']\n",
            "After lemmatization: ['santa', 'calling', 'would', 'little', 'one', 'like', 'call', 'santa', 'xmas', 'eve', 'call', 'book', 'time']\n",
            "After lowercasing: you didnt complete your gist oh.\n",
            "After removing special chars: you didnt complete your gist oh\n",
            "After tokenization: ['you', 'didnt', 'complete', 'your', 'gist', 'oh']\n",
            "After stop word removal: ['didnt', 'complete', 'gist', 'oh']\n",
            "After lemmatization: ['didnt', 'complete', 'gist', 'oh']\n",
            "After lowercasing: er yeah, i will b there at 15:26, sorry! just tell me which pub/cafe to sit in and come wen u can\n",
            "After removing special chars: er yeah i will b there at  sorry just tell me which pubcafe to sit in and come wen u can\n",
            "After tokenization: ['er', 'yeah', 'i', 'will', 'b', 'there', 'at', 'sorry', 'just', 'tell', 'me', 'which', 'pubcafe', 'to', 'sit', 'in', 'and', 'come', 'wen', 'u', 'can']\n",
            "After stop word removal: ['er', 'yeah', 'b', 'sorry', 'tell', 'pubcafe', 'sit', 'come', 'wen', 'u']\n",
            "After lemmatization: ['er', 'yeah', 'b', 'sorry', 'tell', 'pubcafe', 'sit', 'come', 'wen', 'u']\n",
            "After lowercasing: if you can make it any time tonight or whenever you can it's cool, just text me whenever you're around\n",
            "After removing special chars: if you can make it any time tonight or whenever you can its cool just text me whenever youre around\n",
            "After tokenization: ['if', 'you', 'can', 'make', 'it', 'any', 'time', 'tonight', 'or', 'whenever', 'you', 'can', 'its', 'cool', 'just', 'text', 'me', 'whenever', 'youre', 'around']\n",
            "After stop word removal: ['make', 'time', 'tonight', 'whenever', 'cool', 'text', 'whenever', 'youre', 'around']\n",
            "After lemmatization: ['make', 'time', 'tonight', 'whenever', 'cool', 'text', 'whenever', 'youre', 'around']\n",
            "After lowercasing: if i was i wasn't paying attention\n",
            "After removing special chars: if i was i wasnt paying attention\n",
            "After tokenization: ['if', 'i', 'was', 'i', 'wasnt', 'paying', 'attention']\n",
            "After stop word removal: ['wasnt', 'paying', 'attention']\n",
            "After lemmatization: ['wasnt', 'paying', 'attention']\n",
            "After lowercasing: thanx a lot 4 ur help!\n",
            "After removing special chars: thanx a lot  ur help\n",
            "After tokenization: ['thanx', 'a', 'lot', 'ur', 'help']\n",
            "After stop word removal: ['thanx', 'lot', 'ur', 'help']\n",
            "After lemmatization: ['thanx', 'lot', 'ur', 'help']\n",
            "After lowercasing: you're gonna have to be way more specific than that\n",
            "After removing special chars: youre gonna have to be way more specific than that\n",
            "After tokenization: ['youre', 'gon', 'na', 'have', 'to', 'be', 'way', 'more', 'specific', 'than', 'that']\n",
            "After stop word removal: ['youre', 'gon', 'na', 'way', 'specific']\n",
            "After lemmatization: ['youre', 'gon', 'na', 'way', 'specific']\n",
            "After lowercasing: jesus armand really is trying to tell everybody he can find\n",
            "After removing special chars: jesus armand really is trying to tell everybody he can find\n",
            "After tokenization: ['jesus', 'armand', 'really', 'is', 'trying', 'to', 'tell', 'everybody', 'he', 'can', 'find']\n",
            "After stop word removal: ['jesus', 'armand', 'really', 'trying', 'tell', 'everybody', 'find']\n",
            "After lemmatization: ['jesus', 'armand', 'really', 'trying', 'tell', 'everybody', 'find']\n",
            "After lowercasing: i'm wif him now buying tix lar...\n",
            "After removing special chars: im wif him now buying tix lar\n",
            "After tokenization: ['im', 'wif', 'him', 'now', 'buying', 'tix', 'lar']\n",
            "After stop word removal: ['im', 'wif', 'buying', 'tix', 'lar']\n",
            "After lemmatization: ['im', 'wif', 'buying', 'tix', 'lar']\n",
            "After lowercasing: mode men or have you left.\n",
            "After removing special chars: mode men or have you left\n",
            "After tokenization: ['mode', 'men', 'or', 'have', 'you', 'left']\n",
            "After stop word removal: ['mode', 'men', 'left']\n",
            "After lemmatization: ['mode', 'men', 'left']\n",
            "After lowercasing: am slow in using biola's fne\n",
            "After removing special chars: am slow in using biolas fne\n",
            "After tokenization: ['am', 'slow', 'in', 'using', 'biolas', 'fne']\n",
            "After stop word removal: ['slow', 'using', 'biolas', 'fne']\n",
            "After lemmatization: ['slow', 'using', 'biolas', 'fne']\n",
            "After lowercasing: \\what are youdoing later? sar xxx\\\"\"\n",
            "After removing special chars: what are youdoing later sar xxx\n",
            "After tokenization: ['what', 'are', 'youdoing', 'later', 'sar', 'xxx']\n",
            "After stop word removal: ['youdoing', 'later', 'sar', 'xxx']\n",
            "After lemmatization: ['youdoing', 'later', 'sar', 'xxx']\n",
            "After lowercasing: hey i've booked the 2 lessons on sun liao...\n",
            "After removing special chars: hey ive booked the  lessons on sun liao\n",
            "After tokenization: ['hey', 'ive', 'booked', 'the', 'lessons', 'on', 'sun', 'liao']\n",
            "After stop word removal: ['hey', 'ive', 'booked', 'lessons', 'sun', 'liao']\n",
            "After lemmatization: ['hey', 'ive', 'booked', 'lesson', 'sun', 'liao']\n",
            "After lowercasing: thank you. do you generally date the brothas?\n",
            "After removing special chars: thank you do you generally date the brothas\n",
            "After tokenization: ['thank', 'you', 'do', 'you', 'generally', 'date', 'the', 'brothas']\n",
            "After stop word removal: ['thank', 'generally', 'date', 'brothas']\n",
            "After lemmatization: ['thank', 'generally', 'date', 'brothas']\n",
            "After lowercasing: by the way, make sure u get train to worc foregate street not shrub hill. have fun night x\n",
            "After removing special chars: by the way make sure u get train to worc foregate street not shrub hill have fun night x\n",
            "After tokenization: ['by', 'the', 'way', 'make', 'sure', 'u', 'get', 'train', 'to', 'worc', 'foregate', 'street', 'not', 'shrub', 'hill', 'have', 'fun', 'night', 'x']\n",
            "After stop word removal: ['way', 'make', 'sure', 'u', 'get', 'train', 'worc', 'foregate', 'street', 'shrub', 'hill', 'fun', 'night', 'x']\n",
            "After lemmatization: ['way', 'make', 'sure', 'u', 'get', 'train', 'worc', 'foregate', 'street', 'shrub', 'hill', 'fun', 'night', 'x']\n",
            "After lowercasing: i thought i'd get him a watch, just cos thats the kind of thing u get4an18th. and he loves so much!\n",
            "After removing special chars: i thought id get him a watch just cos thats the kind of thing u getanth and he loves so much\n",
            "After tokenization: ['i', 'thought', 'id', 'get', 'him', 'a', 'watch', 'just', 'cos', 'thats', 'the', 'kind', 'of', 'thing', 'u', 'getanth', 'and', 'he', 'loves', 'so', 'much']\n",
            "After stop word removal: ['thought', 'id', 'get', 'watch', 'cos', 'thats', 'kind', 'thing', 'u', 'getanth', 'loves', 'much']\n",
            "After lemmatization: ['thought', 'id', 'get', 'watch', 'co', 'thats', 'kind', 'thing', 'u', 'getanth', 'love', 'much']\n",
            "After lowercasing: you have won a guaranteed 32000 award or maybe even å£1000 cash to claim ur award call free on 0800 ..... (18+). its a legitimat efreefone number wat do u think???\n",
            "After removing special chars: you have won a guaranteed  award or maybe even  cash to claim ur award call free on    its a legitimat efreefone number wat do u think\n",
            "After tokenization: ['you', 'have', 'won', 'a', 'guaranteed', 'award', 'or', 'maybe', 'even', 'cash', 'to', 'claim', 'ur', 'award', 'call', 'free', 'on', 'its', 'a', 'legitimat', 'efreefone', 'number', 'wat', 'do', 'u', 'think']\n",
            "After stop word removal: ['guaranteed', 'award', 'maybe', 'even', 'cash', 'claim', 'ur', 'award', 'call', 'free', 'legitimat', 'efreefone', 'number', 'wat', 'u', 'think']\n",
            "After lemmatization: ['guaranteed', 'award', 'maybe', 'even', 'cash', 'claim', 'ur', 'award', 'call', 'free', 'legitimat', 'efreefone', 'number', 'wat', 'u', 'think']\n",
            "After lowercasing: good morning. at the repair shop--the only reason i'm up at this hour.\n",
            "After removing special chars: good morning at the repair shopthe only reason im up at this hour\n",
            "After tokenization: ['good', 'morning', 'at', 'the', 'repair', 'shopthe', 'only', 'reason', 'im', 'up', 'at', 'this', 'hour']\n",
            "After stop word removal: ['good', 'morning', 'repair', 'shopthe', 'reason', 'im', 'hour']\n",
            "After lemmatization: ['good', 'morning', 'repair', 'shopthe', 'reason', 'im', 'hour']\n",
            "After lowercasing: and that's fine, i got enough bud to last most of the night at least\n",
            "After removing special chars: and thats fine i got enough bud to last most of the night at least\n",
            "After tokenization: ['and', 'thats', 'fine', 'i', 'got', 'enough', 'bud', 'to', 'last', 'most', 'of', 'the', 'night', 'at', 'least']\n",
            "After stop word removal: ['thats', 'fine', 'got', 'enough', 'bud', 'last', 'night', 'least']\n",
            "After lemmatization: ['thats', 'fine', 'got', 'enough', 'bud', 'last', 'night', 'least']\n",
            "After lowercasing: i am back. good journey! let me know if you need any of the receipts. shall i tell you like the pendent?\n",
            "After removing special chars: i am back good journey let me know if you need any of the receipts shall i tell you like the pendent\n",
            "After tokenization: ['i', 'am', 'back', 'good', 'journey', 'let', 'me', 'know', 'if', 'you', 'need', 'any', 'of', 'the', 'receipts', 'shall', 'i', 'tell', 'you', 'like', 'the', 'pendent']\n",
            "After stop word removal: ['back', 'good', 'journey', 'let', 'know', 'need', 'receipts', 'shall', 'tell', 'like', 'pendent']\n",
            "After lemmatization: ['back', 'good', 'journey', 'let', 'know', 'need', 'receipt', 'shall', 'tell', 'like', 'pendent']\n",
            "After lowercasing: so that takes away some money worries\n",
            "After removing special chars: so that takes away some money worries\n",
            "After tokenization: ['so', 'that', 'takes', 'away', 'some', 'money', 'worries']\n",
            "After stop word removal: ['takes', 'away', 'money', 'worries']\n",
            "After lemmatization: ['take', 'away', 'money', 'worry']\n",
            "After lowercasing: aight we can pick some up, you open before tonight?\n",
            "After removing special chars: aight we can pick some up you open before tonight\n",
            "After tokenization: ['aight', 'we', 'can', 'pick', 'some', 'up', 'you', 'open', 'before', 'tonight']\n",
            "After stop word removal: ['aight', 'pick', 'open', 'tonight']\n",
            "After lemmatization: ['aight', 'pick', 'open', 'tonight']\n",
            "After lowercasing: latest news! police station toilet stolen, cops have nothing to go on!\n",
            "After removing special chars: latest news police station toilet stolen cops have nothing to go on\n",
            "After tokenization: ['latest', 'news', 'police', 'station', 'toilet', 'stolen', 'cops', 'have', 'nothing', 'to', 'go', 'on']\n",
            "After stop word removal: ['latest', 'news', 'police', 'station', 'toilet', 'stolen', 'cops', 'nothing', 'go']\n",
            "After lemmatization: ['latest', 'news', 'police', 'station', 'toilet', 'stolen', 'cop', 'nothing', 'go']\n",
            "After lowercasing: sac needs to carry on:)\n",
            "After removing special chars: sac needs to carry on\n",
            "After tokenization: ['sac', 'needs', 'to', 'carry', 'on']\n",
            "After stop word removal: ['sac', 'needs', 'carry']\n",
            "After lemmatization: ['sac', 'need', 'carry']\n",
            "After lowercasing: just sing hu. i think its also important to find someone female that know the place well preferably a citizen that is also smart to help you navigate through. even things like choosing a phone plan require guidance. when in doubt ask especially girls.\n",
            "After removing special chars: just sing hu i think its also important to find someone female that know the place well preferably a citizen that is also smart to help you navigate through even things like choosing a phone plan require guidance when in doubt ask especially girls\n",
            "After tokenization: ['just', 'sing', 'hu', 'i', 'think', 'its', 'also', 'important', 'to', 'find', 'someone', 'female', 'that', 'know', 'the', 'place', 'well', 'preferably', 'a', 'citizen', 'that', 'is', 'also', 'smart', 'to', 'help', 'you', 'navigate', 'through', 'even', 'things', 'like', 'choosing', 'a', 'phone', 'plan', 'require', 'guidance', 'when', 'in', 'doubt', 'ask', 'especially', 'girls']\n",
            "After stop word removal: ['sing', 'hu', 'think', 'also', 'important', 'find', 'someone', 'female', 'know', 'place', 'well', 'preferably', 'citizen', 'also', 'smart', 'help', 'navigate', 'even', 'things', 'like', 'choosing', 'phone', 'plan', 'require', 'guidance', 'doubt', 'ask', 'especially', 'girls']\n",
            "After lemmatization: ['sing', 'hu', 'think', 'also', 'important', 'find', 'someone', 'female', 'know', 'place', 'well', 'preferably', 'citizen', 'also', 'smart', 'help', 'navigate', 'even', 'thing', 'like', 'choosing', 'phone', 'plan', 'require', 'guidance', 'doubt', 'ask', 'especially', 'girl']\n",
            "After lowercasing: what???? hello wats talks email address?\n",
            "After removing special chars: what hello wats talks email address\n",
            "After tokenization: ['what', 'hello', 'wats', 'talks', 'email', 'address']\n",
            "After stop word removal: ['hello', 'wats', 'talks', 'email', 'address']\n",
            "After lemmatization: ['hello', 'wats', 'talk', 'email', 'address']\n",
            "After lowercasing: except theres a chick with huge boobs.\n",
            "After removing special chars: except theres a chick with huge boobs\n",
            "After tokenization: ['except', 'theres', 'a', 'chick', 'with', 'huge', 'boobs']\n",
            "After stop word removal: ['except', 'theres', 'chick', 'huge', 'boobs']\n",
            "After lemmatization: ['except', 'there', 'chick', 'huge', 'boob']\n",
            "After lowercasing: im just wondering what your doing right now?\n",
            "After removing special chars: im just wondering what your doing right now\n",
            "After tokenization: ['im', 'just', 'wondering', 'what', 'your', 'doing', 'right', 'now']\n",
            "After stop word removal: ['im', 'wondering', 'right']\n",
            "After lemmatization: ['im', 'wondering', 'right']\n",
            "After lowercasing: wishing you a beautiful day. each moment revealing even more things to keep you smiling. do enjoy it.\n",
            "After removing special chars: wishing you a beautiful day each moment revealing even more things to keep you smiling do enjoy it\n",
            "After tokenization: ['wishing', 'you', 'a', 'beautiful', 'day', 'each', 'moment', 'revealing', 'even', 'more', 'things', 'to', 'keep', 'you', 'smiling', 'do', 'enjoy', 'it']\n",
            "After stop word removal: ['wishing', 'beautiful', 'day', 'moment', 'revealing', 'even', 'things', 'keep', 'smiling', 'enjoy']\n",
            "After lemmatization: ['wishing', 'beautiful', 'day', 'moment', 'revealing', 'even', 'thing', 'keep', 'smiling', 'enjoy']\n",
            "After lowercasing: \\for the most sparkling shopping breaks from 45 per person; call 0121 2025050 or visit www.shortbreaks.org.uk\\\"\"\n",
            "After removing special chars: for the most sparkling shopping breaks from  per person call   or visit wwwshortbreaksorguk\n",
            "After tokenization: ['for', 'the', 'most', 'sparkling', 'shopping', 'breaks', 'from', 'per', 'person', 'call', 'or', 'visit', 'wwwshortbreaksorguk']\n",
            "After stop word removal: ['sparkling', 'shopping', 'breaks', 'per', 'person', 'call', 'visit', 'wwwshortbreaksorguk']\n",
            "After lemmatization: ['sparkling', 'shopping', 'break', 'per', 'person', 'call', 'visit', 'wwwshortbreaksorguk']\n",
            "After lowercasing: arun can u transfr me d amt\n",
            "After removing special chars: arun can u transfr me d amt\n",
            "After tokenization: ['arun', 'can', 'u', 'transfr', 'me', 'd', 'amt']\n",
            "After stop word removal: ['arun', 'u', 'transfr', 'amt']\n",
            "After lemmatization: ['arun', 'u', 'transfr', 'amt']\n",
            "After lowercasing: sorry, i'll call later\n",
            "After removing special chars: sorry ill call later\n",
            "After tokenization: ['sorry', 'ill', 'call', 'later']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later']\n",
            "After lowercasing: if you hear a loud scream in about &lt;#&gt; minutes its cause my gyno will be shoving things up me that don't belong :/\n",
            "After removing special chars: if you hear a loud scream in about ltgt minutes its cause my gyno will be shoving things up me that dont belong \n",
            "After tokenization: ['if', 'you', 'hear', 'a', 'loud', 'scream', 'in', 'about', 'ltgt', 'minutes', 'its', 'cause', 'my', 'gyno', 'will', 'be', 'shoving', 'things', 'up', 'me', 'that', 'dont', 'belong']\n",
            "After stop word removal: ['hear', 'loud', 'scream', 'ltgt', 'minutes', 'cause', 'gyno', 'shoving', 'things', 'dont', 'belong']\n",
            "After lemmatization: ['hear', 'loud', 'scream', 'ltgt', 'minute', 'cause', 'gyno', 'shoving', 'thing', 'dont', 'belong']\n",
            "After lowercasing: december only! had your mobile 11mths+? you are entitled to update to the latest colour camera mobile for free! call the mobile update co free on 08002986906\n",
            "After removing special chars: december only had your mobile mths you are entitled to update to the latest colour camera mobile for free call the mobile update co free on \n",
            "After tokenization: ['december', 'only', 'had', 'your', 'mobile', 'mths', 'you', 'are', 'entitled', 'to', 'update', 'to', 'the', 'latest', 'colour', 'camera', 'mobile', 'for', 'free', 'call', 'the', 'mobile', 'update', 'co', 'free', 'on']\n",
            "After stop word removal: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
            "After lemmatization: ['december', 'mobile', 'mths', 'entitled', 'update', 'latest', 'colour', 'camera', 'mobile', 'free', 'call', 'mobile', 'update', 'co', 'free']\n",
            "After lowercasing: ok i thk i got it. then u wan me 2 come now or wat?\n",
            "After removing special chars: ok i thk i got it then u wan me  come now or wat\n",
            "After tokenization: ['ok', 'i', 'thk', 'i', 'got', 'it', 'then', 'u', 'wan', 'me', 'come', 'now', 'or', 'wat']\n",
            "After stop word removal: ['ok', 'thk', 'got', 'u', 'wan', 'come', 'wat']\n",
            "After lemmatization: ['ok', 'thk', 'got', 'u', 'wan', 'come', 'wat']\n",
            "After lowercasing: txt: call to no: 86888 & claim your reward of 3 hours talk time to use from your phone now! subscribe6gbp/mnth inc 3hrs 16 stop?txtstop www.gamb.tv\n",
            "After removing special chars: txt call to no   claim your reward of  hours talk time to use from your phone now subscribegbpmnth inc hrs  stoptxtstop wwwgambtv\n",
            "After tokenization: ['txt', 'call', 'to', 'no', 'claim', 'your', 'reward', 'of', 'hours', 'talk', 'time', 'to', 'use', 'from', 'your', 'phone', 'now', 'subscribegbpmnth', 'inc', 'hrs', 'stoptxtstop', 'wwwgambtv']\n",
            "After stop word removal: ['txt', 'call', 'claim', 'reward', 'hours', 'talk', 'time', 'use', 'phone', 'subscribegbpmnth', 'inc', 'hrs', 'stoptxtstop', 'wwwgambtv']\n",
            "After lemmatization: ['txt', 'call', 'claim', 'reward', 'hour', 'talk', 'time', 'use', 'phone', 'subscribegbpmnth', 'inc', 'hr', 'stoptxtstop', 'wwwgambtv']\n",
            "After lowercasing: u goin out 2nite?\n",
            "After removing special chars: u goin out nite\n",
            "After tokenization: ['u', 'goin', 'out', 'nite']\n",
            "After stop word removal: ['u', 'goin', 'nite']\n",
            "After lemmatization: ['u', 'goin', 'nite']\n",
            "After lowercasing: i will treasure every moment we spend together...\n",
            "After removing special chars: i will treasure every moment we spend together\n",
            "After tokenization: ['i', 'will', 'treasure', 'every', 'moment', 'we', 'spend', 'together']\n",
            "After stop word removal: ['treasure', 'every', 'moment', 'spend', 'together']\n",
            "After lemmatization: ['treasure', 'every', 'moment', 'spend', 'together']\n",
            "After lowercasing: shall i bring us a bottle of wine to keep us amused? only joking! iû÷ll bring one anyway\n",
            "After removing special chars: shall i bring us a bottle of wine to keep us amused only joking ill bring one anyway\n",
            "After tokenization: ['shall', 'i', 'bring', 'us', 'a', 'bottle', 'of', 'wine', 'to', 'keep', 'us', 'amused', 'only', 'joking', 'ill', 'bring', 'one', 'anyway']\n",
            "After stop word removal: ['shall', 'bring', 'us', 'bottle', 'wine', 'keep', 'us', 'amused', 'joking', 'ill', 'bring', 'one', 'anyway']\n",
            "After lemmatization: ['shall', 'bring', 'u', 'bottle', 'wine', 'keep', 'u', 'amused', 'joking', 'ill', 'bring', 'one', 'anyway']\n",
            "After lowercasing: http//tms. widelive.com/index. wml?id=820554ad0a1705572711&first=trueåác c ringtoneåá\n",
            "After removing special chars: httptms widelivecomindex wmlidadafirsttruec c ringtone\n",
            "After tokenization: ['httptms', 'widelivecomindex', 'wmlidadafirsttruec', 'c', 'ringtone']\n",
            "After stop word removal: ['httptms', 'widelivecomindex', 'wmlidadafirsttruec', 'c', 'ringtone']\n",
            "After lemmatization: ['httptms', 'widelivecomindex', 'wmlidadafirsttruec', 'c', 'ringtone']\n",
            "After lowercasing: get your garden ready for summer with a free selection of summer bulbs and seeds worth å£33:50 only with the scotsman this saturday. to stop go2 notxt.co.uk\n",
            "After removing special chars: get your garden ready for summer with a free selection of summer bulbs and seeds worth  only with the scotsman this saturday to stop go notxtcouk\n",
            "After tokenization: ['get', 'your', 'garden', 'ready', 'for', 'summer', 'with', 'a', 'free', 'selection', 'of', 'summer', 'bulbs', 'and', 'seeds', 'worth', 'only', 'with', 'the', 'scotsman', 'this', 'saturday', 'to', 'stop', 'go', 'notxtcouk']\n",
            "After stop word removal: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulbs', 'seeds', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxtcouk']\n",
            "After lemmatization: ['get', 'garden', 'ready', 'summer', 'free', 'selection', 'summer', 'bulb', 'seed', 'worth', 'scotsman', 'saturday', 'stop', 'go', 'notxtcouk']\n",
            "After lowercasing: urgent! last weekend's draw shows that you have won å£1000 cash or a spanish holiday! call now 09050000332 to claim. t&c: rstm, sw7 3ss. 150ppm\n",
            "After removing special chars: urgent last weekends draw shows that you have won  cash or a spanish holiday call now  to claim tc rstm sw ss ppm\n",
            "After tokenization: ['urgent', 'last', 'weekends', 'draw', 'shows', 'that', 'you', 'have', 'won', 'cash', 'or', 'a', 'spanish', 'holiday', 'call', 'now', 'to', 'claim', 'tc', 'rstm', 'sw', 'ss', 'ppm']\n",
            "After stop word removal: ['urgent', 'last', 'weekends', 'draw', 'shows', 'cash', 'spanish', 'holiday', 'call', 'claim', 'tc', 'rstm', 'sw', 'ss', 'ppm']\n",
            "After lemmatization: ['urgent', 'last', 'weekend', 'draw', 'show', 'cash', 'spanish', 'holiday', 'call', 'claim', 'tc', 'rstm', 'sw', 's', 'ppm']\n",
            "After lowercasing: ok lor.\n",
            "After removing special chars: ok lor\n",
            "After tokenization: ['ok', 'lor']\n",
            "After stop word removal: ['ok', 'lor']\n",
            "After lemmatization: ['ok', 'lor']\n",
            "After lowercasing: i thought slide is enough.\n",
            "After removing special chars: i thought slide is enough\n",
            "After tokenization: ['i', 'thought', 'slide', 'is', 'enough']\n",
            "After stop word removal: ['thought', 'slide', 'enough']\n",
            "After lemmatization: ['thought', 'slide', 'enough']\n",
            "After lowercasing: yup\n",
            "After removing special chars: yup\n",
            "After tokenization: ['yup']\n",
            "After stop word removal: ['yup']\n",
            "After lemmatization: ['yup']\n",
            "After lowercasing: well obviously not because all the people in my cool college life went home ;_;\n",
            "After removing special chars: well obviously not because all the people in my cool college life went home \n",
            "After tokenization: ['well', 'obviously', 'not', 'because', 'all', 'the', 'people', 'in', 'my', 'cool', 'college', 'life', 'went', 'home']\n",
            "After stop word removal: ['well', 'obviously', 'people', 'cool', 'college', 'life', 'went', 'home']\n",
            "After lemmatization: ['well', 'obviously', 'people', 'cool', 'college', 'life', 'went', 'home']\n",
            "After lowercasing: ok lor ì_ reaching then message me.\n",
            "After removing special chars: ok lor  reaching then message me\n",
            "After tokenization: ['ok', 'lor', 'reaching', 'then', 'message', 'me']\n",
            "After stop word removal: ['ok', 'lor', 'reaching', 'message']\n",
            "After lemmatization: ['ok', 'lor', 'reaching', 'message']\n",
            "After lowercasing: where's mummy's boy ? is he being good or bad ? is he being positive or negative ? why is mummy being made to wait? hmmmm?\n",
            "After removing special chars: wheres mummys boy  is he being good or bad  is he being positive or negative  why is mummy being made to wait hmmmm\n",
            "After tokenization: ['wheres', 'mummys', 'boy', 'is', 'he', 'being', 'good', 'or', 'bad', 'is', 'he', 'being', 'positive', 'or', 'negative', 'why', 'is', 'mummy', 'being', 'made', 'to', 'wait', 'hmmmm']\n",
            "After stop word removal: ['wheres', 'mummys', 'boy', 'good', 'bad', 'positive', 'negative', 'mummy', 'made', 'wait', 'hmmmm']\n",
            "After lemmatization: ['wheres', 'mummy', 'boy', 'good', 'bad', 'positive', 'negative', 'mummy', 'made', 'wait', 'hmmmm']\n",
            "After lowercasing: dhoni have luck to win some big title.so we will win:)\n",
            "After removing special chars: dhoni have luck to win some big titleso we will win\n",
            "After tokenization: ['dhoni', 'have', 'luck', 'to', 'win', 'some', 'big', 'titleso', 'we', 'will', 'win']\n",
            "After stop word removal: ['dhoni', 'luck', 'win', 'big', 'titleso', 'win']\n",
            "After lemmatization: ['dhoni', 'luck', 'win', 'big', 'titleso', 'win']\n",
            "After lowercasing: yes princess! i want to please you every night. your wish is my command...\n",
            "After removing special chars: yes princess i want to please you every night your wish is my command\n",
            "After tokenization: ['yes', 'princess', 'i', 'want', 'to', 'please', 'you', 'every', 'night', 'your', 'wish', 'is', 'my', 'command']\n",
            "After stop word removal: ['yes', 'princess', 'want', 'please', 'every', 'night', 'wish', 'command']\n",
            "After lemmatization: ['yes', 'princess', 'want', 'please', 'every', 'night', 'wish', 'command']\n",
            "After lowercasing: what today-sunday..sunday is holiday..so no work..\n",
            "After removing special chars: what todaysundaysunday is holidayso no work\n",
            "After tokenization: ['what', 'todaysundaysunday', 'is', 'holidayso', 'no', 'work']\n",
            "After stop word removal: ['todaysundaysunday', 'holidayso', 'work']\n",
            "After lemmatization: ['todaysundaysunday', 'holidayso', 'work']\n",
            "After lowercasing: no probably  &lt;#&gt; %.\n",
            "After removing special chars: no probably  ltgt \n",
            "After tokenization: ['no', 'probably', 'ltgt']\n",
            "After stop word removal: ['probably', 'ltgt']\n",
            "After lemmatization: ['probably', 'ltgt']\n",
            "After lowercasing: really do hope the work doesnt get stressful. have a gr8 day.\n",
            "After removing special chars: really do hope the work doesnt get stressful have a gr day\n",
            "After tokenization: ['really', 'do', 'hope', 'the', 'work', 'doesnt', 'get', 'stressful', 'have', 'a', 'gr', 'day']\n",
            "After stop word removal: ['really', 'hope', 'work', 'doesnt', 'get', 'stressful', 'gr', 'day']\n",
            "After lemmatization: ['really', 'hope', 'work', 'doesnt', 'get', 'stressful', 'gr', 'day']\n",
            "After lowercasing: have you seen who's back at holby?!\n",
            "After removing special chars: have you seen whos back at holby\n",
            "After tokenization: ['have', 'you', 'seen', 'whos', 'back', 'at', 'holby']\n",
            "After stop word removal: ['seen', 'whos', 'back', 'holby']\n",
            "After lemmatization: ['seen', 'who', 'back', 'holby']\n",
            "After lowercasing: shall call now dear having food\n",
            "After removing special chars: shall call now dear having food\n",
            "After tokenization: ['shall', 'call', 'now', 'dear', 'having', 'food']\n",
            "After stop word removal: ['shall', 'call', 'dear', 'food']\n",
            "After lemmatization: ['shall', 'call', 'dear', 'food']\n",
            "After lowercasing: urgent we are trying to contact you last weekends draw shows u have won a å£1000 prize guaranteed call 09064017295 claim code k52 valid 12hrs 150p pm\n",
            "After removing special chars: urgent we are trying to contact you last weekends draw shows u have won a  prize guaranteed call  claim code k valid hrs p pm\n",
            "After tokenization: ['urgent', 'we', 'are', 'trying', 'to', 'contact', 'you', 'last', 'weekends', 'draw', 'shows', 'u', 'have', 'won', 'a', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'p', 'pm']\n",
            "After stop word removal: ['urgent', 'trying', 'contact', 'last', 'weekends', 'draw', 'shows', 'u', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hrs', 'p', 'pm']\n",
            "After lemmatization: ['urgent', 'trying', 'contact', 'last', 'weekend', 'draw', 'show', 'u', 'prize', 'guaranteed', 'call', 'claim', 'code', 'k', 'valid', 'hr', 'p', 'pm']\n",
            "After lowercasing: so li hai... me bored now da lecturer repeating last weeks stuff waste time... \n",
            "After removing special chars: so li hai me bored now da lecturer repeating last weeks stuff waste time \n",
            "After tokenization: ['so', 'li', 'hai', 'me', 'bored', 'now', 'da', 'lecturer', 'repeating', 'last', 'weeks', 'stuff', 'waste', 'time']\n",
            "After stop word removal: ['li', 'hai', 'bored', 'da', 'lecturer', 'repeating', 'last', 'weeks', 'stuff', 'waste', 'time']\n",
            "After lemmatization: ['li', 'hai', 'bored', 'da', 'lecturer', 'repeating', 'last', 'week', 'stuff', 'waste', 'time']\n",
            "After lowercasing: , ,  and  picking them up from various points | going 2 yeovil | and they will do the motor project 4 3 hours | and then u take them home. || 12 2 5.30 max. || very easy\n",
            "After removing special chars:    and  picking them up from various points  going  yeovil  and they will do the motor project   hours  and then u take them home     max  very easy\n",
            "After tokenization: ['and', 'picking', 'them', 'up', 'from', 'various', 'points', 'going', 'yeovil', 'and', 'they', 'will', 'do', 'the', 'motor', 'project', 'hours', 'and', 'then', 'u', 'take', 'them', 'home', 'max', 'very', 'easy']\n",
            "After stop word removal: ['picking', 'various', 'points', 'going', 'yeovil', 'motor', 'project', 'hours', 'u', 'take', 'home', 'max', 'easy']\n",
            "After lemmatization: ['picking', 'various', 'point', 'going', 'yeovil', 'motor', 'project', 'hour', 'u', 'take', 'home', 'max', 'easy']\n",
            "After lowercasing: also fuck you and your family for going to rhode island or wherever the fuck and leaving me all alone the week i have a new bong &gt;:(\n",
            "After removing special chars: also fuck you and your family for going to rhode island or wherever the fuck and leaving me all alone the week i have a new bong gt\n",
            "After tokenization: ['also', 'fuck', 'you', 'and', 'your', 'family', 'for', 'going', 'to', 'rhode', 'island', 'or', 'wherever', 'the', 'fuck', 'and', 'leaving', 'me', 'all', 'alone', 'the', 'week', 'i', 'have', 'a', 'new', 'bong', 'gt']\n",
            "After stop word removal: ['also', 'fuck', 'family', 'going', 'rhode', 'island', 'wherever', 'fuck', 'leaving', 'alone', 'week', 'new', 'bong', 'gt']\n",
            "After lemmatization: ['also', 'fuck', 'family', 'going', 'rhode', 'island', 'wherever', 'fuck', 'leaving', 'alone', 'week', 'new', 'bong', 'gt']\n",
            "After lowercasing: ofcourse i also upload some songs\n",
            "After removing special chars: ofcourse i also upload some songs\n",
            "After tokenization: ['ofcourse', 'i', 'also', 'upload', 'some', 'songs']\n",
            "After stop word removal: ['ofcourse', 'also', 'upload', 'songs']\n",
            "After lemmatization: ['ofcourse', 'also', 'upload', 'song']\n",
            "After lowercasing: 2p per min to call germany 08448350055 from your bt line. just 2p per min. check planettalkinstant.com for info & t's & c's. text stop to opt out\n",
            "After removing special chars: p per min to call germany  from your bt line just p per min check planettalkinstantcom for info  ts  cs text stop to opt out\n",
            "After tokenization: ['p', 'per', 'min', 'to', 'call', 'germany', 'from', 'your', 'bt', 'line', 'just', 'p', 'per', 'min', 'check', 'planettalkinstantcom', 'for', 'info', 'ts', 'cs', 'text', 'stop', 'to', 'opt', 'out']\n",
            "After stop word removal: ['p', 'per', 'min', 'call', 'germany', 'bt', 'line', 'p', 'per', 'min', 'check', 'planettalkinstantcom', 'info', 'ts', 'cs', 'text', 'stop', 'opt']\n",
            "After lemmatization: ['p', 'per', 'min', 'call', 'germany', 'bt', 'line', 'p', 'per', 'min', 'check', 'planettalkinstantcom', 'info', 't', 'c', 'text', 'stop', 'opt']\n",
            "After lowercasing: k. i will sent it again\n",
            "After removing special chars: k i will sent it again\n",
            "After tokenization: ['k', 'i', 'will', 'sent', 'it', 'again']\n",
            "After stop word removal: ['k', 'sent']\n",
            "After lemmatization: ['k', 'sent']\n",
            "After lowercasing: oh thanks a lot..i already bought 2 eggs ..\n",
            "After removing special chars: oh thanks a loti already bought  eggs \n",
            "After tokenization: ['oh', 'thanks', 'a', 'loti', 'already', 'bought', 'eggs']\n",
            "After stop word removal: ['oh', 'thanks', 'loti', 'already', 'bought', 'eggs']\n",
            "After lemmatization: ['oh', 'thanks', 'loti', 'already', 'bought', 'egg']\n",
            "After lowercasing: k. i will sent it again\n",
            "After removing special chars: k i will sent it again\n",
            "After tokenization: ['k', 'i', 'will', 'sent', 'it', 'again']\n",
            "After stop word removal: ['k', 'sent']\n",
            "After lemmatization: ['k', 'sent']\n",
            "After lowercasing: u studying in sch or going home? anyway i'll b going 2 sch later.\n",
            "After removing special chars: u studying in sch or going home anyway ill b going  sch later\n",
            "After tokenization: ['u', 'studying', 'in', 'sch', 'or', 'going', 'home', 'anyway', 'ill', 'b', 'going', 'sch', 'later']\n",
            "After stop word removal: ['u', 'studying', 'sch', 'going', 'home', 'anyway', 'ill', 'b', 'going', 'sch', 'later']\n",
            "After lemmatization: ['u', 'studying', 'sch', 'going', 'home', 'anyway', 'ill', 'b', 'going', 'sch', 'later']\n",
            "After lowercasing: marvel mobile play the official ultimate spider-man game (å£4.50) on ur mobile right now. text spider to 83338 for the game & we ll send u a free 8ball wallpaper\n",
            "After removing special chars: marvel mobile play the official ultimate spiderman game  on ur mobile right now text spider to  for the game  we ll send u a free ball wallpaper\n",
            "After tokenization: ['marvel', 'mobile', 'play', 'the', 'official', 'ultimate', 'spiderman', 'game', 'on', 'ur', 'mobile', 'right', 'now', 'text', 'spider', 'to', 'for', 'the', 'game', 'we', 'll', 'send', 'u', 'a', 'free', 'ball', 'wallpaper']\n",
            "After stop word removal: ['marvel', 'mobile', 'play', 'official', 'ultimate', 'spiderman', 'game', 'ur', 'mobile', 'right', 'text', 'spider', 'game', 'send', 'u', 'free', 'ball', 'wallpaper']\n",
            "After lemmatization: ['marvel', 'mobile', 'play', 'official', 'ultimate', 'spiderman', 'game', 'ur', 'mobile', 'right', 'text', 'spider', 'game', 'send', 'u', 'free', 'ball', 'wallpaper']\n",
            "After lowercasing: i think if he rule tamilnadu..then its very tough for our people.\n",
            "After removing special chars: i think if he rule tamilnaduthen its very tough for our people\n",
            "After tokenization: ['i', 'think', 'if', 'he', 'rule', 'tamilnaduthen', 'its', 'very', 'tough', 'for', 'our', 'people']\n",
            "After stop word removal: ['think', 'rule', 'tamilnaduthen', 'tough', 'people']\n",
            "After lemmatization: ['think', 'rule', 'tamilnaduthen', 'tough', 'people']\n",
            "After lowercasing: cool, we shall go and see, have to go to tip anyway. are you at home, got something to drop in later? so lets go to town tonight! maybe mum can take us in.\n",
            "After removing special chars: cool we shall go and see have to go to tip anyway are you at home got something to drop in later so lets go to town tonight maybe mum can take us in\n",
            "After tokenization: ['cool', 'we', 'shall', 'go', 'and', 'see', 'have', 'to', 'go', 'to', 'tip', 'anyway', 'are', 'you', 'at', 'home', 'got', 'something', 'to', 'drop', 'in', 'later', 'so', 'lets', 'go', 'to', 'town', 'tonight', 'maybe', 'mum', 'can', 'take', 'us', 'in']\n",
            "After stop word removal: ['cool', 'shall', 'go', 'see', 'go', 'tip', 'anyway', 'home', 'got', 'something', 'drop', 'later', 'lets', 'go', 'town', 'tonight', 'maybe', 'mum', 'take', 'us']\n",
            "After lemmatization: ['cool', 'shall', 'go', 'see', 'go', 'tip', 'anyway', 'home', 'got', 'something', 'drop', 'later', 'let', 'go', 'town', 'tonight', 'maybe', 'mum', 'take', 'u']\n",
            "After lowercasing: good afternoon, my love ... how goes your day ? how did you sleep ? i hope your well, my boytoy ... i think of you ...\n",
            "After removing special chars: good afternoon my love  how goes your day  how did you sleep  i hope your well my boytoy  i think of you \n",
            "After tokenization: ['good', 'afternoon', 'my', 'love', 'how', 'goes', 'your', 'day', 'how', 'did', 'you', 'sleep', 'i', 'hope', 'your', 'well', 'my', 'boytoy', 'i', 'think', 'of', 'you']\n",
            "After stop word removal: ['good', 'afternoon', 'love', 'goes', 'day', 'sleep', 'hope', 'well', 'boytoy', 'think']\n",
            "After lemmatization: ['good', 'afternoon', 'love', 'go', 'day', 'sleep', 'hope', 'well', 'boytoy', 'think']\n",
            "After lowercasing: yes... i trust u to buy new stuff asap so i can try it out\n",
            "After removing special chars: yes i trust u to buy new stuff asap so i can try it out\n",
            "After tokenization: ['yes', 'i', 'trust', 'u', 'to', 'buy', 'new', 'stuff', 'asap', 'so', 'i', 'can', 'try', 'it', 'out']\n",
            "After stop word removal: ['yes', 'trust', 'u', 'buy', 'new', 'stuff', 'asap', 'try']\n",
            "After lemmatization: ['yes', 'trust', 'u', 'buy', 'new', 'stuff', 'asap', 'try']\n",
            "After lowercasing: sms services. for your inclusive text credits, pls goto www.comuk.net login= 3qxj9 unsubscribe with stop, no extra charge. help 08702840625.comuk. 220-cm2 9ae\n",
            "After removing special chars: sms services for your inclusive text credits pls goto wwwcomuknet login qxj unsubscribe with stop no extra charge help comuk cm ae\n",
            "After tokenization: ['sms', 'services', 'for', 'your', 'inclusive', 'text', 'credits', 'pls', 'goto', 'wwwcomuknet', 'login', 'qxj', 'unsubscribe', 'with', 'stop', 'no', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
            "After stop word removal: ['sms', 'services', 'inclusive', 'text', 'credits', 'pls', 'goto', 'wwwcomuknet', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
            "After lemmatization: ['sm', 'service', 'inclusive', 'text', 'credit', 'pls', 'goto', 'wwwcomuknet', 'login', 'qxj', 'unsubscribe', 'stop', 'extra', 'charge', 'help', 'comuk', 'cm', 'ae']\n",
            "After lowercasing: why did i wake up on my own &gt;:(\n",
            "After removing special chars: why did i wake up on my own gt\n",
            "After tokenization: ['why', 'did', 'i', 'wake', 'up', 'on', 'my', 'own', 'gt']\n",
            "After stop word removal: ['wake', 'gt']\n",
            "After lemmatization: ['wake', 'gt']\n",
            "After lowercasing: now get step 2 outta the way. congrats again.\n",
            "After removing special chars: now get step  outta the way congrats again\n",
            "After tokenization: ['now', 'get', 'step', 'outta', 'the', 'way', 'congrats', 'again']\n",
            "After stop word removal: ['get', 'step', 'outta', 'way', 'congrats']\n",
            "After lemmatization: ['get', 'step', 'outta', 'way', 'congrats']\n",
            "After lowercasing: love has one law; make happy the person you love. in the same way friendship has one law; never make ur friend feel alone until you are alive.... gud night\n",
            "After removing special chars: love has one law make happy the person you love in the same way friendship has one law never make ur friend feel alone until you are alive gud night\n",
            "After tokenization: ['love', 'has', 'one', 'law', 'make', 'happy', 'the', 'person', 'you', 'love', 'in', 'the', 'same', 'way', 'friendship', 'has', 'one', 'law', 'never', 'make', 'ur', 'friend', 'feel', 'alone', 'until', 'you', 'are', 'alive', 'gud', 'night']\n",
            "After stop word removal: ['love', 'one', 'law', 'make', 'happy', 'person', 'love', 'way', 'friendship', 'one', 'law', 'never', 'make', 'ur', 'friend', 'feel', 'alone', 'alive', 'gud', 'night']\n",
            "After lemmatization: ['love', 'one', 'law', 'make', 'happy', 'person', 'love', 'way', 'friendship', 'one', 'law', 'never', 'make', 'ur', 'friend', 'feel', 'alone', 'alive', 'gud', 'night']\n",
            "After lowercasing: private! your 2003 account statement for 07808247860 shows 800 un-redeemed s. i. m. points. call 08719899229 identifier code: 40411 expires 06/11/04\n",
            "After removing special chars: private your  account statement for  shows  unredeemed s i m points call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 's', 'i', 'm', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'point', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: apo all other are mokka players only\n",
            "After removing special chars: apo all other are mokka players only\n",
            "After tokenization: ['apo', 'all', 'other', 'are', 'mokka', 'players', 'only']\n",
            "After stop word removal: ['apo', 'mokka', 'players']\n",
            "After lemmatization: ['apo', 'mokka', 'player']\n",
            "After lowercasing: perhaps * is much easy give your account identification, so i will tomorrow at uni\n",
            "After removing special chars: perhaps  is much easy give your account identification so i will tomorrow at uni\n",
            "After tokenization: ['perhaps', 'is', 'much', 'easy', 'give', 'your', 'account', 'identification', 'so', 'i', 'will', 'tomorrow', 'at', 'uni']\n",
            "After stop word removal: ['perhaps', 'much', 'easy', 'give', 'account', 'identification', 'tomorrow', 'uni']\n",
            "After lemmatization: ['perhaps', 'much', 'easy', 'give', 'account', 'identification', 'tomorrow', 'uni']\n",
            "After lowercasing: wait . i will msg after  &lt;#&gt;  min.\n",
            "After removing special chars: wait  i will msg after  ltgt  min\n",
            "After tokenization: ['wait', 'i', 'will', 'msg', 'after', 'ltgt', 'min']\n",
            "After stop word removal: ['wait', 'msg', 'ltgt', 'min']\n",
            "After lemmatization: ['wait', 'msg', 'ltgt', 'min']\n",
            "After lowercasing: what i told before i tell. stupid hear after i wont tell anything to you. you dad called to my brother and spoken. not with me.\n",
            "After removing special chars: what i told before i tell stupid hear after i wont tell anything to you you dad called to my brother and spoken not with me\n",
            "After tokenization: ['what', 'i', 'told', 'before', 'i', 'tell', 'stupid', 'hear', 'after', 'i', 'wont', 'tell', 'anything', 'to', 'you', 'you', 'dad', 'called', 'to', 'my', 'brother', 'and', 'spoken', 'not', 'with', 'me']\n",
            "After stop word removal: ['told', 'tell', 'stupid', 'hear', 'wont', 'tell', 'anything', 'dad', 'called', 'brother', 'spoken']\n",
            "After lemmatization: ['told', 'tell', 'stupid', 'hear', 'wont', 'tell', 'anything', 'dad', 'called', 'brother', 'spoken']\n",
            "After lowercasing: god's love has no limit. god's grace has no measure. god's power has no boundaries. may u have god's endless blessings always in ur life...!! gud ni8\n",
            "After removing special chars: gods love has no limit gods grace has no measure gods power has no boundaries may u have gods endless blessings always in ur life gud ni\n",
            "After tokenization: ['gods', 'love', 'has', 'no', 'limit', 'gods', 'grace', 'has', 'no', 'measure', 'gods', 'power', 'has', 'no', 'boundaries', 'may', 'u', 'have', 'gods', 'endless', 'blessings', 'always', 'in', 'ur', 'life', 'gud', 'ni']\n",
            "After stop word removal: ['gods', 'love', 'limit', 'gods', 'grace', 'measure', 'gods', 'power', 'boundaries', 'may', 'u', 'gods', 'endless', 'blessings', 'always', 'ur', 'life', 'gud', 'ni']\n",
            "After lemmatization: ['god', 'love', 'limit', 'god', 'grace', 'measure', 'god', 'power', 'boundary', 'may', 'u', 'god', 'endless', 'blessing', 'always', 'ur', 'life', 'gud', 'ni']\n",
            "After lowercasing: i want to be inside you every night...\n",
            "After removing special chars: i want to be inside you every night\n",
            "After tokenization: ['i', 'want', 'to', 'be', 'inside', 'you', 'every', 'night']\n",
            "After stop word removal: ['want', 'inside', 'every', 'night']\n",
            "After lemmatization: ['want', 'inside', 'every', 'night']\n",
            "After lowercasing: machan you go to gym tomorrow,  i wil come late goodnight.\n",
            "After removing special chars: machan you go to gym tomorrow  i wil come late goodnight\n",
            "After tokenization: ['machan', 'you', 'go', 'to', 'gym', 'tomorrow', 'i', 'wil', 'come', 'late', 'goodnight']\n",
            "After stop word removal: ['machan', 'go', 'gym', 'tomorrow', 'wil', 'come', 'late', 'goodnight']\n",
            "After lemmatization: ['machan', 'go', 'gym', 'tomorrow', 'wil', 'come', 'late', 'goodnight']\n",
            "After lowercasing: lol they were mad at first but then they woke up and gave in.\n",
            "After removing special chars: lol they were mad at first but then they woke up and gave in\n",
            "After tokenization: ['lol', 'they', 'were', 'mad', 'at', 'first', 'but', 'then', 'they', 'woke', 'up', 'and', 'gave', 'in']\n",
            "After stop word removal: ['lol', 'mad', 'first', 'woke', 'gave']\n",
            "After lemmatization: ['lol', 'mad', 'first', 'woke', 'gave']\n",
            "After lowercasing: i went to project centre\n",
            "After removing special chars: i went to project centre\n",
            "After tokenization: ['i', 'went', 'to', 'project', 'centre']\n",
            "After stop word removal: ['went', 'project', 'centre']\n",
            "After lemmatization: ['went', 'project', 'centre']\n",
            "After lowercasing: itû÷s reassuring, in this crazy world.\n",
            "After removing special chars: its reassuring in this crazy world\n",
            "After tokenization: ['its', 'reassuring', 'in', 'this', 'crazy', 'world']\n",
            "After stop word removal: ['reassuring', 'crazy', 'world']\n",
            "After lemmatization: ['reassuring', 'crazy', 'world']\n",
            "After lowercasing: just making dinner, you ?\n",
            "After removing special chars: just making dinner you \n",
            "After tokenization: ['just', 'making', 'dinner', 'you']\n",
            "After stop word removal: ['making', 'dinner']\n",
            "After lemmatization: ['making', 'dinner']\n",
            "After lowercasing: yes. please leave at  &lt;#&gt; . so that at  &lt;#&gt;  we can leave\n",
            "After removing special chars: yes please leave at  ltgt  so that at  ltgt  we can leave\n",
            "After tokenization: ['yes', 'please', 'leave', 'at', 'ltgt', 'so', 'that', 'at', 'ltgt', 'we', 'can', 'leave']\n",
            "After stop word removal: ['yes', 'please', 'leave', 'ltgt', 'ltgt', 'leave']\n",
            "After lemmatization: ['yes', 'please', 'leave', 'ltgt', 'ltgt', 'leave']\n",
            "After lowercasing: oh... okie lor...we go on sat... \n",
            "After removing special chars: oh okie lorwe go on sat \n",
            "After tokenization: ['oh', 'okie', 'lorwe', 'go', 'on', 'sat']\n",
            "After stop word removal: ['oh', 'okie', 'lorwe', 'go', 'sat']\n",
            "After lemmatization: ['oh', 'okie', 'lorwe', 'go', 'sat']\n",
            "After lowercasing: you are a great role model. you are giving so much and i really wish each day for a miracle but god as a reason for everything and i must say i wish i knew why but i dont. i've looked up to you since i was young and i still do. have a great day.\n",
            "After removing special chars: you are a great role model you are giving so much and i really wish each day for a miracle but god as a reason for everything and i must say i wish i knew why but i dont ive looked up to you since i was young and i still do have a great day\n",
            "After tokenization: ['you', 'are', 'a', 'great', 'role', 'model', 'you', 'are', 'giving', 'so', 'much', 'and', 'i', 'really', 'wish', 'each', 'day', 'for', 'a', 'miracle', 'but', 'god', 'as', 'a', 'reason', 'for', 'everything', 'and', 'i', 'must', 'say', 'i', 'wish', 'i', 'knew', 'why', 'but', 'i', 'dont', 'ive', 'looked', 'up', 'to', 'you', 'since', 'i', 'was', 'young', 'and', 'i', 'still', 'do', 'have', 'a', 'great', 'day']\n",
            "After stop word removal: ['great', 'role', 'model', 'giving', 'much', 'really', 'wish', 'day', 'miracle', 'god', 'reason', 'everything', 'must', 'say', 'wish', 'knew', 'dont', 'ive', 'looked', 'since', 'young', 'still', 'great', 'day']\n",
            "After lemmatization: ['great', 'role', 'model', 'giving', 'much', 'really', 'wish', 'day', 'miracle', 'god', 'reason', 'everything', 'must', 'say', 'wish', 'knew', 'dont', 'ive', 'looked', 'since', 'young', 'still', 'great', 'day']\n",
            "After lowercasing: ya, i'm referin to mei's ex wat... no ah, waitin 4 u to treat, somebody shld b rich liao...so gd, den u dun have to work frm tmr onwards...\n",
            "After removing special chars: ya im referin to meis ex wat no ah waitin  u to treat somebody shld b rich liaoso gd den u dun have to work frm tmr onwards\n",
            "After tokenization: ['ya', 'im', 'referin', 'to', 'meis', 'ex', 'wat', 'no', 'ah', 'waitin', 'u', 'to', 'treat', 'somebody', 'shld', 'b', 'rich', 'liaoso', 'gd', 'den', 'u', 'dun', 'have', 'to', 'work', 'frm', 'tmr', 'onwards']\n",
            "After stop word removal: ['ya', 'im', 'referin', 'meis', 'ex', 'wat', 'ah', 'waitin', 'u', 'treat', 'somebody', 'shld', 'b', 'rich', 'liaoso', 'gd', 'den', 'u', 'dun', 'work', 'frm', 'tmr', 'onwards']\n",
            "After lemmatization: ['ya', 'im', 'referin', 'mei', 'ex', 'wat', 'ah', 'waitin', 'u', 'treat', 'somebody', 'shld', 'b', 'rich', 'liaoso', 'gd', 'den', 'u', 'dun', 'work', 'frm', 'tmr', 'onwards']\n",
            "After lowercasing: miles and smiles r made frm same letters but do u know d difference..? smile on ur face keeps me happy even though i am miles away from u.. :-)keep smiling.. good nyt\n",
            "After removing special chars: miles and smiles r made frm same letters but do u know d difference smile on ur face keeps me happy even though i am miles away from u keep smiling good nyt\n",
            "After tokenization: ['miles', 'and', 'smiles', 'r', 'made', 'frm', 'same', 'letters', 'but', 'do', 'u', 'know', 'd', 'difference', 'smile', 'on', 'ur', 'face', 'keeps', 'me', 'happy', 'even', 'though', 'i', 'am', 'miles', 'away', 'from', 'u', 'keep', 'smiling', 'good', 'nyt']\n",
            "After stop word removal: ['miles', 'smiles', 'r', 'made', 'frm', 'letters', 'u', 'know', 'difference', 'smile', 'ur', 'face', 'keeps', 'happy', 'even', 'though', 'miles', 'away', 'u', 'keep', 'smiling', 'good', 'nyt']\n",
            "After lemmatization: ['mile', 'smile', 'r', 'made', 'frm', 'letter', 'u', 'know', 'difference', 'smile', 'ur', 'face', 'keep', 'happy', 'even', 'though', 'mile', 'away', 'u', 'keep', 'smiling', 'good', 'nyt']\n",
            "After lowercasing: by the way, i've put a skip right outside the front of the house so you can see which house it is. just pull up before it.\n",
            "After removing special chars: by the way ive put a skip right outside the front of the house so you can see which house it is just pull up before it\n",
            "After tokenization: ['by', 'the', 'way', 'ive', 'put', 'a', 'skip', 'right', 'outside', 'the', 'front', 'of', 'the', 'house', 'so', 'you', 'can', 'see', 'which', 'house', 'it', 'is', 'just', 'pull', 'up', 'before', 'it']\n",
            "After stop word removal: ['way', 'ive', 'put', 'skip', 'right', 'outside', 'front', 'house', 'see', 'house', 'pull']\n",
            "After lemmatization: ['way', 'ive', 'put', 'skip', 'right', 'outside', 'front', 'house', 'see', 'house', 'pull']\n",
            "After lowercasing: can you pls send me that company name. in saibaba colany\n",
            "After removing special chars: can you pls send me that company name in saibaba colany\n",
            "After tokenization: ['can', 'you', 'pls', 'send', 'me', 'that', 'company', 'name', 'in', 'saibaba', 'colany']\n",
            "After stop word removal: ['pls', 'send', 'company', 'name', 'saibaba', 'colany']\n",
            "After lemmatization: ['pls', 'send', 'company', 'name', 'saibaba', 'colany']\n",
            "After lowercasing: no. i dont want to hear anything\n",
            "After removing special chars: no i dont want to hear anything\n",
            "After tokenization: ['no', 'i', 'dont', 'want', 'to', 'hear', 'anything']\n",
            "After stop word removal: ['dont', 'want', 'hear', 'anything']\n",
            "After lemmatization: ['dont', 'want', 'hear', 'anything']\n",
            "After lowercasing: you are a big chic. common. declare\n",
            "After removing special chars: you are a big chic common declare\n",
            "After tokenization: ['you', 'are', 'a', 'big', 'chic', 'common', 'declare']\n",
            "After stop word removal: ['big', 'chic', 'common', 'declare']\n",
            "After lemmatization: ['big', 'chic', 'common', 'declare']\n",
            "After lowercasing: thats cool. i want to please you...\n",
            "After removing special chars: thats cool i want to please you\n",
            "After tokenization: ['thats', 'cool', 'i', 'want', 'to', 'please', 'you']\n",
            "After stop word removal: ['thats', 'cool', 'want', 'please']\n",
            "After lemmatization: ['thats', 'cool', 'want', 'please']\n",
            "After lowercasing: going to join tomorrow.\n",
            "After removing special chars: going to join tomorrow\n",
            "After tokenization: ['going', 'to', 'join', 'tomorrow']\n",
            "After stop word removal: ['going', 'join', 'tomorrow']\n",
            "After lemmatization: ['going', 'join', 'tomorrow']\n",
            "After lowercasing: you are awarded a sipix digital camera! call 09061221061 from landline. delivery within 28days. t cs box177. m221bp. 2yr warranty. 150ppm. 16 . p på£3.99\n",
            "After removing special chars: you are awarded a sipix digital camera call  from landline delivery within days t cs box mbp yr warranty ppm   p p\n",
            "After tokenization: ['you', 'are', 'awarded', 'a', 'sipix', 'digital', 'camera', 'call', 'from', 'landline', 'delivery', 'within', 'days', 't', 'cs', 'box', 'mbp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
            "After stop word removal: ['awarded', 'sipix', 'digital', 'camera', 'call', 'landline', 'delivery', 'within', 'days', 'cs', 'box', 'mbp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
            "After lemmatization: ['awarded', 'sipix', 'digital', 'camera', 'call', 'landline', 'delivery', 'within', 'day', 'c', 'box', 'mbp', 'yr', 'warranty', 'ppm', 'p', 'p']\n",
            "After lowercasing: i want to tell you how bad i feel that basically the only times i text you lately are when i need drugs\n",
            "After removing special chars: i want to tell you how bad i feel that basically the only times i text you lately are when i need drugs\n",
            "After tokenization: ['i', 'want', 'to', 'tell', 'you', 'how', 'bad', 'i', 'feel', 'that', 'basically', 'the', 'only', 'times', 'i', 'text', 'you', 'lately', 'are', 'when', 'i', 'need', 'drugs']\n",
            "After stop word removal: ['want', 'tell', 'bad', 'feel', 'basically', 'times', 'text', 'lately', 'need', 'drugs']\n",
            "After lemmatization: ['want', 'tell', 'bad', 'feel', 'basically', 'time', 'text', 'lately', 'need', 'drug']\n",
            "After lowercasing: private! your 2003 account statement for shows 800 un-redeemed s.i.m. points. call 08718738001 identifier code: 49557 expires 26/11/04\n",
            "After removing special chars: private your  account statement for shows  unredeemed sim points call  identifier code  expires \n",
            "After tokenization: ['private', 'your', 'account', 'statement', 'for', 'shows', 'unredeemed', 'sim', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After stop word removal: ['private', 'account', 'statement', 'shows', 'unredeemed', 'sim', 'points', 'call', 'identifier', 'code', 'expires']\n",
            "After lemmatization: ['private', 'account', 'statement', 'show', 'unredeemed', 'sim', 'point', 'call', 'identifier', 'code', 'expires']\n",
            "After lowercasing: total disappointment, when i texted you was the craziest shit got :(\n",
            "After removing special chars: total disappointment when i texted you was the craziest shit got \n",
            "After tokenization: ['total', 'disappointment', 'when', 'i', 'texted', 'you', 'was', 'the', 'craziest', 'shit', 'got']\n",
            "After stop word removal: ['total', 'disappointment', 'texted', 'craziest', 'shit', 'got']\n",
            "After lemmatization: ['total', 'disappointment', 'texted', 'craziest', 'shit', 'got']\n",
            "After lowercasing: its just the effect of irritation. just ignore it\n",
            "After removing special chars: its just the effect of irritation just ignore it\n",
            "After tokenization: ['its', 'just', 'the', 'effect', 'of', 'irritation', 'just', 'ignore', 'it']\n",
            "After stop word removal: ['effect', 'irritation', 'ignore']\n",
            "After lemmatization: ['effect', 'irritation', 'ignore']\n",
            "After lowercasing: what about this one then.\n",
            "After removing special chars: what about this one then\n",
            "After tokenization: ['what', 'about', 'this', 'one', 'then']\n",
            "After stop word removal: ['one']\n",
            "After lemmatization: ['one']\n",
            "After lowercasing: i think that tantrum's finished so yeah i'll be by at some point\n",
            "After removing special chars: i think that tantrums finished so yeah ill be by at some point\n",
            "After tokenization: ['i', 'think', 'that', 'tantrums', 'finished', 'so', 'yeah', 'ill', 'be', 'by', 'at', 'some', 'point']\n",
            "After stop word removal: ['think', 'tantrums', 'finished', 'yeah', 'ill', 'point']\n",
            "After lemmatization: ['think', 'tantrum', 'finished', 'yeah', 'ill', 'point']\n",
            "After lowercasing: compliments to you. was away from the system. how your side.\n",
            "After removing special chars: compliments to you was away from the system how your side\n",
            "After tokenization: ['compliments', 'to', 'you', 'was', 'away', 'from', 'the', 'system', 'how', 'your', 'side']\n",
            "After stop word removal: ['compliments', 'away', 'system', 'side']\n",
            "After lemmatization: ['compliment', 'away', 'system', 'side']\n",
            "After lowercasing: happened here while you were adventuring\n",
            "After removing special chars: happened here while you were adventuring\n",
            "After tokenization: ['happened', 'here', 'while', 'you', 'were', 'adventuring']\n",
            "After stop word removal: ['happened', 'adventuring']\n",
            "After lemmatization: ['happened', 'adventuring']\n",
            "After lowercasing: hey chief, can you give me a bell when you get this. need to talk to you about this royal visit on the 1st june. \n",
            "After removing special chars: hey chief can you give me a bell when you get this need to talk to you about this royal visit on the st june \n",
            "After tokenization: ['hey', 'chief', 'can', 'you', 'give', 'me', 'a', 'bell', 'when', 'you', 'get', 'this', 'need', 'to', 'talk', 'to', 'you', 'about', 'this', 'royal', 'visit', 'on', 'the', 'st', 'june']\n",
            "After stop word removal: ['hey', 'chief', 'give', 'bell', 'get', 'need', 'talk', 'royal', 'visit', 'st', 'june']\n",
            "After lemmatization: ['hey', 'chief', 'give', 'bell', 'get', 'need', 'talk', 'royal', 'visit', 'st', 'june']\n",
            "After lowercasing: ok which your another number\n",
            "After removing special chars: ok which your another number\n",
            "After tokenization: ['ok', 'which', 'your', 'another', 'number']\n",
            "After stop word removal: ['ok', 'another', 'number']\n",
            "After lemmatization: ['ok', 'another', 'number']\n",
            "After lowercasing: i know you are thinkin malaria. but relax, children cant handle malaria. she would have been worse and its gastroenteritis. if she takes enough to replace her loss her temp will reduce. and if you give her malaria meds now she will just vomit. its a self limiting illness she has which means in a few days it will completely stop\n",
            "After removing special chars: i know you are thinkin malaria but relax children cant handle malaria she would have been worse and its gastroenteritis if she takes enough to replace her loss her temp will reduce and if you give her malaria meds now she will just vomit its a self limiting illness she has which means in a few days it will completely stop\n",
            "After tokenization: ['i', 'know', 'you', 'are', 'thinkin', 'malaria', 'but', 'relax', 'children', 'cant', 'handle', 'malaria', 'she', 'would', 'have', 'been', 'worse', 'and', 'its', 'gastroenteritis', 'if', 'she', 'takes', 'enough', 'to', 'replace', 'her', 'loss', 'her', 'temp', 'will', 'reduce', 'and', 'if', 'you', 'give', 'her', 'malaria', 'meds', 'now', 'she', 'will', 'just', 'vomit', 'its', 'a', 'self', 'limiting', 'illness', 'she', 'has', 'which', 'means', 'in', 'a', 'few', 'days', 'it', 'will', 'completely', 'stop']\n",
            "After stop word removal: ['know', 'thinkin', 'malaria', 'relax', 'children', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'takes', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'meds', 'vomit', 'self', 'limiting', 'illness', 'means', 'days', 'completely', 'stop']\n",
            "After lemmatization: ['know', 'thinkin', 'malaria', 'relax', 'child', 'cant', 'handle', 'malaria', 'would', 'worse', 'gastroenteritis', 'take', 'enough', 'replace', 'loss', 'temp', 'reduce', 'give', 'malaria', 'med', 'vomit', 'self', 'limiting', 'illness', 'mean', 'day', 'completely', 'stop']\n",
            "After lowercasing: aiyah ok wat as long as got improve can already wat...\n",
            "After removing special chars: aiyah ok wat as long as got improve can already wat\n",
            "After tokenization: ['aiyah', 'ok', 'wat', 'as', 'long', 'as', 'got', 'improve', 'can', 'already', 'wat']\n",
            "After stop word removal: ['aiyah', 'ok', 'wat', 'long', 'got', 'improve', 'already', 'wat']\n",
            "After lemmatization: ['aiyah', 'ok', 'wat', 'long', 'got', 'improve', 'already', 'wat']\n",
            "After lowercasing: want explicit sex in 30 secs? ring 02073162414 now! costs 20p/min gsex pobox 2667 wc1n 3xx\n",
            "After removing special chars: want explicit sex in  secs ring  now costs pmin gsex pobox  wcn xx\n",
            "After tokenization: ['want', 'explicit', 'sex', 'in', 'secs', 'ring', 'now', 'costs', 'pmin', 'gsex', 'pobox', 'wcn', 'xx']\n",
            "After stop word removal: ['want', 'explicit', 'sex', 'secs', 'ring', 'costs', 'pmin', 'gsex', 'pobox', 'wcn', 'xx']\n",
            "After lemmatization: ['want', 'explicit', 'sex', 'sec', 'ring', 'cost', 'pmin', 'gsex', 'pobox', 'wcn', 'xx']\n",
            "After lowercasing: i can't believe how attached i am to seeing you every day. i know you will do the best you can to get to me babe. i will go to teach my class at your midnight\n",
            "After removing special chars: i cant believe how attached i am to seeing you every day i know you will do the best you can to get to me babe i will go to teach my class at your midnight\n",
            "After tokenization: ['i', 'cant', 'believe', 'how', 'attached', 'i', 'am', 'to', 'seeing', 'you', 'every', 'day', 'i', 'know', 'you', 'will', 'do', 'the', 'best', 'you', 'can', 'to', 'get', 'to', 'me', 'babe', 'i', 'will', 'go', 'to', 'teach', 'my', 'class', 'at', 'your', 'midnight']\n",
            "After stop word removal: ['cant', 'believe', 'attached', 'seeing', 'every', 'day', 'know', 'best', 'get', 'babe', 'go', 'teach', 'class', 'midnight']\n",
            "After lemmatization: ['cant', 'believe', 'attached', 'seeing', 'every', 'day', 'know', 'best', 'get', 'babe', 'go', 'teach', 'class', 'midnight']\n",
            "After lowercasing: just sleeping..and surfing\n",
            "After removing special chars: just sleepingand surfing\n",
            "After tokenization: ['just', 'sleepingand', 'surfing']\n",
            "After stop word removal: ['sleepingand', 'surfing']\n",
            "After lemmatization: ['sleepingand', 'surfing']\n",
            "After lowercasing: asked 3mobile if 0870 chatlines inclu in free mins. india cust servs sed yes. l8er got mega bill. 3 dont giv a shit. bailiff due in days. i o å£250 3 want å£800\n",
            "After removing special chars: asked mobile if  chatlines inclu in free mins india cust servs sed yes ler got mega bill  dont giv a shit bailiff due in days i o   want \n",
            "After tokenization: ['asked', 'mobile', 'if', 'chatlines', 'inclu', 'in', 'free', 'mins', 'india', 'cust', 'servs', 'sed', 'yes', 'ler', 'got', 'mega', 'bill', 'dont', 'giv', 'a', 'shit', 'bailiff', 'due', 'in', 'days', 'i', 'o', 'want']\n",
            "After stop word removal: ['asked', 'mobile', 'chatlines', 'inclu', 'free', 'mins', 'india', 'cust', 'servs', 'sed', 'yes', 'ler', 'got', 'mega', 'bill', 'dont', 'giv', 'shit', 'bailiff', 'due', 'days', 'want']\n",
            "After lemmatization: ['asked', 'mobile', 'chatlines', 'inclu', 'free', 'min', 'india', 'cust', 'servs', 'sed', 'yes', 'ler', 'got', 'mega', 'bill', 'dont', 'giv', 'shit', 'bailiff', 'due', 'day', 'want']\n",
            "After lowercasing: yeah it's jus rite...\n",
            "After removing special chars: yeah its jus rite\n",
            "After tokenization: ['yeah', 'its', 'jus', 'rite']\n",
            "After stop word removal: ['yeah', 'jus', 'rite']\n",
            "After lemmatization: ['yeah', 'jus', 'rite']\n",
            "After lowercasing: armand says get your ass over to epsilon\n",
            "After removing special chars: armand says get your ass over to epsilon\n",
            "After tokenization: ['armand', 'says', 'get', 'your', 'ass', 'over', 'to', 'epsilon']\n",
            "After stop word removal: ['armand', 'says', 'get', 'ass', 'epsilon']\n",
            "After lemmatization: ['armand', 'say', 'get', 'as', 'epsilon']\n",
            "After lowercasing: u still havent got urself a jacket ah?\n",
            "After removing special chars: u still havent got urself a jacket ah\n",
            "After tokenization: ['u', 'still', 'havent', 'got', 'urself', 'a', 'jacket', 'ah']\n",
            "After stop word removal: ['u', 'still', 'havent', 'got', 'urself', 'jacket', 'ah']\n",
            "After lemmatization: ['u', 'still', 'havent', 'got', 'urself', 'jacket', 'ah']\n",
            "After lowercasing: i'm taking derek &amp; taylor to walmart, if i'm not back by the time you're done just leave the mouse on my desk and i'll text you when priscilla's ready\n",
            "After removing special chars: im taking derek amp taylor to walmart if im not back by the time youre done just leave the mouse on my desk and ill text you when priscillas ready\n",
            "After tokenization: ['im', 'taking', 'derek', 'amp', 'taylor', 'to', 'walmart', 'if', 'im', 'not', 'back', 'by', 'the', 'time', 'youre', 'done', 'just', 'leave', 'the', 'mouse', 'on', 'my', 'desk', 'and', 'ill', 'text', 'you', 'when', 'priscillas', 'ready']\n",
            "After stop word removal: ['im', 'taking', 'derek', 'amp', 'taylor', 'walmart', 'im', 'back', 'time', 'youre', 'done', 'leave', 'mouse', 'desk', 'ill', 'text', 'priscillas', 'ready']\n",
            "After lemmatization: ['im', 'taking', 'derek', 'amp', 'taylor', 'walmart', 'im', 'back', 'time', 'youre', 'done', 'leave', 'mouse', 'desk', 'ill', 'text', 'priscillas', 'ready']\n",
            "After lowercasing: hi its in durban are you still on this number\n",
            "After removing special chars: hi its in durban are you still on this number\n",
            "After tokenization: ['hi', 'its', 'in', 'durban', 'are', 'you', 'still', 'on', 'this', 'number']\n",
            "After stop word removal: ['hi', 'durban', 'still', 'number']\n",
            "After lemmatization: ['hi', 'durban', 'still', 'number']\n",
            "After lowercasing: ic. there are a lotta childporn cars then.\n",
            "After removing special chars: ic there are a lotta childporn cars then\n",
            "After tokenization: ['ic', 'there', 'are', 'a', 'lotta', 'childporn', 'cars', 'then']\n",
            "After stop word removal: ['ic', 'lotta', 'childporn', 'cars']\n",
            "After lemmatization: ['ic', 'lotta', 'childporn', 'car']\n",
            "After lowercasing: had your contract mobile 11 mnths? latest motorola, nokia etc. all free! double mins & text on orange tariffs. text yes for callback, no to remove from records.\n",
            "After removing special chars: had your contract mobile  mnths latest motorola nokia etc all free double mins  text on orange tariffs text yes for callback no to remove from records\n",
            "After tokenization: ['had', 'your', 'contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'all', 'free', 'double', 'mins', 'text', 'on', 'orange', 'tariffs', 'text', 'yes', 'for', 'callback', 'no', 'to', 'remove', 'from', 'records']\n",
            "After stop word removal: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'mins', 'text', 'orange', 'tariffs', 'text', 'yes', 'callback', 'remove', 'records']\n",
            "After lemmatization: ['contract', 'mobile', 'mnths', 'latest', 'motorola', 'nokia', 'etc', 'free', 'double', 'min', 'text', 'orange', 'tariff', 'text', 'yes', 'callback', 'remove', 'record']\n",
            "After lowercasing: no, i was trying it all weekend ;v\n",
            "After removing special chars: no i was trying it all weekend v\n",
            "After tokenization: ['no', 'i', 'was', 'trying', 'it', 'all', 'weekend', 'v']\n",
            "After stop word removal: ['trying', 'weekend', 'v']\n",
            "After lemmatization: ['trying', 'weekend', 'v']\n",
            "After lowercasing: you know, wot people wear. t shirts, jumpers, hat, belt, is all we know. we r at cribbs\n",
            "After removing special chars: you know wot people wear t shirts jumpers hat belt is all we know we r at cribbs\n",
            "After tokenization: ['you', 'know', 'wot', 'people', 'wear', 't', 'shirts', 'jumpers', 'hat', 'belt', 'is', 'all', 'we', 'know', 'we', 'r', 'at', 'cribbs']\n",
            "After stop word removal: ['know', 'wot', 'people', 'wear', 'shirts', 'jumpers', 'hat', 'belt', 'know', 'r', 'cribbs']\n",
            "After lemmatization: ['know', 'wot', 'people', 'wear', 'shirt', 'jumper', 'hat', 'belt', 'know', 'r', 'cribbs']\n",
            "After lowercasing: cool, what time you think you can get here?\n",
            "After removing special chars: cool what time you think you can get here\n",
            "After tokenization: ['cool', 'what', 'time', 'you', 'think', 'you', 'can', 'get', 'here']\n",
            "After stop word removal: ['cool', 'time', 'think', 'get']\n",
            "After lemmatization: ['cool', 'time', 'think', 'get']\n",
            "After lowercasing: wen did you get so spiritual and deep. that's great\n",
            "After removing special chars: wen did you get so spiritual and deep thats great\n",
            "After tokenization: ['wen', 'did', 'you', 'get', 'so', 'spiritual', 'and', 'deep', 'thats', 'great']\n",
            "After stop word removal: ['wen', 'get', 'spiritual', 'deep', 'thats', 'great']\n",
            "After lemmatization: ['wen', 'get', 'spiritual', 'deep', 'thats', 'great']\n",
            "After lowercasing: have a safe trip to nigeria. wish you happiness and very soon company to share moments with\n",
            "After removing special chars: have a safe trip to nigeria wish you happiness and very soon company to share moments with\n",
            "After tokenization: ['have', 'a', 'safe', 'trip', 'to', 'nigeria', 'wish', 'you', 'happiness', 'and', 'very', 'soon', 'company', 'to', 'share', 'moments', 'with']\n",
            "After stop word removal: ['safe', 'trip', 'nigeria', 'wish', 'happiness', 'soon', 'company', 'share', 'moments']\n",
            "After lemmatization: ['safe', 'trip', 'nigeria', 'wish', 'happiness', 'soon', 'company', 'share', 'moment']\n",
            "After lowercasing: hahaha..use your brain dear\n",
            "After removing special chars: hahahause your brain dear\n",
            "After tokenization: ['hahahause', 'your', 'brain', 'dear']\n",
            "After stop word removal: ['hahahause', 'brain', 'dear']\n",
            "After lemmatization: ['hahahause', 'brain', 'dear']\n",
            "After lowercasing: well keep in mind i've only got enough gas for one more round trip barring a sudden influx of cash\n",
            "After removing special chars: well keep in mind ive only got enough gas for one more round trip barring a sudden influx of cash\n",
            "After tokenization: ['well', 'keep', 'in', 'mind', 'ive', 'only', 'got', 'enough', 'gas', 'for', 'one', 'more', 'round', 'trip', 'barring', 'a', 'sudden', 'influx', 'of', 'cash']\n",
            "After stop word removal: ['well', 'keep', 'mind', 'ive', 'got', 'enough', 'gas', 'one', 'round', 'trip', 'barring', 'sudden', 'influx', 'cash']\n",
            "After lemmatization: ['well', 'keep', 'mind', 'ive', 'got', 'enough', 'gas', 'one', 'round', 'trip', 'barring', 'sudden', 'influx', 'cash']\n",
            "After lowercasing: yeh. indians was nice. tho it did kane me off a bit he he. we shud go out 4 a drink sometime soon. mite hav 2 go 2 da works 4 a laugh soon. love pete x x\n",
            "After removing special chars: yeh indians was nice tho it did kane me off a bit he he we shud go out  a drink sometime soon mite hav  go  da works  a laugh soon love pete x x\n",
            "After tokenization: ['yeh', 'indians', 'was', 'nice', 'tho', 'it', 'did', 'kane', 'me', 'off', 'a', 'bit', 'he', 'he', 'we', 'shud', 'go', 'out', 'a', 'drink', 'sometime', 'soon', 'mite', 'hav', 'go', 'da', 'works', 'a', 'laugh', 'soon', 'love', 'pete', 'x', 'x']\n",
            "After stop word removal: ['yeh', 'indians', 'nice', 'tho', 'kane', 'bit', 'shud', 'go', 'drink', 'sometime', 'soon', 'mite', 'hav', 'go', 'da', 'works', 'laugh', 'soon', 'love', 'pete', 'x', 'x']\n",
            "After lemmatization: ['yeh', 'indian', 'nice', 'tho', 'kane', 'bit', 'shud', 'go', 'drink', 'sometime', 'soon', 'mite', 'hav', 'go', 'da', 'work', 'laugh', 'soon', 'love', 'pete', 'x', 'x']\n",
            "After lowercasing: yes i have. so that's why u texted. pshew...missing you so much\n",
            "After removing special chars: yes i have so thats why u texted pshewmissing you so much\n",
            "After tokenization: ['yes', 'i', 'have', 'so', 'thats', 'why', 'u', 'texted', 'pshewmissing', 'you', 'so', 'much']\n",
            "After stop word removal: ['yes', 'thats', 'u', 'texted', 'pshewmissing', 'much']\n",
            "After lemmatization: ['yes', 'thats', 'u', 'texted', 'pshewmissing', 'much']\n",
            "After lowercasing: no. i meant the calculation is the same. that  &lt;#&gt; units at  &lt;#&gt; . this school is really expensive. have you started practicing your accent. because its important. and have you decided if you are doing 4years of dental school or if you'll just do the nmde exam.\n",
            "After removing special chars: no i meant the calculation is the same that  ltgt units at  ltgt  this school is really expensive have you started practicing your accent because its important and have you decided if you are doing years of dental school or if youll just do the nmde exam\n",
            "After tokenization: ['no', 'i', 'meant', 'the', 'calculation', 'is', 'the', 'same', 'that', 'ltgt', 'units', 'at', 'ltgt', 'this', 'school', 'is', 'really', 'expensive', 'have', 'you', 'started', 'practicing', 'your', 'accent', 'because', 'its', 'important', 'and', 'have', 'you', 'decided', 'if', 'you', 'are', 'doing', 'years', 'of', 'dental', 'school', 'or', 'if', 'youll', 'just', 'do', 'the', 'nmde', 'exam']\n",
            "After stop word removal: ['meant', 'calculation', 'ltgt', 'units', 'ltgt', 'school', 'really', 'expensive', 'started', 'practicing', 'accent', 'important', 'decided', 'years', 'dental', 'school', 'youll', 'nmde', 'exam']\n",
            "After lemmatization: ['meant', 'calculation', 'ltgt', 'unit', 'ltgt', 'school', 'really', 'expensive', 'started', 'practicing', 'accent', 'important', 'decided', 'year', 'dental', 'school', 'youll', 'nmde', 'exam']\n",
            "After lowercasing: sorry, i'll call later\n",
            "After removing special chars: sorry ill call later\n",
            "After tokenization: ['sorry', 'ill', 'call', 'later']\n",
            "After stop word removal: ['sorry', 'ill', 'call', 'later']\n",
            "After lemmatization: ['sorry', 'ill', 'call', 'later']\n",
            "After lowercasing: if you aren't here in the next  &lt;#&gt;  hours imma flip my shit\n",
            "After removing special chars: if you arent here in the next  ltgt  hours imma flip my shit\n",
            "After tokenization: ['if', 'you', 'arent', 'here', 'in', 'the', 'next', 'ltgt', 'hours', 'imma', 'flip', 'my', 'shit']\n",
            "After stop word removal: ['arent', 'next', 'ltgt', 'hours', 'imma', 'flip', 'shit']\n",
            "After lemmatization: ['arent', 'next', 'ltgt', 'hour', 'imma', 'flip', 'shit']\n",
            "After lowercasing: anything lor. juz both of us lor.\n",
            "After removing special chars: anything lor juz both of us lor\n",
            "After tokenization: ['anything', 'lor', 'juz', 'both', 'of', 'us', 'lor']\n",
            "After stop word removal: ['anything', 'lor', 'juz', 'us', 'lor']\n",
            "After lemmatization: ['anything', 'lor', 'juz', 'u', 'lor']\n",
            "After lowercasing: get me out of this dump heap. my mom decided to come to lowes. boring.\n",
            "After removing special chars: get me out of this dump heap my mom decided to come to lowes boring\n",
            "After tokenization: ['get', 'me', 'out', 'of', 'this', 'dump', 'heap', 'my', 'mom', 'decided', 'to', 'come', 'to', 'lowes', 'boring']\n",
            "After stop word removal: ['get', 'dump', 'heap', 'mom', 'decided', 'come', 'lowes', 'boring']\n",
            "After lemmatization: ['get', 'dump', 'heap', 'mom', 'decided', 'come', 'lowes', 'boring']\n",
            "After lowercasing: ok lor... sony ericsson salesman... i ask shuhui then she say quite gd 2 use so i considering...\n",
            "After removing special chars: ok lor sony ericsson salesman i ask shuhui then she say quite gd  use so i considering\n",
            "After tokenization: ['ok', 'lor', 'sony', 'ericsson', 'salesman', 'i', 'ask', 'shuhui', 'then', 'she', 'say', 'quite', 'gd', 'use', 'so', 'i', 'considering']\n",
            "After stop word removal: ['ok', 'lor', 'sony', 'ericsson', 'salesman', 'ask', 'shuhui', 'say', 'quite', 'gd', 'use', 'considering']\n",
            "After lemmatization: ['ok', 'lor', 'sony', 'ericsson', 'salesman', 'ask', 'shuhui', 'say', 'quite', 'gd', 'use', 'considering']\n",
            "After lowercasing: ard 6 like dat lor.\n",
            "After removing special chars: ard  like dat lor\n",
            "After tokenization: ['ard', 'like', 'dat', 'lor']\n",
            "After stop word removal: ['ard', 'like', 'dat', 'lor']\n",
            "After lemmatization: ['ard', 'like', 'dat', 'lor']\n",
            "After lowercasing: why don't you wait 'til at least wednesday to see if you get your .\n",
            "After removing special chars: why dont you wait til at least wednesday to see if you get your \n",
            "After tokenization: ['why', 'dont', 'you', 'wait', 'til', 'at', 'least', 'wednesday', 'to', 'see', 'if', 'you', 'get', 'your']\n",
            "After stop word removal: ['dont', 'wait', 'til', 'least', 'wednesday', 'see', 'get']\n",
            "After lemmatization: ['dont', 'wait', 'til', 'least', 'wednesday', 'see', 'get']\n",
            "After lowercasing: huh y lei...\n",
            "After removing special chars: huh y lei\n",
            "After tokenization: ['huh', 'y', 'lei']\n",
            "After stop word removal: ['huh', 'lei']\n",
            "After lemmatization: ['huh', 'lei']\n",
            "After lowercasing: reminder from o2: to get 2.50 pounds free call credit and details of great offers pls reply 2 this text with your valid name, house no and postcode\n",
            "After removing special chars: reminder from o to get  pounds free call credit and details of great offers pls reply  this text with your valid name house no and postcode\n",
            "After tokenization: ['reminder', 'from', 'o', 'to', 'get', 'pounds', 'free', 'call', 'credit', 'and', 'details', 'of', 'great', 'offers', 'pls', 'reply', 'this', 'text', 'with', 'your', 'valid', 'name', 'house', 'no', 'and', 'postcode']\n",
            "After stop word removal: ['reminder', 'get', 'pounds', 'free', 'call', 'credit', 'details', 'great', 'offers', 'pls', 'reply', 'text', 'valid', 'name', 'house', 'postcode']\n",
            "After lemmatization: ['reminder', 'get', 'pound', 'free', 'call', 'credit', 'detail', 'great', 'offer', 'pls', 'reply', 'text', 'valid', 'name', 'house', 'postcode']\n",
            "After lowercasing: this is the 2nd time we have tried 2 contact u. u have won the å£750 pound prize. 2 claim is easy, call 087187272008 now1! only 10p per minute. bt-national-rate.\n",
            "After removing special chars: this is the nd time we have tried  contact u u have won the  pound prize  claim is easy call  now only p per minute btnationalrate\n",
            "After tokenization: ['this', 'is', 'the', 'nd', 'time', 'we', 'have', 'tried', 'contact', 'u', 'u', 'have', 'won', 'the', 'pound', 'prize', 'claim', 'is', 'easy', 'call', 'now', 'only', 'p', 'per', 'minute', 'btnationalrate']\n",
            "After stop word removal: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'btnationalrate']\n",
            "After lemmatization: ['nd', 'time', 'tried', 'contact', 'u', 'u', 'pound', 'prize', 'claim', 'easy', 'call', 'p', 'per', 'minute', 'btnationalrate']\n",
            "After lowercasing: will ì_ b going to esplanade fr home?\n",
            "After removing special chars: will  b going to esplanade fr home\n",
            "After tokenization: ['will', 'b', 'going', 'to', 'esplanade', 'fr', 'home']\n",
            "After stop word removal: ['b', 'going', 'esplanade', 'fr', 'home']\n",
            "After lemmatization: ['b', 'going', 'esplanade', 'fr', 'home']\n",
            "After lowercasing: pity, * was in mood for that. so...any other suggestions?\n",
            "After removing special chars: pity  was in mood for that soany other suggestions\n",
            "After tokenization: ['pity', 'was', 'in', 'mood', 'for', 'that', 'soany', 'other', 'suggestions']\n",
            "After stop word removal: ['pity', 'mood', 'soany', 'suggestions']\n",
            "After lemmatization: ['pity', 'mood', 'soany', 'suggestion']\n",
            "After lowercasing: the guy did some bitching but i acted like i'd be interested in buying something else next week and he gave it to us for free\n",
            "After removing special chars: the guy did some bitching but i acted like id be interested in buying something else next week and he gave it to us for free\n",
            "After tokenization: ['the', 'guy', 'did', 'some', 'bitching', 'but', 'i', 'acted', 'like', 'id', 'be', 'interested', 'in', 'buying', 'something', 'else', 'next', 'week', 'and', 'he', 'gave', 'it', 'to', 'us', 'for', 'free']\n",
            "After stop word removal: ['guy', 'bitching', 'acted', 'like', 'id', 'interested', 'buying', 'something', 'else', 'next', 'week', 'gave', 'us', 'free']\n",
            "After lemmatization: ['guy', 'bitching', 'acted', 'like', 'id', 'interested', 'buying', 'something', 'else', 'next', 'week', 'gave', 'u', 'free']\n",
            "After lowercasing: rofl. its true to its name\n",
            "After removing special chars: rofl its true to its name\n",
            "After tokenization: ['rofl', 'its', 'true', 'to', 'its', 'name']\n",
            "After stop word removal: ['rofl', 'true', 'name']\n",
            "After lemmatization: ['rofl', 'true', 'name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "2AXrOeZtCUfK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the label to numerical value\n",
        "# Encode labels to 0 and 1\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['label'])  # spam=1, ham=0"
      ],
      "metadata": {
        "id": "DhE6CPEVDxq-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(df['clean_text'], y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Vw4RWgNTEET8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW vectorizer\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train_text)\n",
        "X_test_bow = bow_vectorizer.transform(X_test_text)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb_bow = MultinomialNB()\n",
        "nb_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = nb_bow.predict(X_test_bow)\n",
        "print(\"=== Bag-of-Words Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bow))\n",
        "print(classification_report(y_test, y_pred_bow))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcUyck9SFFP2",
        "outputId": "0b595508-32e6-46c5-9dd6-8d7151134d3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bag-of-Words Results ===\n",
            "Accuracy: 0.9802690582959641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       965\n",
            "           1       0.96      0.89      0.92       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.97      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
        "print(\"=== TF-IDF Results ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
        "print(classification_report(y_test, y_pred_tfidf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6P9NhSXFL1W",
        "outputId": "8b23fbb9-92dd-4804-f13d-bd6fa54ed5ef"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TF-IDF Results ===\n",
            "Accuracy: 0.9668161434977578\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       965\n",
            "           1       1.00      0.75      0.86       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.98      0.88      0.92      1115\n",
            "weighted avg       0.97      0.97      0.96      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def predict_message(message, model, vectorizer):\n",
        "#     # Preprocess the message\n",
        "#     tokens = preprocess_text_tokens(message)\n",
        "#     clean_text = ' '.join(tokens)\n",
        "\n",
        "#     # Convert to vector\n",
        "#     vect = vectorizer.transform([clean_text])\n",
        "\n",
        "#     # Predict\n",
        "#     pred = model.predict(vect)[0]\n",
        "\n",
        "#     # Convert numeric label back to text\n",
        "#     label = le.inverse_transform([pred])[0]\n",
        "\n",
        "#     return label\n"
      ],
      "metadata": {
        "id": "nszrRnkwGyV_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# while True:\n",
        "#     msg = input(\"Enter an email/message (or type 'exit' to stop):\\n\")\n",
        "#     if msg.lower() == 'exit':\n",
        "#         break\n",
        "#     result = predict_message(msg, nb_bow, bow_vectorizer)\n",
        "#     print(\"Prediction:\", result)\n",
        "#     print(\"-\"*40)\n"
      ],
      "metadata": {
        "id": "Vgmp8FXsG18Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqAHenoSHlQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}